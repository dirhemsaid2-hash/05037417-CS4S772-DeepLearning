{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e0d3e9b4",
   "metadata": {},
   "source": [
    "# Deep Learning \n",
    "\n",
    "# Practical Assessment \n",
    "\n",
    "# CS4S772\n",
    "\n",
    "# Dirhem Kassim Said \n",
    "\n",
    "# 05037417\n",
    "___________________________________________________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14973ded",
   "metadata": {},
   "source": [
    "# Student Pipline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c450d6f",
   "metadata": {},
   "source": [
    "### 1.0 Import Core Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20687ef1",
   "metadata": {},
   "source": [
    "#### 1.1 Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "770efb87",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Dirhem\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Dirhem\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Dirhem\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import core libraries\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# NLP preprocessing tools\n",
    "import re\n",
    "import nltk\n",
    "from pathlib import Path\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Ensure required NLTK resources are available\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b025e8",
   "metadata": {},
   "source": [
    "#### 1.2 Import Deep Learning Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "fee0c108",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import deep learning libraries\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense, Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85afc2f0",
   "metadata": {},
   "source": [
    "#### 1.3 Embedding and Vector Database Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "d96221db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding and Vector Database Tools\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import chromadb\n",
    "from chromadb.config import Settings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8b9006f",
   "metadata": {},
   "source": [
    "#### 1.4 Topic Modelling Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "49f338cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Topic modelling tools\n",
    "\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "from gensim.models.ldamodel import LdaModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be30ef6",
   "metadata": {},
   "source": [
    "### 1.5 Check GPU availability "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "0247cfad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "e83ccff7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU detected. \n"
     ]
    }
   ],
   "source": [
    "# Check GPU availability\n",
    "\n",
    "device_name = tf.test.gpu_device_name()\n",
    "print(\"GPU detected:\" if device_name else \"No GPU detected.\", device_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3135cff5",
   "metadata": {},
   "source": [
    "### 1.6 Set Random Seeds for Reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "d4ffe7c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seeds for reproducibility\n",
    "\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82d648ed",
   "metadata": {},
   "source": [
    "### 1.7 Define Global Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "e9ddd271",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define global paths\n",
    "\n",
    "DATA_PATH = \"SherlockHolmesStories\"   # Folder containing .txt files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba956c4b",
   "metadata": {},
   "source": [
    "___________________________________________________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97831732",
   "metadata": {},
   "source": [
    "# 2.0 Load and Explore the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed3c42a",
   "metadata": {},
   "source": [
    "### 2.1 Load All Sherlock Holmes Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "d1421b6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 67 stories.\n",
      "Example filenames: ['3gab.txt', '3gar.txt', '3stu.txt', 'abbe.txt', 'advs.txt']\n"
     ]
    }
   ],
   "source": [
    "# Load all Sherlock Holmes files\n",
    "# Loads all .txt files from the Sherlock folder into a list of strings.\n",
    "\n",
    "import os\n",
    "\n",
    "sherlock_path = r\"C:\\Users\\Dirhem\\Deep Learning 202526\\Assessment\\Sherlock Holmes Stories\\sherlock\"\n",
    "story_files = [f for f in os.listdir(sherlock_path) if f.endswith(\".txt\")]\n",
    "\n",
    "stories = []\n",
    "for filename in story_files:\n",
    "    with open(os.path.join(sherlock_path, filename), \"r\", encoding=\"utf-8\") as f:\n",
    "        stories.append(f.read())\n",
    "\n",
    "print(f\"Loaded {len(stories)} stories.\")\n",
    "print(\"Example filenames:\", story_files[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70153a3a",
   "metadata": {},
   "source": [
    "### 2.2 Combine Into Structured Format (Stories to Paragraphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "27389362",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example story has 226 paragraphs.\n"
     ]
    }
   ],
   "source": [
    "# Convert each story into a list of paragraphs\n",
    "# Splits on double newlines to approximate paragraph boundaries.\n",
    "\n",
    "structured_stories = []\n",
    "\n",
    "for text in stories:\n",
    "    paragraphs = [p.strip() for p in text.split(\"\\n\\n\") if p.strip()]\n",
    "    structured_stories.append(paragraphs)\n",
    "\n",
    "print(f\"Example story has {len(structured_stories[0])} paragraphs.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "418acffa",
   "metadata": {},
   "source": [
    "### 2.3 Basic Exploratory Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50f9f2b5",
   "metadata": {},
   "source": [
    "#### 2.3.1 Word Count Per Story"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "53de02b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word count per story: [6129, 6274, 6546, 9231, 104527, 9764, 8187, 7792, 7895, 9702, 10758, 657438, 8721, 82660, 6789, 574860, 10028, 7736, 7213, 9819, 10058, 5859, 8779, 8371, 7236, 7400, 7922, 9038, 7083, 59251, 7059, 9818, 7755, 6141, 7261, 67768, 5729, 87135, 8128, 7655, 12714, 8190, 9303, 11575, 7382, 9185, 7307, 6694, 5588, 112425, 8615, 9711, 6320, 43149, 9659, 8409, 7914, 9891, 6869, 43467, 6047, 9658, 9282, 57687, 4547, 11489, 7584]\n",
      "Average words per story: 35047.40298507463\n"
     ]
    }
   ],
   "source": [
    "# Word count per story\n",
    "# Simple whitespace tokenization for quick analysis.\n",
    "\n",
    "word_counts = [len(story.split()) for story in stories]\n",
    "\n",
    "print(\"Word count per story:\", word_counts)\n",
    "print(\"Average words per story:\", sum(word_counts) / len(word_counts))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f17aa5",
   "metadata": {},
   "source": [
    "#### 2.3.2 Sentence Counts Per Story "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "66a77df9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence count per story: [603, 553, 591, 649, 7278, 657, 612, 571, 591, 671, 945, 48126, 534, 7003, 481, 41130, 691, 625, 431, 674, 649, 584, 522, 535, 429, 497, 463, 697, 479, 4116, 457, 756, 669, 488, 560, 5340, 639, 5762, 606, 416, 933, 615, 655, 961, 656, 637, 519, 469, 490, 8342, 691, 813, 575, 3050, 679, 598, 573, 654, 529, 2785, 563, 759, 659, 4504, 365, 855, 487]\n",
      "Average sentences per story: 2559.6417910447763\n"
     ]
    }
   ],
   "source": [
    "# Sentence count per story\n",
    "# Uses a simple split on '.', '!', '?' for approximate sentence boundaries.\n",
    "\n",
    "import re\n",
    "\n",
    "def count_sentences(text):\n",
    "    sentences = re.split(r\"[.!?]+\", text)\n",
    "    return len([s for s in sentences if s.strip()])\n",
    "\n",
    "sentence_counts = [count_sentences(story) for story in stories]\n",
    "\n",
    "print(\"Sentence count per story:\", sentence_counts)\n",
    "print(\"Average sentences per story:\", sum(sentence_counts) / len(sentence_counts))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b871e6",
   "metadata": {},
   "source": [
    "#### 2.3.3 Example Raw Text (Safe Sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "ceb2f1d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample text snippet:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                        THE ADVENTURE OF THE THREE GABLES\n",
      "\n",
      "                               Arthur Conan Doyle\n",
      "\n",
      "\n",
      "\n",
      "     I don't think that any of my adventures with Mr. Sherlock Holmes\n",
      "     opened quite so abruptly, or so dramatically, as that which I\n",
      "     associate with The Three Gables. I had not\n"
     ]
    }
   ],
   "source": [
    "# Show a small safe sample (first 300 characters)\n",
    "# Avoids printing copyrighted text in full.\n",
    "\n",
    "sample_text = stories[0][:300]\n",
    "print(\"Sample text snippet:\\n\")\n",
    "print(sample_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "544808e3",
   "metadata": {},
   "source": [
    "___________________________________________________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e44de23",
   "metadata": {},
   "source": [
    "# 3.0 Text Preprocessing Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1b10143",
   "metadata": {},
   "source": [
    "### 3.1 Define Preprocessing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "4d5120a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Dirhem\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Dirhem\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Dirhem\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Define preprocessing functions\n",
    "# Applies lowercasing, punctuation, number removal, stopword filtering, lemmatization, and tokenization.\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"wordnet\")\n",
    "\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"\\d+\", \"\", text)\n",
    "    text = text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [lemmatizer.lemmatize(w) for w in tokens if w not in stop_words]\n",
    "    return tokens\n",
    "\n",
    "def preprocess_sentences(text):\n",
    "    sentences = sent_tokenize(text)\n",
    "    return [preprocess_text(s) for s in sentences]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ab3071",
   "metadata": {},
   "source": [
    "### 3.2 Apply Preprocessing to All Stories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "2f21b3df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing complete for all stories.\n",
      "Total cleaned stories: 67\n"
     ]
    }
   ],
   "source": [
    "# Apply preprocessing to all stories\n",
    "# Applies the full preprocessing pipeline to each story and stores the cleaned output.\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"\\d+\", \"\", text)\n",
    "    text = text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "    tokens = text.split()  # replaces word_tokenize()\n",
    "    tokens = [lemmatizer.lemmatize(w) for w in tokens if w not in stop_words]\n",
    "    return tokens\n",
    "\n",
    "cleaned_stories = [preprocess_text(story) for story in stories]\n",
    "\n",
    "print(\"Preprocessing complete for all stories.\")\n",
    "print(f\"Total cleaned stories: {len(cleaned_stories)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79296565",
   "metadata": {},
   "source": [
    "### 3.3 Before and After Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "cd53b76e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAW TEXT SAMPLE:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                        THE ADVENTURE OF THE THREE GABLES\n",
      "\n",
      "                               Arthur Conan Doyle\n",
      "\n",
      "\n",
      "\n",
      "     I don't think that any of my adventures with Mr. Sherlock Holmes\n",
      "     opened quite so abruptly, or so dramatically, as that which I\n",
      "     associate with The Three Gables. I had not\n",
      "\n",
      "CLEANED & TOKENISED SAMPLE:\n",
      "\n",
      "['adventure', 'three', 'gable', 'arthur', 'conan', 'doyle', 'dont', 'think', 'adventure', 'mr', 'sherlock', 'holmes', 'opened', 'quite', 'abruptly', 'dramatically', 'associate', 'three', 'gable']\n"
     ]
    }
   ],
   "source": [
    "# Before and After Examples\n",
    "# Shows a short raw text sample and its cleaned, tokenised version.\n",
    "\n",
    "example_raw = stories[0][:300]  # first 300 characters of the first story\n",
    "example_clean = preprocess_text(example_raw)\n",
    "\n",
    "print(\"RAW TEXT SAMPLE:\\n\")\n",
    "print(example_raw)\n",
    "\n",
    "print(\"\\nCLEANED & TOKENISED SAMPLE:\\n\")\n",
    "print(example_clean[:50])  # show first 50 cleaned tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c50d50f7",
   "metadata": {},
   "source": [
    "___________________________________________________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a09f82d1",
   "metadata": {},
   "source": [
    "# 4.0 Prepare Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5172edf",
   "metadata": {},
   "source": [
    "### 4.1 Paragraph Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "3cc8b2b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paragraph splitting complete.\n",
      "Total stories: 67\n",
      "Total paragraphs extracted: 55705\n"
     ]
    }
   ],
   "source": [
    "# Paragraph Splitting\n",
    "# Splits each story into paragraphs based on double line breaks.\n",
    "\n",
    "def split_into_paragraphs(text):\n",
    "    paragraphs = [p.strip() for p in text.split(\"\\n\\n\") if p.strip()]\n",
    "    return paragraphs\n",
    "\n",
    "# Apply paragraph splitting to all stories\n",
    "paragraphs_per_story = [split_into_paragraphs(story) for story in stories]\n",
    "\n",
    "# Flatten into a single list of paragraphs (useful for summarisation or embeddings)\n",
    "all_paragraphs = [p for story in paragraphs_per_story for p in story]\n",
    "\n",
    "print(\"Paragraph splitting complete.\")\n",
    "print(f\"Total stories: {len(stories)}\")\n",
    "print(f\"Total paragraphs extracted: {len(all_paragraphs)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ad36db0",
   "metadata": {},
   "source": [
    "___________________________________________________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa44049c",
   "metadata": {},
   "source": [
    "# 5.0 Prepare Date for Summarisation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ba27d06",
   "metadata": {},
   "source": [
    "### 5.1 Create Input–Output Pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "9a74eb4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input–output pairs created.\n",
      "Total inputs: 100\n",
      "Total summaries: 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Dirhem\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# 4.1 Create Input–Output Pairs (100 Samples)\n",
    "# Each paragraph becomes an input; summary = first sentence (extractive baseline)\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "# Ensure punkt is available\n",
    "nltk.download(\"punkt\")\n",
    "\n",
    "# Select first 100 paragraphs\n",
    "input_texts = all_paragraphs[:100]\n",
    "\n",
    "# Extractive summaries: first sentence of each paragraph\n",
    "output_summaries = []\n",
    "for paragraph in input_texts:\n",
    "    sentences = sent_tokenize(paragraph)\n",
    "    if len(sentences) > 0:\n",
    "        output_summaries.append(sentences[0])\n",
    "    else:\n",
    "        output_summaries.append(\"\")  # fallback if paragraph is empty\n",
    "\n",
    "print(\"Input–output pairs created.\")\n",
    "print(f\"Total inputs: {len(input_texts)}\")\n",
    "print(f\"Total summaries: {len(output_summaries)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa4be7ca",
   "metadata": {},
   "source": [
    "### 5.2 Tokenise Text for Seq2Seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "06926c7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenisation and padding complete.\n",
      "Input shape: (100, 211)\n",
      "Output shape: (100, 29)\n"
     ]
    }
   ],
   "source": [
    "# Tokenise Text for Seq2Seq\n",
    "# Fit tokenizer, convert to sequences, pad inputs and outputs.\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Fit tokenizer on both inputs and outputs\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(input_texts + output_summaries)\n",
    "\n",
    "# Convert to sequences\n",
    "input_sequences = tokenizer.texts_to_sequences(input_texts)\n",
    "output_sequences = tokenizer.texts_to_sequences(output_summaries)\n",
    "\n",
    "# Pad sequences to uniform length\n",
    "max_input_len = max(len(seq) for seq in input_sequences)\n",
    "max_output_len = max(len(seq) for seq in output_sequences)\n",
    "\n",
    "input_padded = pad_sequences(input_sequences, maxlen=max_input_len, padding=\"post\")\n",
    "output_padded = pad_sequences(output_sequences, maxlen=max_output_len, padding=\"post\")\n",
    "\n",
    "print(\"Tokenisation and padding complete.\")\n",
    "print(f\"Input shape: {input_padded.shape}\")\n",
    "print(f\"Output shape: {output_padded.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c867e8a",
   "metadata": {},
   "source": [
    "### 5.3 Train and Validation Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "f8a1b70c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train/validation split complete.\n",
      "Training samples: 80\n",
      "Validation samples: 20\n"
     ]
    }
   ],
   "source": [
    "# rain/Validation Split\n",
    "# Splits padded sequences into training and validation sets.\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    input_padded, output_padded, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(\"Train/validation split complete.\")\n",
    "print(f\"Training samples: {X_train.shape[0]}\")\n",
    "print(f\"Validation samples: {X_val.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abc379bc",
   "metadata": {},
   "source": [
    "___________________________________________________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a84912ca",
   "metadata": {},
   "source": [
    "# 6.0 Build and Train the Seq2Seq Summarisation Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9756f037",
   "metadata": {},
   "source": [
    "### 6.1 Define Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "1e771f2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder defined successfully.\n"
     ]
    }
   ],
   "source": [
    "# Define Encoder\n",
    "# The encoder processes the input sequence and returns hidden states.\n",
    "\n",
    "from tensorflow.keras.layers import Input, Embedding, LSTM\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# Vocabulary size\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "# Encoder input layer\n",
    "encoder_inputs = Input(shape=(max_input_len,))\n",
    "\n",
    "# Encoder embedding layer\n",
    "encoder_embedding = Embedding(\n",
    "    input_dim=vocab_size,\n",
    "    output_dim=128,\n",
    "    mask_zero=True\n",
    ")(encoder_inputs)\n",
    "\n",
    "# Encoder LSTM (returns hidden + cell states)\n",
    "encoder_lstm = LSTM(\n",
    "    256,\n",
    "    return_state=True\n",
    ")\n",
    "\n",
    "encoder_outputs, state_h, state_c = encoder_lstm(encoder_embedding)\n",
    "\n",
    "# Store encoder states\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "print(\"Encoder defined successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a41cc40",
   "metadata": {},
   "source": [
    "### 6.2 Define Decode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "8ca3f1c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoder defined successfully.\n"
     ]
    }
   ],
   "source": [
    "# Define Decoder\n",
    "# The decoder generates the output summary sequence using encoder states.\n",
    "\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "\n",
    "# Decoder input layer\n",
    "decoder_inputs = Input(shape=(max_output_len - 1,))\n",
    "\n",
    "# Decoder embedding layer\n",
    "decoder_embedding = Embedding(\n",
    "    input_dim=vocab_size,\n",
    "    output_dim=128,\n",
    "    mask_zero=True\n",
    ")(decoder_inputs)\n",
    "\n",
    "# Decoder LSTM (receives encoder states as initial state)\n",
    "decoder_lstm = LSTM(\n",
    "    256,\n",
    "    return_sequences=True,\n",
    "    return_state=True\n",
    ")\n",
    "\n",
    "decoder_outputs, _, _ = decoder_lstm(\n",
    "    decoder_embedding,\n",
    "    initial_state=encoder_states\n",
    ")\n",
    "\n",
    "# Dense output layer (predicts next token)\n",
    "decoder_dense = Dense(vocab_size, activation=\"softmax\")\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "print(\"Decoder defined successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4003b6bb",
   "metadata": {},
   "source": [
    "### 6.3 Connect Encoder and Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "3614e63d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder and decoder connected successfully.\n"
     ]
    }
   ],
   "source": [
    "# Connect Encoder and Decoder\n",
    "# Combines encoder and decoder into a single Seq2Seq model.\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# Connect encoder inputs and decoder inputs to decoder outputs\n",
    "model = Model(\n",
    "    [encoder_inputs, decoder_inputs],\n",
    "    decoder_outputs\n",
    ")\n",
    "\n",
    "print(\"Encoder and decoder connected successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a8d869c",
   "metadata": {},
   "source": [
    "### 6.4 Compile the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "e7f1c3af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model compiled successfully.\n"
     ]
    }
   ],
   "source": [
    "# Compile the Model\n",
    "# Configures the Seq2Seq model for training.\n",
    "\n",
    "model.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "print(\"Model compiled successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a03745d",
   "metadata": {},
   "source": [
    "### 6.5 Train the Seq2Seq model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "680791f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoder input shape: (100, 28)\n",
      "Decoder target shape: (100, 28)\n",
      "Epoch 1/10\n",
      "5/5 [==============================] - 7s 701ms/step - loss: 6.7262 - accuracy: 0.0800 - val_loss: 6.7156 - val_accuracy: 0.1266\n",
      "Epoch 2/10\n",
      "5/5 [==============================] - 1s 281ms/step - loss: 6.6527 - accuracy: 0.1170 - val_loss: 6.4398 - val_accuracy: 0.1266\n",
      "Epoch 3/10\n",
      "5/5 [==============================] - 1s 284ms/step - loss: 5.9213 - accuracy: 0.1170 - val_loss: 6.0898 - val_accuracy: 0.1266\n",
      "Epoch 4/10\n",
      "5/5 [==============================] - 1s 293ms/step - loss: 5.3760 - accuracy: 0.1170 - val_loss: 6.0591 - val_accuracy: 0.1266\n",
      "Epoch 5/10\n",
      "5/5 [==============================] - 2s 306ms/step - loss: 5.1644 - accuracy: 0.1170 - val_loss: 6.2644 - val_accuracy: 0.1266\n",
      "Epoch 6/10\n",
      "5/5 [==============================] - 2s 324ms/step - loss: 5.0537 - accuracy: 0.1170 - val_loss: 6.4659 - val_accuracy: 0.1266\n",
      "Epoch 7/10\n",
      "5/5 [==============================] - 2s 337ms/step - loss: 4.9761 - accuracy: 0.1170 - val_loss: 6.6136 - val_accuracy: 0.1266\n",
      "Epoch 8/10\n",
      "5/5 [==============================] - 2s 339ms/step - loss: 4.8962 - accuracy: 0.1170 - val_loss: 6.6778 - val_accuracy: 0.1266\n",
      "Epoch 9/10\n",
      "5/5 [==============================] - 2s 364ms/step - loss: 4.8545 - accuracy: 0.1170 - val_loss: 6.7778 - val_accuracy: 0.1266\n",
      "Epoch 10/10\n",
      "5/5 [==============================] - 2s 367ms/step - loss: 4.8382 - accuracy: 0.1170 - val_loss: 6.8410 - val_accuracy: 0.1266\n",
      "Model training complete.\n"
     ]
    }
   ],
   "source": [
    "# Train the Seq2Seq Model\n",
    "# Trains the Seq2Seq model using teacher forcing.\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Shift output sequences for decoder input (teacher forcing)\n",
    "decoder_input_data = output_padded[:, :-1]\n",
    "decoder_target_data = output_padded[:, 1:]\n",
    "\n",
    "# Adjust max_output_len for decoder inputs\n",
    "decoder_max_len = decoder_input_data.shape[1]\n",
    "\n",
    "print(f\"Decoder input shape: {decoder_input_data.shape}\")\n",
    "print(f\"Decoder target shape: {decoder_target_data.shape}\")\n",
    "\n",
    "history = model.fit(\n",
    "    [X_train, decoder_input_data[:X_train.shape[0]]],\n",
    "    np.expand_dims(decoder_target_data[:X_train.shape[0]], -1),\n",
    "    validation_data=(\n",
    "        [X_val, decoder_input_data[X_train.shape[0]:X_train.shape[0] + X_val.shape[0]]],\n",
    "        np.expand_dims(decoder_target_data[X_train.shape[0]:X_train.shape[0] + X_val.shape[0]], -1)\n",
    "    ),\n",
    "    batch_size=16,\n",
    "    epochs=10\n",
    ")\n",
    "\n",
    "print(\"Model training complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c82c2f2",
   "metadata": {},
   "source": [
    "### 6.6 Generate Example Summaries (Inference Mode)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fbcdb41",
   "metadata": {},
   "source": [
    "#### 6.6.1 Build Inference Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "fdd0c88e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Inference Models\n",
    "# Separate encoder and decoder models for inference.\n",
    "\n",
    "# Encoder inference model\n",
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "# Decoder inference setup\n",
    "decoder_state_input_h = Input(shape=(256,))\n",
    "decoder_state_input_c = Input(shape=(256,))\n",
    "decoder_state_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "\n",
    "decoder_inf_outputs, state_h_inf, state_c_inf = decoder_lstm(\n",
    "    decoder_embedding,\n",
    "    initial_state=decoder_state_inputs\n",
    ")\n",
    "\n",
    "decoder_inf_states = [state_h_inf, state_c_inf]\n",
    "decoder_inf_outputs = decoder_dense(decoder_inf_outputs)\n",
    "\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + decoder_state_inputs,\n",
    "    [decoder_inf_outputs] + decoder_inf_states\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daa336d6",
   "metadata": {},
   "source": [
    "#### 6.6.2 Define a Function to Generate a Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "80979ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary Generation Function\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "index_to_word = {v: k for k, v in tokenizer.word_index.items()}\n",
    "\n",
    "def generate_summary(input_seq):\n",
    "    # Encode the input\n",
    "    states = encoder_model.predict(input_seq)\n",
    "\n",
    "    # Start token (use 0 or a padding token)\n",
    "    target_seq = np.zeros((1, 1))\n",
    "\n",
    "    summary = []\n",
    "\n",
    "    for _ in range(max_output_len - 1):\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + states)\n",
    "\n",
    "        # Choose the most likely word\n",
    "        sampled_token = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_word = index_to_word.get(sampled_token, \"\")\n",
    "\n",
    "        summary.append(sampled_word)\n",
    "\n",
    "        # Update target sequence\n",
    "        target_seq = np.zeros((1, 1))\n",
    "        target_seq[0, 0] = sampled_token\n",
    "\n",
    "        # Update states\n",
    "        states = [h, c]\n",
    "\n",
    "    return \" \".join(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ac891f",
   "metadata": {},
   "source": [
    "#### 6.6.3 Example Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "fc7ffc4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 1\n",
      "------------------------------------------------------------\n",
      "Input paragraph:\n",
      "THE ADVENTURE OF THE THREE GABLES\n",
      "\n",
      "Extractive summary (ground truth):\n",
      "THE ADVENTURE OF THE THREE GABLES\n",
      "1/1 [==============================] - 1s 838ms/step\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 28) for input KerasTensor(type_spec=TensorSpec(shape=(None, 28), dtype=tf.float32, name='input_6'), name='input_6', description=\"created by layer 'input_6'\"), but it was called on an input with incompatible shape (None, 1).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model was constructed with shape (None, 28) for input KerasTensor(type_spec=TensorSpec(shape=(None, 28), dtype=tf.float32, name='input_6'), name='input_6', description=\"created by layer 'input_6'\"), but it was called on an input with incompatible shape (None, 1).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 798ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "\n",
      "Model-generated summary:\n",
      "am  am  am  am  am  am  am  am  am  am  am  am  am  am \n",
      "\n",
      "Example 2\n",
      "------------------------------------------------------------\n",
      "Input paragraph:\n",
      "Arthur Conan Doyle\n",
      "\n",
      "Extractive summary (ground truth):\n",
      "Arthur Conan Doyle\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "1/1 [==============================] - 0s 6ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "\n",
      "Model-generated summary:\n",
      "am  am  am  am  am  am  am  am  am  am  am  am  am  am \n",
      "\n",
      "Example 3\n",
      "------------------------------------------------------------\n",
      "Input paragraph:\n",
      "I don't think that any of my adventures with Mr. Sherlock Holmes\n",
      "     opened quite so abruptly, or so dramatically, as that which I\n",
      "     associate with The Three Gables. I had not seen Holmes for some days\n",
      "     and had no idea of the new channel into which his activities had been\n",
      "     directed. He was in a chatty mood that morning, however, and had just\n",
      "     settled me into the well-worn low armchair on one side of the fire,\n",
      "     while he had curled down with his pipe in his mouth upon the opposite\n",
      "     chair, when our visitor arrived. If I had said that a mad bull had\n",
      "     arrived it would give a clearer impression of what occurred.\n",
      "\n",
      "Extractive summary (ground truth):\n",
      "I don't think that any of my adventures with Mr. Sherlock Holmes\n",
      "     opened quite so abruptly, or so dramatically, as that which I\n",
      "     associate with The Three Gables.\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "\n",
      "Model-generated summary:\n",
      "am was  am  am  am  am  am  am  am  am  am  am  am  am  am\n"
     ]
    }
   ],
   "source": [
    "# Generate Example Outputs\n",
    "\n",
    "for i in range(3):\n",
    "    print(f\"\\nExample {i+1}\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "    # Input paragraph\n",
    "    input_text = input_texts[i]\n",
    "    print(\"Input paragraph:\")\n",
    "    print(input_text)\n",
    "\n",
    "    # Ground truth summary\n",
    "    print(\"\\nExtractive summary (ground truth):\")\n",
    "    print(output_summaries[i])\n",
    "\n",
    "    # Model-generated summary\n",
    "    input_seq = input_padded[i:i+1]\n",
    "    predicted = generate_summary(input_seq)\n",
    "    print(\"\\nModel-generated summary:\")\n",
    "    print(predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ec45dd5",
   "metadata": {},
   "source": [
    "### 6.7  Plot Training Curves (Loss and Accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "b8f15d9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA1cAAAGJCAYAAABmacmGAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjgsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvwVt1zgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAUz9JREFUeJzt3Qd4FOXWwPGTHgKEFkoSqcKlSgJSRcECRKQqNiwUFb1XUbxYQQURFVRErGADbBT1CkFBqiKCKAokNEEQlRoSaiCBJCT7PeeFzZeyCUnYZLb8f8+zZnd2svvuzBDn7DnvGR+bzWYTAAAAAMAF8b2wXwcAAAAAEFwBAAAAgJOQuQIAAAAAJyC4AgAAAAAnILgCAAAAACcguAIAAAAAJyC4AgAAAAAnILgCAAAAACcguAIAAAAAJyC4AgAXMHjwYKlXr16JfvfZZ58VHx8fp4/JEznaVrrddfufz4wZM8zv/v33304bj76Wvqa+NgDA/RFcAUAh9MS3KLcVK1awHZ0oMTFR/P395Y477ihwnRMnTki5cuXkhhtucPltP3PmTJk8ebK4qptvvtkcx0888YTVQwEAt+Zv9QAAwJV98sknuR5//PHHsnTp0nzLmzZtekHv8/7770tWVlaJfvfpp5+WJ598UjxJjRo1pFu3bhIbGyupqakSEhKSb52vvvpKTp8+XWgAVhTbt28XX1/fUg+uNm/eLA8//HCu5XXr1pVTp05JQECAWCU5OVm+/vprk8GbNWuWTJgwgUwoAJQQwRUAFCLvifvPP/9sgqvzndAXFBAU5EJOrjXDozdPc/vtt8uiRYtk/vz5cuuttzoMWCpVqiQ9e/a8oPcJCgoSq2i2KDg4WKz0v//9TzIzM2XatGly9dVXy8qVK6VLly7iamw2mwmmNVsJAK6KskAAuEBXXnmltGjRQtatWyedO3c2QdWoUaPMc5p50ZP/iIgIcxJ/8cUXy7hx48zJbGFzruxzcSZOnCjvvfee+T39/bZt28qvv/563nlE+njYsGEyb948Mzb93ebNm5tgJS8taWzTpo05ydf3effdd4s0j0tfv0KFCiaQzGvAgAFSq1at7M/522+/SUxMjISFhZmT4/r168tdd91V6Otff/31Ur58eRNEOSobXL58udx4443ms/34449y0003SZ06dczj2rVry3//+1+TFTofR3OutmzZYgINHetFF10kzz//vMPMYlH2rx4fCxYskH/++Se7jNS+rwuac/Xdd9/JFVdcYT5/5cqVpW/fvvL777/nWse+j3bu3GnGr+tpsDlkyBCH+6Qgn332mckSXnXVVSYDq48d2bZtmykfrF69utkujRs3lqeeeirXOvv27ZO77747e3vofv7Pf/4j6enpucZclPlsuo169eolixcvNsenvqcem2r69Olm/2iGU9+nWbNmMmXKFIfj/vbbb02wWLFiRQkNDTX/huzH1JgxY8wXG0lJSfl+79577zXbVAM6ACgqz/uqEwAscPjwYenRo4fJsGhWq2bNmtknjRqAjBgxwvzUk+bRo0ebUqxXXnnlvK+rJ4E6t+i+++4zJ58vv/yymWO0a9eu82a7Vq1aZUrn7r//fnNi+cYbb0j//v1l9+7dUq1aNbPOhg0b5Nprr5Xw8HAZO3asCQqee+45cwJ9Prfccou8/fbbJnDQwMZOT+y1zExP+P38/Ewg1L17d/OaWr6oJ6x6Eq1jK4wGFhpUfPnll3LkyBGpWrVq9nNz5swxY9Xslvriiy/M++qJvH62tWvXyptvvil79+41zxVHQkKCCTTOnDljxqvj0ADXUcakKPtXA5Djx4+bsbz22mtmma5bkGXLlpljqUGDBiYY0QBRP0unTp1k/fr1+RqfaMCjQcz48ePN8x988IEJOl566aXzftb9+/fL999/Lx999FF2UKxjfOuttyQwMDB7vY0bN5pgT485DTp0DH/++afZzy+88EL2a7Vr106OHTtm1mnSpIkJtnT/6b7J+XrFKdnUMenxP3ToUBPQKQ2k9MuCPn36mKytjkOPcw2AH3jggVz7R4N4XXfkyJHm2NNjXr9kuO222+TOO+80x7seT/plgZ0Ggzpu/fdidWYRgJuxAQCK7IEHHrDl/dPZpUsXs2zq1Kn51k9NTc237L777rOFhITYTp8+nb1s0KBBtrp162Y//uuvv8xrVqtWzXbkyJHs5bGxsWb5119/nb1szJgx+cakjwMDA207d+7MXhYfH2+Wv/nmm9nLevfubcayb9++7GU7duyw+fv753vNvLKysmyRkZG2/v3751r++eefm99duXKleTx37lzz+Ndff7UV14IFC8zvvvvuu7mWd+jQwbx3ZmZmgdt5/PjxNh8fH9s///xT6LbS7a7b3+7hhx826/zyyy/ZyxITE22VKlUyy3XfFHf/9uzZM9f+zbufp0+fnr0sOjraVqNGDdvhw4dz7TtfX1/bwIED832Wu+66K9drXn/99ea4KYqJEyfaypUrZ0tOTjaP//jjD/Oaus9y6ty5s61ixYq5tqX9GLDTsekYHe1n+3qOtr/Sz5932+r20mWLFi3Kt76j7R4TE2Nr0KBB9uNjx46ZMbdv39526tSpAsfdsWNHs05OX331lXnv77//Pt/7AEBhKAsEACfQ0iQtx8orZ7ZDM1CHDh0yGQD9Jl/LrIqSHapSpUr2Y/1dpZmr8+nataspU7Nr2bKlKYuy/65mfjRL0q9fP1PGZdewYUOTOTkfzaRpxmrhwoVy8uTJ7OWaBYiMjJTLL7/cPNZsgfrmm28kIyNDisOe8cpZGvjXX3+ZuW+a0bA3osi5nVNSUsx2vuyyy8w8Hc1UFId+ng4dOpgsjJ2OwZ4lc+b+zevAgQMSFxdnsn45M3W677R0T8eW17///e9cj/X9NZOq2bPz0RJALWvUzKZq1KiRXHrppblKA7VkTudhaQZIyy5zspf4acZIS1B79+5tSvjyKumlAjQjp+WkhW13zQrqdtfSPz229bHSuZG6TzT7mDf7lHM8AwcOlF9++cVk4nJuFy0tdcW5ZwBcG8EVADiBBhOOyp507o7OHdK5MBrY6Em6vRmG/SSwMHlPZu2B1tGjR4v9u/bft/+ulutpyZkGU3k5WlZQ8KevoU0nlAZZGgBo0GU/gdUTVC2v0rJDnXOlpX46ZyYtLe28r68lX/oeOqdKS8yUPdDKGexoqaM9INGSO93O9hPjomznnHRulAYZedlL0py5fx29d0HvpfOhNIjQ4NEZx4jO4dLAU8sNdd6W/aZzxDQQtgdn9mBc5+4VRAMwXb+wdUoaXDmyevVq8+WBfU6abnf7PEf7drcHS+cbkx5f+uWIPaDU39fPr8cX148DUFwEVwDgBI7m4+jcEz3Bj4+PN/M6dF6IfptunwtTlNbrOmfJkbOVf6X3u0WlGR6df/P555+bx/oZNdjSE1Y7PUHV+Str1qwx81o0SNIsiGZIcma8CqLBim4rbROu9Kc2MIiOjs7OwGlWR+d+6XWaNIOi29neJKKkLe7Pxxn71xlKup8//fRT81Mbf2gwab+9+uqrpomDdhF0toKClbwNXgr7d6VB0zXXXGMCzUmTJpn9rttdP0dJtrsGo9o4wx5c6bGqgf+FtvgH4J1oaAEApUS78Gl5ljZu0C6COcvaXIE2PdByKc1W5OVoWUG0ocLrr79uMhdaEqjBlgZdeekyvWkDBM0+aWZg9uzZcs899xT6+u3btzfljfo7GkRptsjeREFt2rRJ/vjjD9OUQUu87PSEuyT02lM7duxw2FyhpPu3qBkQfW9H76W0zFAzf5qtuVAaeOn21MYd2ggiL+14qMGGlrpqYw2l1+kqiGaONHNX2Do5s2oamNrLRXNm7IpCg1gNfjRbmjNrp405crKXxOqYzpeJ1eNGM6raiVM/d6tWrUwTDAAoLjJXAFDKGYWcGQTtQvbOO++4zPi0tEozPdrpLWdgpe2ri0qzVHqyq8GNdmHTYCsnLU/Lm0WxZ52KUhqoNBDTEjZtna2BinZ6y/k5VM730Psa8JXEddddZ+Z0acfBnGVveVuUF2f/akBUlDJB7dqo20a3pQYgdhogLFmyxIzNGbSsTjs2avCk7ezz3nSfarCix4UGTho86nWwtPwyJ/tn17lvOndPAx9tu5+XfT17wKNzuOy0zNHerbAoHG133bZaapp3vp7OJdMuinnbqec9HnWOoQaumnX84YcfyFoBKDEyVwBQSrShgn5TP2jQIHnooYdMUPDJJ584tSzvQmmrbz1p13k32sZcy7O0DbfOU9HGCkXRunVrkxnQluMaLOUsCVR64qwBh85N0pNrbTLw/vvvm0xHUYMFLdHS0ju9rpSONWc7cm35ra/76KOPmpJDfV0taSvKvDRHHn/8cbOftEX98OHDs1uxa1ZJW5KXZP9qCaRm9bRlu15nSeeFafMHR7SFu57sd+zY0Vwzyt6KXed16f5yBg0UNUgp6ALM2uJc96dmFnXM2sZfG5TovtY26zoXSoMzLcmzHycvvviiOZa0VFLX0Tli2qBDW+HrZQE0U6UBj2ab9HM99thjZgwatGkAlzdwK4i+hs5v1O2nLdq1tFSPJ83E6vvZ6XGgbeU1M6rbXANy3V9axqkNR3IGdNpiXi+joMe+jkmbpQBAiRTaSxAAUKRW7M2bN3e4pVavXm3ahmu764iICNvjjz9uW7x4cb42zwW1Yn/llVfyvaYu15bW52vFrmPNK2/bcbV8+XJbq1atTOv2iy++2PbBBx/YHnnkEVtwcHCR9/5TTz1l3rNhw4b5nlu/fr1twIABtjp16tiCgoJMm/FevXrZfvvtN1txtG3b1rzHO++8k++5rVu32rp27WqrUKGCLSwszDZ06NDs1vM525wXpRW72rhxo9mvug205fu4ceNsH374Yb524UXdvydPnrTddttttsqVK5vn7PvaUSt2tWzZMlunTp3M64aGhpqW+foZc7J/lqSkpPO2Nc8pPT3dtGq/4oorCt3e9evXN8eF3ebNm02bd/0Mul0aN25se+aZZ3L9jrZq15bs1atXN/taW6PrcZiWlpa9zrp160zrcz3e9JiYNGlSga3YtYW9I/Pnz7e1bNnSjKNevXq2l156yTZt2jSHn1vXveyyy7K3Zbt27WyzZs3K95pr1641v9+9e/dCtwsAFMZH/1OysAwA4Km0xEvnNjmaewR4Is1oaUnmxx9/bC4uDAAlwZwrAPByWnaWkwZU2k5dW3ID3kJLC7Vc84YbbrB6KADcGHOuAMDLaTc4vUaU/tSubVOmTDFzWnTuEeDptAnH1q1bzbw6vVSAM7oxAvBelAUCgJfTjnHaGS4hIcFcTFUbKWhzAm1eAHg6bY5y8OBBiYmJMQ1JtMMgAJQUwRUAAAAAOAFzrgAAAADACQiuAAAAAMAJaGjhQFZWlrkqvdZd60UhAQAAAHgnm80mJ06ckIiICPH1LTw3RXDlgAZWtWvXLq39AwAAAMDN7NmzRy666KJC1yG4csDeKUg3YGhoaOnsHZS6jIwMWbJkiXTv3l0CAgLY4uB4g0fhbxw43uDJMlzoPC45OdkkXorSTZTgygF7KaAGVgRX7v2PMiQkxOxDq/9RwvNxvIFjDp6Mv3HgmJMiTReioQUAAAAAOAHBFQAAAAA4AcEVAAAAADgBwRUAAAAAOAHBFQAAAAA4AcEVAAAAADgBwRUAAAAAOAHBFQAAAAA4AcEVAAAAADiBvzNeBKXEZhPJSGXzllRGhvhlpomkp4jYAtiOKF0cbyhrHHPgeIM3/I2z2cSd+NhsbjbiMpCcnCyVKlWS48ePS2hoqHUD0aDgxQjr3h8AAACwUMZj/0hA+cpuExtQFggAAAAATkBZoCsLCBEZtd/qUbitjIwMWbx4icTEdJeAAMoCwfEGz8LfOHC8wSv+xgWEiDshuHJlPj4igeWtHoX78smQTL+gs9uQ4Aocb/A0/I0Dxxu84W+cj4+4E8oCAQAAAMAJCK4AAAAAwAkIrgAAAADACQiuAAAAAMAJCK4AAAAAwAkIrgAAAADACQiuAAAAAMAJCK4AAAAAwAkIrgAAAADACQiuAAAAAMAJCK4AAAAAwAkIrgAAAADACQiuAAAAAMAJCK4AAAAAwAkIrgAAAADACQiuAAAAAMAJCK4AAAAAwAkIrgAAAACA4AoAAAAAXAOZKwAAAABwAoIrAAAAAHACgisAAAAAcAKCKwAAAABwAoIrAAAAAHACgisAAAAAcAKCKwAAAABwAoIrAAAAAHACgisAAAAAcAKCKwAAAABwAoIrAAAAAHACgisAAAAAcAKCKwAAAADwlODq7bfflnr16klwcLC0b99e1q5dW+C6W7Zskf79+5v1fXx8ZPLkyfnWGT9+vLRt21YqVqwoNWrUkH79+sn27dtL+VMAAAAA8GaWB1dz5syRESNGyJgxY2T9+vUSFRUlMTExkpiY6HD91NRUadCggUyYMEFq1arlcJ0ffvhBHnjgAfn5559l6dKlkpGRId27d5eUlJRS/jQAAAAAvJW/1QOYNGmSDB06VIYMGWIeT506VRYsWCDTpk2TJ598Mt/6mpHSm3L0vFq0aFGuxzNmzDAZrHXr1knnzp1L5XMAAAAA8G6WBlfp6ekm4Bk5cmT2Ml9fX+natausWbPGae9z/Phx87Nq1aoOn09LSzM3u+TkZPNTM156g3uy7zv2ITje4In4GweON3iyDBc6jyvOGCwNrg4dOiSZmZlSs2bNXMv18bZt25zyHllZWfLwww9Lp06dpEWLFg7X0TlaY8eOzbd8yZIlEhIS4pRxwDpaGgpwvMFT8TcOHG/wZEtd4DxOpyW5TVlgadO5V5s3b5ZVq1YVuI5mznTeV87MVe3atc08rdDQ0DIaKUrjWwb9B9mtWzcJCAhgA6NUcbyhrHHMgeMNnizDhc7j7FVtLh9chYWFiZ+fnxw8eDDXcn1cULOK4hg2bJh88803snLlSrnooosKXC8oKMjc8tIdafXOxIVjP6IscbyhrHHMgeMNnizABc7Hi/P+lnYLDAwMlEsvvVSWL1+eq4xPH3fs2LHEr2uz2UxgNXfuXPnuu++kfv36ThoxAAAAALhoWaCW4w0aNEjatGkj7dq1M9et0pbp9u6BAwcOlMjISDMvyt4EY+vWrdn39+3bJ3FxcVKhQgVp2LBhdingzJkzJTY21lzrKiEhwSyvVKmSlCtXzrLPCgAAAMBzWR5c3XLLLZKUlCSjR482QVB0dLRppW5vcrF7927TQdBu//790qpVq+zHEydONLcuXbrIihUrzLIpU6aYn1deeWWu95o+fboMHjy4jD4ZAAAAAG9ieXCltIRPb47YAya7evXqmbK/wpzveQAAAABwNkvnXAEAAACApyC4AgAAAAAnILgCAAAAACcguAIAAAAAJyC4AgAAAAAnILgCAAAAACcguAIAAAAAJyC4AgAAAAAnILgCAAAAACcguAIAAAAAJ/Cx2Ww2Z7yQJ0lOTpZKlSrJ8ePHJTQ01OrhoAR2Jp6QQdPWyqETp8Tf3198xIftiFJlE5ucOXOG4w1lhmMOZYnjDVYcc5J5RuLGxEhAQIDbxAb+ZTYqoAzN+Olv2XfstH5/IGmZmWx7lBGON5Q1jjlwvMFz+bvhd+MEV/A4GZlZsmDjAXP/zoaZMqhXF5NNAEqTZq1WrFghV155JccbygTHHMoSxxusOOZ++GGF2214zjjhcVbtOCRHUzOkWvlAaRWWKnWrhVieTobny8jIkOrlhOMNHHPwSPyNgxXHXFiw+213GlrA48TG7TM/r7uklvi5YToZAAAA7ongCh4lNf2MLNl60Nzv3bKW1cMBAACAFyG4gkdZ9nuipKZnSu2q5ST6okpWDwcAAABehOAKHmX+uZLAvlGR4uNDTSAAAADKDsEVPMbRlHRZsT3J3O8bHWH1cAAAAOBlCK7gMb7dnCBnsmzSNDxUGtWsaPVwAAAA4GUIruBxXQLJWgEAAMAKBFfwCPuPnZK1fx8x93tHURIIAACAskdwBY/wzcb9YrOJtKtXVSIrl7N6OAAAAPBCBFfwCLFx+83PPjSyAAAAgEUIruD2diaekC37k8Xf10euuyTc6uEAAADASxFcwe3NP5e16vyv6lK1fKDVwwEAAICXIriCW7PZbBIbfza4oksgAAAArERwBbcWv/e4/HM4VcoF+EnXpjWtHg4AAAC8GMEVPOLaVt2a1ZTyQf5WDwcAAABejOAKbiszyyZfxx8w9ykJBAAAgNUIruC21vx5WA6dTJPKIQFyRaPqVg8HAAAAXo7gCm5fEqjt1wP9OZQBAABgLc5I4ZZOZ2TKos0J5n7fqAirhwMAAAAQXME9rdieKCfSzkh4pWBpW6+q1cMBAAAACK7gnmLPXTi4T1SE+Pr6WD0cAAAAgOAK7if5dIYs35Zo7veJpiQQAAAAroE5V3A7izcnSPqZLGlYo4I0Cw+1ejgAAACAQXAFty0J1EYWPj6UBAIAAMA1EFzBrSQmn5af/jxk7lMSCAAAAFdCcAW38s3GA5JlE4muXVnqVitv9XAAAACAbARXcCux8edKAmlkAQAAABdDcAW38fehFInfc0y083qvlnQJBAAAgGshuILbmH8ua9WpYZhUrxhk9XAAAACAXAiu4BZsNpvMi9tn7veNjrR6OAAAAEA+BFdwC1v2J8uupBQJ9PeVmOY1rR4OAAAAkA/BFdyqJLBr0xpSMTjA6uEAAAAA+RBcweVlZdlk/rkLB/eJoiQQAAAArongCi5v7d9HJCH5tFQM9pcrG1e3ejgAAACAQwRXcHmx57JWPVrUkuAAP6uHAwAAADhEcAWXln4mSxZuOmDu0yUQAAAArszy4Ortt9+WevXqSXBwsLRv317Wrl1b4LpbtmyR/v37m/V9fHxk8uTJ+dZZuXKl9O7dWyIiIsw68+bNK+VPgNK08o8kOX4qw1zXqkODamxsAAAAuCxLg6s5c+bIiBEjZMyYMbJ+/XqJioqSmJgYSUxMdLh+amqqNGjQQCZMmCC1atVyuE5KSop5HQ3a4P5iz3UJ7N0yQvx8faweDgAAAFAgf7HQpEmTZOjQoTJkyBDzeOrUqbJgwQKZNm2aPPnkk/nWb9u2rbkpR8+rHj16mFtxpKWlmZtdcnKy+ZmRkWFusEZK2hlZujXB3O/Zokax94V9ffYhygLHG8oaxxw43uDJMlzoPK44Y7AsuEpPT5d169bJyJEjs5f5+vpK165dZc2aNWU6lvHjx8vYsWPzLV+yZImEhISU6Vjw/35L8pHTGX4SFmyTPfGrZe/Gkm2dpUuXsllRZjjeUNY45sDxBk+21AXO47R6zuWDq0OHDklmZqbUrFkz13J9vG3btjIdiwZ4Wp6YM3NVu3Zt6d69u4SGhpbpWPD/vvpkvR4pcmuHi6XnNQ1L9C2D/oPs1q2bBARw4WGULo43lDWOOXC8wZNluNB5nL2qzeXLAl1FUFCQueWlO9LqnemtDp9Mk1U7D5v7/VrXvqD9wH5EWeJ4Q1njmAPHGzxZgAucjxfn/S1raBEWFiZ+fn5y8ODBXMv1cUHNKuA9Fm5OkMwsm7SIDJWGNSpYPRwAAADAdYOrwMBAufTSS2X58uXZy7Kysszjjh07WjUsuIj5cfvMz75RkVYPBQAAAHD9skCd5zRo0CBp06aNtGvXzly3Slup27sHDhw4UCIjI03DCXsTjK1bt2bf37dvn8TFxUmFChWkYcOzc3JOnjwpO3fuzH6Pv/76y6xTtWpVqVOnjiWfE8Wz92iq/Pr3UfHxEekVFc7mAwAAgFuwNLi65ZZbJCkpSUaPHi0JCQkSHR0tixYtym5ysXv3btNB0G7//v3SqlWr7McTJ040ty5dusiKFSvMst9++02uuuqq7HXsjSo0iJsxY0YZfjqU1NfxB8zP9vWrSnilcmxIAAAAuAXLG1oMGzbM3ByxB0x29erVE5vNVujrXXnlleddB64t1l4SGE1JIAAAANyHZXOuAEe2J5yQbQknJMDPR3q0oLEJAAAA3AfBFVzK/PizWasu/6ohlUMCrR4OAAAAUGQEV3AZWs4ZG7ff3O8bHWH1cAAAAIBiIbiCy1i/+5jsPXpKQgL9pGvTs01NAAAAAI8NrrSpxHPPPWc6+QGlcW2rmOa1pFygHxsXAAAAnh1cPfzww/LVV19JgwYNpFu3bjJ79mxJS0srndHBa5zJzJJvNp5twd6HkkAAAAB4S3ClF+Vdu3atNG3aVB588EEJDw837dTXr19fOqOEx1v952E5nJIuVcsHyuUNw6weDgAAAFB2c65at24tb7zxhrmw75gxY+SDDz6Qtm3bmgsBT5s2jWtNoUTXtup5SbgE+DEVEAAAAF50EeGMjAyZO3euTJ8+XZYuXSodOnSQu+++W/bu3SujRo2SZcuWycyZM507Wnik0xmZsnhzgrlPl0AAAAB4TXClpX8aUM2aNUt8fX1l4MCB8tprr0mTJk2y17n++utNFgsoiuW/J0pKeqZEVi4nretUYaMBAADAO4IrDZq0kcWUKVOkX79+EhAQkG+d+vXry6233uqsMcJLSgK1kYWvr4/VwwEAAADKJrjatWuX1K1bt9B1ypcvb7JbwPkcT82QFduTzH1KAgEAAODOit05IDExUX755Zd8y3XZb7/95qxxwUss2nJA0jOzpHHNitKkVqjVwwEAAADKLrh64IEHZM+ePfmW79u3zzwHFEds3H7zk2tbAQAAwOuCq61bt5o27Hm1atXKPAcU1cHk07Jm12Fzv09UBBsOAAAA3hVcBQUFycGDB/MtP3DggPj7l7izO7zQ1/H7xWYTubRuFaldNcTq4QAAAABlG1x1795dRo4cKcePH89eduzYMXNtK+0iCBTV/PizJYE0sgAAAIAnKHaqaeLEidK5c2fTMVBLAVVcXJzUrFlTPvnkk9IYIzzQrqSTsnHvcfHz9ZHrLgm3ejgAAABA2QdXkZGRsnHjRvnss88kPj5eypUrJ0OGDJEBAwY4vOYVUFjW6vKGYRJWIYiNBAAAALdXoklSeh2re++91/mjgVew2Wwy/1yXQEoCAQAA4ClK3IFCOwPu3r1b0tPTcy3v06ePM8YFD7Z5X7LsOpQiQf6+0r15LauHAwAAAFgTXO3atUuuv/562bRpk/j4+JgshNL7KjMz0zkjg8eKjdtnfnZtVlMqBNFhEgAAAF7aLXD48OFSv359SUxMlJCQENmyZYusXLlS2rRpIytWrCidUcJjZGbZ5OuN50oCubYVAAAAPEix0wZr1qyR7777TsLCwsTX19fcLr/8chk/frw89NBDsmHDhtIZKTzCL38dloPJaRIa7C9dGle3ejgAAACAdZkrLfurWLGiua8B1v79Z7MQ2pp9+/btzhsZPJK9kUXPluES5O9n9XAAAAAA6zJXLVq0MC3YtTSwffv28vLLL0tgYKC899570qBBA+eNDB4n7UymLNx0wNzvExVp9XAAAAAAa4Orp59+WlJSUsz95557Tnr16iVXXHGFVKtWTebMmePc0cGj/LA9SZJPn5FaocHSrn5Vq4cDAAAAWBtcxcTEZN9v2LChbNu2TY4cOSJVqlTJ7hgIOBJ77sLBvaPCxc+XYwUAAABePOcqIyND/P39ZfPmzbmWV61alcAKhTqZdkaWbT1o7veNpiQQAAAAXh5cBQQESJ06dbiWFYptyZYESTuTJQ2ql5fmEaFsQQAAAHicYncLfOqpp2TUqFGmFBAoqnnnugT2jYokywkAAACPVOw5V2+99Zbs3LlTIiIiTPv18uXL53p+/fr1zhwfPEDSiTRZvfOQud8nOsLq4QAAAACuEVz169evdEYCj6Xt1zOzbBJ1USWpH5Y7GAcAAAC8NrgaM2ZM6YwEHis2bp/52YdGFgAAAPBgxZ5zBRTH7sOpsn73MdEu/b1bhrPxAAAA4LGKnbny9fUttCFBZmbmhY4JHuTrjWcbWVx2cTWpERps9XAAAAAA1wmu5s6dm+/aVxs2bJCPPvpIxo4d68yxwc3ZbDaZt2FfdpdAAAAAwJMVO7jq27dvvmU33nijNG/eXObMmSN33323s8YGN7ct4YTsSDwpgX6+EtOiltXDAQAAANxjzlWHDh1k+fLlzno5eIDYc9e2uqpJdalULsDq4QAAAACuH1ydOnVK3njjDYmMpPQLZ2Vl2eTr+HMXDqZLIAAAALxAscsCq1Spkquhhc6rOXHihISEhMinn37q7PHBTa3bfVT2HTslFYL85eomNaweDgAAAOB6wdVrr72WK7jS7oHVq1eX9u3bm8ALyHltq5jmtSQ4wI+NAgAAAI9X7OBq8ODBpTMSeIyMzCxZsPGAud83OsLq4QAAAACuOedq+vTp8sUXX+Rbrsu0HTuwaschOZqaIWEVAs31rQAAAABvUOzgavz48RIWFpZveY0aNeTFF1901rjgASWBvVpGiL+f0xpSAgAAAC6t2Ge+u3fvlvr16+dbXrduXfMcvFtq+hlZsvWgud+HkkAAAAB4kWIHV5qh2rhxY77l8fHxUq0aJWDebtnviZKanim1q5aTVrUrWz0cAAAAwHWDqwEDBshDDz0k33//vWRmZprbd999J8OHD5dbb721dEYJtzH/XElg36jIXF0lAQAAAE9X7G6B48aNk7///luuueYa8fc/++tZWVkycOBA5lx5uaMp6bJie5K5T5dAAAAAeJtiZ64CAwNlzpw5sn37dvnss8/kq6++kj///FOmTZtmniuJt99+W+rVqyfBwcHmellr164tcN0tW7ZI//79zfqaGZk8efIFvyac49vNCXImyyZNw0OlUc2KbFYAAAB4lRK3cmvUqJHcdNNN0qtXL9PMoqQ0UBsxYoSMGTNG1q9fL1FRURITEyOJiYkO109NTZUGDRrIhAkTpFatWk55TTi3SyBZKwAAAHijYgdXmjV66aWX8i1/+eWXTbBVXJMmTZKhQ4fKkCFDpFmzZjJ16lQJCQkxmTBH2rZtK6+88oqZ3xUUFOSU18SF23/slKz9+4i53zuKCwcDAADA+xR7ztXKlSvl2Wefzbe8R48e8uqrrxbrtdLT02XdunUycuTI7GW+vr7StWtXWbNmTXGHVuLXTEtLMze75ORk8zMjI8PccH6xG/aKzSbSpm5lqVHe3yW2m30MrjAWeD6ON3DMwZPxNw7efMxlFGMMxQ6uTp486XBuVUBAQHZQUlSHDh0y3QZr1qyZa7k+3rZtW3GHVuLX1Asjjx07Nt/yJUuWmIwXzu+zjX4i4iP1fQ/LwoULXWqTLV261OohwItwvIFjDp6Mv3HwxmMuNTW19IKrSy65xMxpGj16dK7ls2fPNiV47kizXDpHy06DxNq1a0v37t0lNDTU0rG5g52JJ2Xvmp/E39dHHr3lGqlavmSNTUrjWwb9B9mtWzcT/AMcb/Ak/I0Dxxs8WYYLnccVJ4FU7ODqmWeekRtuuMF0CLz66qvNsuXLl8vMmTPlyy+/LNZrhYWFiZ+fnxw8eDDXcn1cULOK0nhNnbvlaP6W7kird6Y7+HbL2UYhnf9VXWpWLi+uhv0Ijjd4Mv7GgeMNnizABc7Hi/P+xW5o0bt3b5k3b57s3LlT7r//fnnkkUdk37595kLCDRs2LNZraXnhpZdeaoIzO71mlj7u2LFjcYdWaq+JgtlsNomN32/u0yUQAAAA3qzYmSvVs2dPc7OnyWbNmiWPPvqoaSSh852KQ8vxBg0aJG3atJF27dqZ61alpKSYTn9KL04cGRlp5kXZG1Zs3bo1+74GdnFxcVKhQoXs4O58rwnnid97XP45nCrlAvyka9Pc89wAAAAAb1Ki4MreNfDDDz+U//3vfxIREWFKBfXCvcV1yy23SFJSkpnDlZCQINHR0bJo0aLshhS7d+823f7s9u/fL61atcp+PHHiRHPr0qWLrFixokivCedf26pbs5pSPqjEhxMAAADg9op1NqyByowZM0xQpRmrm2++2bQw1zLBC2lmMWzYMHNzxB4w2dWrV8+Uol3Ia8I5MrNs8nX8AXOfkkAAAAB4O9/izLVq3LixbNy40ZTZaQbpzTffLN3RwaWt+fOwHDqZJpVDAuSKRtWtHg4AAADgHpmrb7/9Vh566CH5z3/+I40aNSrdUcGtSgKvuyRcAv2L3RsFAAAA8ChFPiNetWqVnDhxwnTia9++vbz11lvmgr3wTqczMmXR5gRzv29UhNXDAQAAANwnuOrQoYO8//77cuDAAbnvvvvMRYO1kYW2OdcLfGngBe+xYnuinEg7I+GVgqVtvapWDwcAAACwXLFrucqXLy933XWXyWRt2rTJXOdqwoQJUqNGDenTp0/pjBIuJzbu7LWt+kRFiK+vj9XDAQAAACx3QRNltMHFyy+/LHv37jXXuoJ3SD6dIcu3JZr7faIpCQQAAACUU7oQ+Pn5Sb9+/WT+/PlsVS+weHOCpJ/JkoY1Kkiz8FCrhwMAAAC4BFq8odjmx+/PbmTh40NJIAAAAEBwhWJLPHFaVu882yWyb3QkWxAAAAA4h8wVimXBxgOSZRNpVaey1KkWwtYDAAAAziG4Qom6BHJtKwAAACA3gisU2T+HUyRuzzHRzus9W9IlEAAAAMiJ4ApFNv9c1qpTwzCpXjGILQcAAADkQHCFIrHZbDIvbp+5TyMLAAAAID+CKxTJ1gPJ8mdSigT6+0pM85psNQAAACAPgisUqySwa9MaUjE4gK0GAAAA5EFwhfPKyrJlXzi4TxTXtgIAAAAcIbjCef369xE5cPy0VAz2lysbV2eLAQAAAA4QXOG8Ys9lrXq0qCXBAX5sMQAAAMABgisUKv1MlizcdMDcp0sgAAAAUDCCKxTqxx1Jciw1w1zXqkODamwtAAAAoAAEVyhU7Lkugb1bRoifrw9bCwAAACgAwRUKlJJ2RpZuPWju942OYEsBAAAAhSC4QoGW/X5QTmVkSr1qIdLyokpsKQAAAKAQBFco0LwN+8zPPtGR4uNDSSAAAABQGIIrOHT4ZJqs3HHI3O8TRUkgAAAAcD4EV3Bo4eYEycyySYvIUGlYowJbCQAAADgPgis4ND/ubElg36hIthAAAABQBARXyGfv0VT59e+jotOsekWFs4UAAACAIiC4Qj5fxx8wP9vXryrhlcqxhQAAAIAiILhCPrH2ksBoSgIBAACAoiK4Qi7bE07ItoQTEuDnIz1a1GLrAAAAAEVEcIVc5sefzVp1+VcNqRwSyNYBAAAAiojgCtlsNpvExu039/tGc20rAAAAoDgIrpBt/e5jsvfoKQkJ9JOuTWuyZQAAAIBiILhCvmtbxTSvJeUC/dgyAAAAAMEViutMZpZ8s/FsC/Y+lAQCAAAAxUbmCsbqPw/L4ZR0qVo+UC5vGMZWAQAAAIqJ4Aq5rm3V85JwCfDjsAAAAACKi7NoyOmMTFm8OcFsCboEAgAAACVDcAVZ/nuipKRnSmTlctK6ThW2CAAAAFACBFfILgnURha+vj5sEQAAAKAECK683PHUDFmxPcncpyQQAAAAKDmCKy+3aMsBSc/MksY1K0qTWqFWDwcAAABwWwRXXi42br/5ybWtAAAAgAtDcOXFDiafljW7Dpv7faIirB4OAAAA4NYIrrzY1/H7xWYTubRuFaldNcTq4QAAAABujeDKi82PP1sSSCMLAAAA4MIRXHmpXUknZePe4+Ln6yPXXRJu9XAAAAAAt+cSwdXbb78t9erVk+DgYGnfvr2sXbu20PW/+OILadKkiVn/kksukYULF+Z6/uDBgzJ48GCJiIiQkJAQufbaa2XHjh2l/CncM2t1ecMwCasQZPVwAAAAALdneXA1Z84cGTFihIwZM0bWr18vUVFREhMTI4mJiQ7X/+mnn2TAgAFy9913y4YNG6Rfv37mtnnzZvO8zWYzj3ft2iWxsbFmnbp160rXrl0lJSWljD+da9JtNP9cl0BKAgEAAAAPCa4mTZokQ4cOlSFDhkizZs1k6tSpJts0bdo0h+u//vrrJhP12GOPSdOmTWXcuHHSunVreeutt8zzmqH6+eefZcqUKdK2bVtp3LixuX/q1CmZNWtWGX8617R5X7LsOpQiwQG+0r15LauHAwAAAHgEfyvfPD09XdatWycjR47MXubr62uyTGvWrHH4O7pcM105aaZr3rx55n5aWpr5qSWDOV8zKChIVq1aJffcc0++19Tfsf+eSk5ONj8zMjLMzdPMXb/H/LymcQ0J8rV55GdU9s/lqZ8ProXjDRxz8GT8jYM3H3MZxRiDpcHVoUOHJDMzU2rWrJlruT7etm2bw99JSEhwuL4uVzoXq06dOiZge/fdd6V8+fLy2muvyd69e+XAgQMOX3P8+PEyduzYfMuXLFlismieJMsm8r91fiLiI+EZ+2Thwr3i6ZYuXWr1EOBFON7AMQdPxt84eOMxl5qa6h7BVWkICAiQr776yszJqlq1qvj5+ZlMWI8ePcxcI0c0EMuZDdPMVe3ataV79+4SGhoqnuTnXUfk+M+/SaVy/vLfW7tKoL/llaGl+i2D/oPs1q2bOS4Ajjd4Ev7GgeMNnizDhc7j7FVtLh9chYWFmeBHu/vlpI9r1XI8F0iXn2/9Sy+9VOLi4uT48eOm9LB69eqmC2GbNm0cvqaWDOotL92RVu9MZ1uw+ey20/br5ct5R5dAT9yPcF0cb+CYgyfjbxy88ZgLKMb7W5q2CAwMNIHQ8uXLs5dlZWWZxx07dnT4O7o85/pKo1pH61eqVMkEVtrk4rfffpO+ffuKN0s7kykLN50tjewTFWn1cAAAAACPYnlZoJbjDRo0yGSV2rVrJ5MnTzYt07V7oBo4cKBERkaaeVFq+PDh0qVLF3n11VelZ8+eMnv2bBM4vffee7mug6VBlc692rRpk/kdbc+uZX7e7IftSZJ8+ozUCg2WdvWrWj0cAAAAwKNYHlzdcsstkpSUJKNHjzZNKaKjo2XRokXZTSt2795tuv3ZXXbZZTJz5kx5+umnZdSoUdKoUSPTKbBFixbZ62jjCg3atFwwPDzcBGjPPPOMeLvYcxcO7h0VLn6+PlYPBwAAAPAolgdXatiwYebmyIoVK/Itu+mmm8ytIA899JC54f+dTDsjy7aenW/VN5qSQAAAAMDZPLdVHHJZsiVB0s5kSYPq5aV5hGd1QAQAAABcAcGVl4iNO1sS2DcqUnx8KAkEAAAAnI3gygscOpkmq3YeMvf7REdYPRwAAADAIxFceQFtv56ZZZOoiypJ/bDyVg8HAAAA8EgEV15UEtiHRhYAAABAqSG48nB7jqTKun+Oik6z6t0y3OrhAAAAAB6L4MrDzT93bavLLq4mNUKDrR4OAAAA4LEIrjzc/BxdAgEAAACUHoIrD7YtIVm2HzwhgX6+EtOiltXDAQAAADwawZUXNLK4qkl1qVQuwOrhAAAAAB6N4MpDZWXZ/r8kkC6BAAAAQKkjuPJQ63cflX3HTkmFIH+5ukkNq4cDAAAAeDyCKw8vCYxpXkuCA/ysHg4AAADg8fytHgCcLyMzSxZsOmDu942OYBMDAIAL5uPjI2lpaZKZmcnWRKnLyMgQf39/OX36dKkfcwEBAeLn55xkBMGVB1q185AcSUmXsAqB5vpWAAAAJWWz2eTgwYMSHh4uu3fvNkEWUBbHXa1atWTPnj1lcsxVrlzZvN+FvhfBlQeyN7Lo1TJC/P2o/AQAACWXkJAgycnJ5sSzatWqTvuGHyhMVlaWnDx5UipUqCC+vr6lGsSlpqZKYmKieaxfIlwIgisPcyo9UxZvSTD3+1ASCAAALoCWYx07dkyqV69uSqfKlStXqie6QM7gKj09XYKDg0v9mNPjWmmAVaNGjQv6AoF/HR5m2e8HJTU9U2pXLSetale2ejgAAMDN572okJAQq4cClCr7MW4/5kuK4MrDxMbtMz/7RkVSEw0AAJyCeVbwdD5OmtdFcOVBjqaky4rtSeY+XQIBAACAskVw5UG+3ZwgZ7Js0jQ8VBrVrGj1cAAAADxKvXr1ZPLkyUVef8WKFSYjovPW4B0IrjyxJJBGFgAAwItpQFPY7dlnny3R6/76669y7733Fnn9yy67TA4cOCCVKlWSstKkSRMJCgoyXR5R9giuPMT+Y6dk7d9HzP3eUVw4GAAAeC8NaOw3zTSFhobmWvboo4/masV95syZIr2udk0sTnOPwMBAp1w7qahWrVolp06dkhtvvFE++ugjsVrGBTaHcEcEVx7im437xWYTaVevqkRWPttOEgAAoFSuC5R+xpKbvndRaEBjv2nWSIMb++Nt27ZJxYoV5dtvv5VLL73UZHk0KPnzzz+lb9++UrNmTXNtpbZt28qyZcsKLQvU1/3ggw/k+uuvN0FXo0aNZP78+QWWBc6YMcNcrHbx4sXStGlT8z7XXnutCfjsNNB76KGHzHrVqlWTJ554QgYNGiT9+vU77+f+8MMP5bbbbpM777xTpk2blu/5vXv3yoABA8z1ysqXLy9t2rSRX375Jfv5r7/+2nxubX8eFhZmPlfOzzpv3rxcr6dj1M+k/v77b7POnDlzpEuXLuY1PvvsMzl8+LB5z8jISLONLrnkEpk1a1a+tusvv/yyNGzY0OyPOnXqyIsvvmie69q1qwwbNizX+klJSSZwXb58ubgarnPlIWLPXTiYa1sBAIDSdCojU5qNXmzJRt76XIyEBDrn9PXJJ5+UiRMnSoMGDaRKlSqyZ88eue666+SFF14wJ/gff/yx9O7dW7Zv325O9gsyduxYExi88sor8uabb8rtt98u//zzjwlgHNEL1ur7fvLJJ+b6TXfccYfJpGkgol566SVzf/r06SYAe/31101Qc9VVVxX6eU6cOCFffPGFCZa0NPD48ePy448/yhVXXGGe1wvyatCjQY4GgBporl+/3gQ2asGCBSaYeuqpp8xn12tMLVy4sETb9dVXX5VWrVqZAOv06dMmiNUgUTOI+j4a/F188cXSrl078zsjR46U999/X1577TW5/PLLTbC5detW89xdd91lgk19Td0v6tNPPzWf4+qrrxZXQ3DlAXYmnpAt+5PF39dHrrvkwq4qDQAA4A2ee+456datW/ZjDYaioqKyH48bN07mzp1rApG8mZOcBg8ebDIzSrMtb7zxhqxdu9ZkpAoqlZs6daoJLpS+to7FTgM0DTbsWaO33nqrSEHO7NmzTeasefPm5vGtt95qMln24GrmzJkm46PzxuyBn2aK7DSo1N/RYNEu5/YoqocfflhuuOGGXMtylmE++OCDJnP3+eefm+BKg0INIPVzaoZO6bbR+WrJycnmtTS4io2NlZtvvtk8r9ky3e6ueIkAgisPMP9c1qrzv6pL1fKBVg8HAAB4sHIBfiaDZNV7O4uWxOWkmR1tdKGZFc2caHmezl/avXt3oa/TsmXL7PtaaqfZmcTExALX19I4e2ClwsPDs9fXbNPBgwezMzrKz8/PZH7sGaaCaBmgZsHs9L5mqjRY0zLIuLg4k00qKKOmzw8dOlScvV0zMzNN0KnB1L59+0xGLC0tLXvu2u+//24eX3PNNQ5fT7Nf9jJHDa4027Z58+Zc5ZeuhODKzWntcWz82eCKLoEAAKC0abbAWaV5VtJAKG92ZenSpaZkTzM65cqVM40hNBgoTEBAQL7tU1gg5Gj9os4lK4iW0P38888mY6bldzkDG81oadCkn6cw53ve0TgdNawon2e7armkZqZ0rprOt9LnNbtl367ne191zz33SHR0tJkzpuWSWg5Yt25dcUU0tHBz8XuPyz+HU803OV2b1rR6OAAAAG5p9erVptRMy/E0CNA5SdqkoSxp8w1tqKGlezkDJM3WFEbL/zp37izx8fEmA2W/jRgxwjxnz7DpsiNHznaXzkufL6xBhHZKzNl4Y8eOHWb+WFG2a9++fU0mTcsMdY7bH3/8kf28ljJqgFXYe+v+0IyYzsvS8kadh+Wq3P9rBy9nv7ZVt2Y1pXwQuxMAAKAk9CT/q6++Mk0sNEvzzDPPnLcUrzTonKTx48eb7Jk2ptCyvqNHjxY4v0izR9ocQ+dttWjRIl/GZ9KkSbJlyxYzL0zL87TroL6+liNu2LBBIiIipGPHjjJmzBhTmqclizr3Sssida6XPROm2SKdF6XrasCny/Nm4Qrarl9++aX89NNPpnGIjkdLH5s1a5Zd9qev9fjjj5sOgJ06dTJzwzZt2iQ33XRTrs+i89M085Wzi6GrIXPlxjKzbPJ1/NlvECgJBAAAKDk96deTf22koAFWTEyMtG7dusw3qQYaGggNHDjQBDLarl3HokGIIzr3SNudOwo4tNug3jR7pYHLkiVLpEaNGqYromaDJkyYYOZ0qSuvvNJ0G9TX0xI8Daa0zNBOu/XVrl3bNMjQdu9aRlmUa349/fTTZjvqZ9D30Ixg3rbyGsg+8sgjMnr0aDPeW265xQRYOek28ff3Nz8L2hauwMd2oUWeHkg7k2haVicV6qREV7VqxyG548NfpHJIgKwd1VUC/YmV836To9+46B+QonyzAlwIjjeUNY45lAVto/3XX3+Z+S06R0bPi7R9OMqOZs804NBmDtrB0Js+d3JycvYxpyWamlXTksnSCHrtx3r9+vXzBW/FiQ2oI/OAkkBtv05gBQAA4P70GlmaYdJOf9pFT0vx9KRfs0Xe+kXS0aNHTQasQ4cOlmQTi4OvHtzU6YxMWbQ5wdzvGxVh9XAAAADgBJql0es4tW3b1sw/0rlHy5YtM9krb7R69WozP0wzVnp9MFdH5spNrdieKCfSzkh4pWBpW8/x9QoAAADgXnRekwYUOEvnabnTLCYyV24q9tyFg/tERYivr+tdnRoAAADwNgRXbij5dIYs33b2St59oyOtHg4AAAAAgiv3tHhzgqSfyZJGNSpI0/CKVg8HAAAAAMGVe5ofvz/72lYFXVAOAAAAQNmiLNDNJJ44Lat3HjL3+0RREggAAAC4CoIrN7Ng4wHJsom0qlNZ6lQ7/1WxAQAAAJQNgis37RLIta0AAABKvw34ww8/nP24Xr16Mnny5EJ/R6dszJs374Lf21mvg7JFcOVG/jmcInF7jol2Xu/ZkgsHAwAAONK7d2+59tprHT73448/msBl48aNxd54eiHbe++916kb/dlnn5Xo6Oh8yw8cOCA9evSQsnDq1CmpWrWqhIWFSVpaWpm8p6ciuHIj889lrTo1DJPqFYOsHg4AAIBLuvvuu2Xp0qWyd+/efM9Nnz5d2rRpIy1btiz261avXl1CQspmWkatWrUkKKhszvf+97//SfPmzaVJkyaWZ8tsNpucOXNG3BXBlZvQA21e3D5zn2tbAQAAy9hsIukp1tz0vYugV69eJhCaMWNGruUnT56UL774wgRfhw8flgEDBkhkZKQJmC655BKZNWtWoa+btyxwx44d0rlzZwkODpZmzZqZgC6vJ554Qv71r3+Z92jQoIE888wzkpGRYZ7T8Y0dO1bi4+NNNk1v9jHnLQvctGmTXH311VKuXDmpVq2ayaDp57EbPHiw9OvXTyZOnCjh4eFmnQceeCD7vQrz4Ycfyh133GFuej+vLVu2mG0aGhoqFStWlCuuuEL+/PPP7OenTZtmgrOgoCDz3sOGDTPL//77b/M54uListc9duyYWbZixQrzWH/q42+//VYuvfRS8xqrVq0yr3/bbbeZ16tQoYK0bdtWli1blmtcmmXT7Vu7dm3zew0bNjTj1/Nmva/bIicdh77Xzp07pbT4l9orw6m2HkiWP5NSJNDfV2Ka12TrAgAAa2Skirxo0fSEUftFAsufdzV/f38ZOHCgCVSeeuqp7EvXaGCVmZlpgioNTPRkXk/ONWhYsGCB3HnnnXLxxRdLu3btzvseWVlZcsMNN0jNmjXll19+kePHj+ean2WnwYiOIyIiwgRIQ4cONcsef/xxueWWW2Tz5s2yaNGi7MChUqVK+V4jJSVFYmJipGPHjqY0MTExUe655x4TxOQMIL///nsTjOhPDSD09bXkUN+zIBrErFmzRr766isTlPz3v/+Vf/75R+rWrWue37dvnwkgdf7Zd999Z7bV6tWrs7NLU6ZMkREjRsiECRNMGaNuB32+uJ588kkTDGkAWqVKFTOGbt26mdfVgPLjjz825Z7bt2+XOnXqmN/Rfaxjf+ONNyQqKkr++usvOXTokNnfd911l8lSPvroo9nvoY/1s2jgVVoIrtysJLBr0xpSMTjA6uEAAAC4ND25fuWVV+SHH34wgYH95Lp///4mgNFbzhPvBx98UBYvXiyff/55kYIrDYa2bdtmfkcDJ/Xiiy/mmyf19NNP58p86XvOnj3bBFcaNGhWRoNBLQMsyMyZM+X06dMmwChf/mxw+dZbb5lg46WXXjIBntKgRJf7+fmZEr+ePXvK8uXLCw2uNOukY9bfVRrE6XbSuWDq7bffNttKxxwQcPYcVDNxds8//7w88sgjMnz48OxlmmUqrueee84EU3aVK1eW+vXrm2DO19dXxo0bJ3PnzpX58+eboPKPP/4w+0qzhV27djW/o4FZzkze6NGjZe3atWZ/agZPt2PebJazEVy5gawsW/aFg7m2FQAAsFRAyNkMklXvXUQaXFx22WUmeNDgSjM52sxCT+KVZrA0GNITdM3OpKenmzKzos6p+v333005mj2wUppZymvOnDkms6IZIs2WacZHA4bi0PfSzIw9sFKdOnUy2TPN5NiDKy3N08DKTrNYmi0riG6Djz76SF5//fXsZVoaqAGgBiYa1GgpnZYB2gOrnDSDtn//frnmmmvkQrVp0ybXY91WWkKpQaw299Dtpo03du/ebZ7Xceln7dKli8PX0/2iwaXufw2uvv76a7N/b7rpJilNzLlyA7/+fUQOHD8tFYP95crG1a0eDgAA8GZaYqeleVbczpX3FZXOrdJmDSdOnDDZGC35s5+Ma1ZLgwotC9QyOj1Z16yNBlnOoiVrt99+u1x33XXyzTffyIYNG0yZojPfI6e8AZCWx2kAVhDNumlgqeWDmj3T26233mpK8jTjpTS7VpDCnlManCktN7QraA5YzsBRPfbYY2abaWZMg2LdPzovzr7tzvfeSksnNeOmQZnuf/2cpd2QxCWCK003appUJwO2b9/epO8Ko/Wy+m2Erq8beeHChfkiXU0XXnTRRWbD6wTDqVOniruKPZe16tGilgQH/P+3EQAAACjYzTffbE7wtRxMS+q0VNA+/0rnBfXt29dkajQrpCVlWmpWVE2bNpU9e/aYrIrdzz//nGudn376ycxd0oBKMzONGjUygUtOgYGBJoN0vvfSphc698pOx6+frXHjxiU+BLT5gwZTGrjkvOkye2ML7aqowY2joEjnjuk5vD0Qy0ubiqic2yhnc4vC6LbThhbXX3+9Od/XskltkGGnyzRw1LLPgmhQq0GbzgvTeW26/0ub5cGVpkp1EtyYMWNk/fr15uDWbw00zVjQhtZJiPpNhEb/2hVFbzoZ0E5fTzfgp59+atKoOrlQgy2t0XQ36WeyZOGmswckXQIBAACKTuczabZi5MiR5gRf5+HYaaCj83X03FLPF++77z45ePBgkV9b5/no3KNBgwaZwEcDEA2ictL30DI2zZ5oWaCWB+q8oZw0ONFGDBp0aDMGR9eZ0uyXJhX0vfScVzNtOkdMG3DYSwKLKykpyZTK6Wu2aNEi100bRWinwiNHjphz6OTkZBNw/fbbb6ZD4ieffGLKEZXOzXr11VfNZ9uxY4c5n3/zzTfNc5rk6NChg2lKodtYA6Gcc9AKo00ndHy6XXT7aqCVMwun203HrgGTjlW3oXYe1DJPOy0b1H2u+1/3haOyTY8LriZNmmQm2Q0ZMiQ7w6TpOq2PdETTt3pROE0VahSvk9tat25tJu/Z6T8S3dhaX6sbXltVatB2voyYK/pxR5IcS80w17Xq0KCa1cMBAABwK/qF/NGjR82X9znnR+lJvp5D6nI9Z9TMiH5hX1SaNdJASUvOdE6PlqC98MILudbp06eP6b6nAYp27dNzVJ1HlJM22NBz26uuuspkehy1g9dzYy3h02BHm0XceOONZp5TzvPf4rI3x3A0X0qXaWCkiQpt6a5dArUyTEsqtcPi+++/n12CqOfc2p7+nXfeMXO+tGW7Bll2ek6v86X09zThoWV+RaEBmza1uPzyy03jDt1Pur9y0oyUbov777/fVLVpTJEzu2ff/1pKqLFGWfCx5SyCLGP6QfVg+fLLL3MdzLqTtAd+bGxsvt/R1ouamcrZ6lKzXhqxalSrNJjSrJYu039EGsXqwa0tNrX9Yl76DUHObwk0OtcJivrtQXEnHDrbfz/fKN9sSpDBHevIU9c1sXQs7kbT1/qNlHaecTQJE+B4gzvjbxzKgnao09I3LW3TY07LwOxldUBpstlsZq7chR5zmlHUc0Etxywsy6fHupYdagygWcKcNDYICwszbebPFxtY2i1QgxetMc37QfWxtrZ0JCEhweH6utxOU5EaYOmcK52Yp98saITtKLBS48ePNxdwy2vJkiVldhXugtiO+Ui1IF+penKXLFy4y9KxuCtHF/QDON7gKfgbh9JkbxGu2QCdG6Qnu0BZOlHCY04TJxpraNdDnVunmTgNkgpL+mgWcuXKldnX8LJLTU317lbsGlzphEKdY6XftOhG0itUaxbL3gc/J63D1GxY3sxV9+7dLc9cXZejwwrfFBUP3+qiLHG8oaxxzKEsM1daPkbmCu6UuZoxY4YpE9RyTC1vPN85vR7rGoBpMsZR5sotgitNr+lEs7yTB/VxQRdS0+WFra8R56hRo0wNrPa2t3c50clwetEwR8FVUFCQueWlpWSUk7k/9iM43uDJ+BuH0qQVRnpiaz+51Z/29tpAaco617yipMecNrooTndAfQ99L0d/U4sTD1j6r0PTyzq5LWf7Rt2Q+rigbh66PG+7Ry2JsK+v36roLe9O0CCusD7/AAAAAHAhLC8L1HI8bWChvf+104p2G9G6XntHD20FGRkZaeZFqeHDh5tOJdpBRDNT2tpS20K+99575nlN+enz2k1QU3taFqhtH7UjinYmBAAAQPFY2P8McKtj3PLgSq89oH32dbKZNqXQuki9RpW9aYVeGyBnFuqyyy4zF4LT9pla/qc967UroPbkt9OAS+dR6TUBtGWlBljaGvPf//63JZ8RAADAHdnLoXRCP1Ml4MlSzzWtuNDj3PLgSmnvf705om3U87rpppvMrSA6/2r69OlOHSMAAIC30WkVeq0h/SJcGwvoiacuA0pbVlaW6eCnjSZKc56fZqw0sEpMTDTH+oUe3y4RXAEAAMA16ZfW2tjiwIEDpnsb3YtRFmw2m2lUp9N8yuKY08CqoIZ6xUFwBQAAgALpia1O11i/fr1cffXV5tpXQGnLyMgwl1PS1uilXZLqzIws/zoAAABQpEyCXrqGuVcoC35+fuZivnrNKXc65rhQAQAAAAA4AcEVAAAAADgBwRUAAAAAOAFzrgq5iFhycrIztjEsnAiprTV1P7pTrS7cE8cbOObgyfgbB28+5pLPxQRFudAwwZUD2mZU1a5d29n7BgAAAICbxgiVKlUqdB0fW1FCMC+8aNn+/fvNxfK4loP70m8ZNEDes2ePhIaGWj0ceDiON3DMwZPxNw7efMzZbDYTWEVERJz3gsZkrhzQjXbRRReV1v5BGdN/kFb/o4T34HgDxxw8GX/j4K3HXKXzZKzsaGgBAAAAAE5AcAUAAAAATkBwBY+lV5EfM2aM+QlwvMHT8DcOHG/wZEFueh5HQwsAAAAAcAIyVwAAAADgBARXAAAAAOAEBFcAAAAA4AQEVwAAAADgBARX8Cjjx4+Xtm3bSsWKFaVGjRrSr18/2b59u9XDgheZMGGC+Pj4yMMPP2z1UOCh9u3bJ3fccYdUq1ZNypUrJ5dccon89ttvVg8LHiozM1OeeeYZqV+/vjneLr74Yhk3bpzYbDarhwYPsXLlSundu7dERESY/3/Omzcv1/N6rI0ePVrCw8PNMdi1a1fZsWOHuCqCK3iUH374QR544AH5+eefZenSpZKRkSHdu3eXlJQUq4cGL/Drr7/Ku+++Ky1btrR6KPBQR48elU6dOklAQIB8++23snXrVnn11VelSpUqVg8NHuqll16SKVOmyFtvvSW///67efzyyy/Lm2++afXQ4CFSUlIkKipK3n77bYfP6/H2xhtvyNSpU+WXX36R8uXLS0xMjJw+fVpcEa3Y4dGSkpJMBkuDrs6dO1s9HHiwkydPSuvWreWdd96R559/XqKjo2Xy5MlWDwse5sknn5TVq1fLjz/+aPVQ4CV69eolNWvWlA8//DB7Wf/+/U0G4dNPP7V0bPA8Pj4+MnfuXFN5ZM9aaUbrkUcekUcffdQsO378uDkmZ8yYIbfeequ4GjJX8Gj6D1BVrVrV6qHAw2nGtGfPnqZcASgt8+fPlzZt2shNN91kvjhq1aqVvP/++2xwlJrLLrtMli9fLn/88Yd5HB8fL6tWrZIePXqw1VHq/vrrL0lISMj1/9ZKlSpJ+/btZc2aNS65B/ytHgBQWrKyssy8Fy2hadGiBRsapWb27Nmyfv16UxYIlKZdu3aZEq0RI0bIqFGjzDH30EMPSWBgoAwaNIiNj1LJliYnJ0uTJk3Ez8/PzMF64YUX5Pbbb2dro9QlJCSYn5qpykkf259zNQRX8OhMwubNm803bEBp2bNnjwwfPtzM8QsODmZDo9S/NNLM1Ysvvmgea+ZK/87pXASCK5SGzz//XD777DOZOXOmNG/eXOLi4swXl1qqxTEH5EdZIDzSsGHD5JtvvpHvv/9eLrroIquHAw+2bt06SUxMNPOt/P39zU3n+OnkW72v3/ICzqLdspo1a5ZrWdOmTWX37t1sZJSKxx57zGSvdG6Ldqa888475b///a/pzguUtlq1apmfBw8ezLVcH9ufczUEV/AoOvFRAyudDPndd9+Z1rFAabrmmmtk06ZN5ttc+00zC1oyo/e1jAZwFi1zznt5CZ0LU7duXTYySkVqaqr4+uY+XdS/a5pFBUpb/fr1TRCl8/7stExVuwZ27NjRJXcAZYHwuFJALV2IjY0117qy1+Pq5EftbAQ4mx5neef0aZtYvQYRc/3gbJox0AYDWhZ48803y9q1a+W9994zN6A06PWHdI5VnTp1TFnghg0bZNKkSXLXXXexweG0brs7d+7M1cRCv5zUZmR63GkZqnbhbdSokQm29LprWpZq7yjoamjFDo9r4enI9OnTZfDgwWU+HninK6+8klbsKDVa8jxy5EhzEU090dDmFkOHDmWLo1ScOHHCnMxqRYiWQOtJ7YABA8xFXbWRCnChVqxYIVdddVW+5TqnT9uta1XSmDFjzJdIx44dk8svv9xc9uRf//qXS258gisAAAAAcALmXAEAAACAExBcAQAAAADBFQAAAAC4BjJXAAAAAOAEBFcAAAAA4AQEVwAAAADgBARXAAAAAOAEBFcAAAAA4AQEVwAAOJmPj4/MmzeP7QoAXobgCgDgUQYPHmyCm7y3a6+91uqhAQA8nL/VAwAAwNk0kJo+fXquZUFBQWxoAECpInMFAPA4GkjVqlUr161KlSrmOc1iTZkyRXr06CHlypWTBg0ayJdffpnr9zdt2iRXX321eb5atWpy7733ysmTJ3OtM23aNGnevLl5r/DwcBk2bFiu5w8dOiTXX3+9hISESKNGjWT+/Pll8MkBAFYiuAIAeJ1nnnlG+vfvL/Hx8XL77bfLrbfeKr///rt5LiUlRWJiYkww9uuvv8oXX3why5YtyxU8aXD2wAMPmKBLAzENnBo2bJjrPcaOHSs333yzbNy4Ua677jrzPkeOHCnzzwoAKDs+NpvNVobvBwBAqc+5+vTTTyU4ODjX8lGjRpmbZq7+/e9/mwDJrkOHDtK6dWt555135P3335cnnnhC9uzZI+XLlzfPL1y4UHr37i379++XmjVrSmRkpAwZMkSef/55h2PQ93j66adl3Lhx2QFbhQoV5Ntvv2XuFwB4MOZcAQA8zlVXXZUreFJVq1bNvt+xY8dcz+njuLg4c18zWFFRUdmBlerUqZNkZWXJ9u3bTeCkQdY111xT6BhatmyZfV9fKzQ0VBITEy/4swEAXBfBFQDA42gwk7dMz1l0HlZRBAQE5HqsQZkGaAAAz8WcKwCA1/n555/zPW7atKm5rz91LpaW8tmtXr1afH19pXHjxlKxYkWpV6+eLF++vMzHDQBwbWSuAAAeJy0tTRISEnIt8/f3l7CwMHNfm1S0adNGLr/8cvnss89k7dq18uGHH5rntPHEmDFjZNCgQfLss89KUlKSPPjgg3LnnXea+VZKl+u8rRo1apiugydOnDABmK4HAPBeBFcAAI+zaNEi0x49J806bdu2LbuT3+zZs+X+++83682aNUuaNWtmntPW6YsXL5bhw4dL27ZtzWPtLDhp0qTs19LA6/Tp0/Laa6/Jo48+aoK2G2+8sYw/JQDA1dAtEADgVXTu09y5c6Vfv35WDwUA4GGYcwUAAAAATkBwBQAAAABOwJwrAIBXsdlsVg8BAOChyFwBAAAAgBMQXAEAAACAExBcAQAAAIATEFwBAAAAgBMQXAEAAACAExBcAQAAAIATEFwBAAAAgBMQXAEAAACAXLj/A/1IN2vZfPDbAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 1000x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot Training Curves\n",
    "# Visualises training and validation loss/accuracy across epochs.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Extract metrics\n",
    "train_loss = history.history[\"loss\"]\n",
    "val_loss = history.history[\"val_loss\"]\n",
    "train_acc = history.history[\"accuracy\"]\n",
    "val_acc = history.history[\"val_accuracy\"]\n",
    "\n",
    "epochs = range(1, len(train_loss) + 1)\n",
    "\n",
    "# Plot Loss\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(epochs, train_loss, label=\"Training Loss\")\n",
    "plt.plot(epochs, val_loss, label=\"Validation Loss\")\n",
    "plt.title(\"Training vs Validation Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Plot Accuracy\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(epochs, train_acc, label=\"Training Accuracy\")\n",
    "plt.plot(epochs, val_acc, label=\"Validation Accuracy\")\n",
    "plt.title(\"Training vs Validation Accuracy\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37f734e1",
   "metadata": {},
   "source": [
    "### 6.8 Save Model and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "8fe7f1d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and tokenizer saved successfully.\n"
     ]
    }
   ],
   "source": [
    "# Save Model and Tokenizer\n",
    "# Saves the trained Seq2Seq model and tokenizer for future use.\n",
    "\n",
    "import pickle\n",
    "\n",
    "# Save the trained model\n",
    "model.save(\"seq2seq_summarisation_model.h5\")\n",
    "\n",
    "# Save the tokenizer\n",
    "with open(\"tokenizer.pkl\", \"wb\") as f:\n",
    "    pickle.dump(tokenizer, f)\n",
    "\n",
    "print(\"Model and tokenizer saved successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "514f814a",
   "metadata": {},
   "source": [
    "___________________________________________________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "913756f3",
   "metadata": {},
   "source": [
    "# 7.0 Evaluate the Summarisation Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "863cebaa",
   "metadata": {},
   "source": [
    "### 7.1 Generate Summaries for Test Stories (Batch Mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "e6a3902c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 4ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 6ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 7ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "1/1 [==============================] - 0s 7ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 6ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "Generated summaries for all test samples.\n"
     ]
    }
   ],
   "source": [
    "# Generate Summaries for Test Stories (Batch Mode)\n",
    "# Produces model summaries for the entire test set.\n",
    "\n",
    "generated_summaries = []\n",
    "\n",
    "for i in range(len(X_val)):\n",
    "    input_seq = X_val[i:i+1]\n",
    "    pred = generate_summary(input_seq)\n",
    "    generated_summaries.append(pred)\n",
    "    \n",
    "print(\"Generated summaries for all test samples.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cc15f8f",
   "metadata": {},
   "source": [
    "### 7.2 Compute Evaluation Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea8e267c",
   "metadata": {},
   "source": [
    "#### 7.2.1 BLEU Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "7240a096",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average BLEU Score: 0.0005783997547556235\n"
     ]
    }
   ],
   "source": [
    "# Compute BLEU Score\n",
    "\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "\n",
    "smooth = SmoothingFunction().method1\n",
    "bleu_scores = []\n",
    "\n",
    "for ref, pred in zip(output_summaries[len(X_train):], generated_summaries):\n",
    "    reference = ref.split()\n",
    "    candidate = pred.split()\n",
    "    score = sentence_bleu([reference], candidate, smoothing_function=smooth)\n",
    "    bleu_scores.append(score)\n",
    "\n",
    "average_bleu = sum(bleu_scores) / len(bleu_scores)\n",
    "print(\"Average BLEU Score:\", average_bleu)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66692014",
   "metadata": {},
   "source": [
    "#### 7.2.2 ROUGE Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "b5ccd9a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average ROUGE-1: 0.003125\n",
      "Average ROUGE-2: 0.0\n",
      "Average ROUGE-L: 0.003125\n"
     ]
    }
   ],
   "source": [
    "from rouge_score import rouge_scorer\n",
    "\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "\n",
    "rouge1_scores = []\n",
    "rouge2_scores = []\n",
    "rougeL_scores = []\n",
    "\n",
    "for ref, pred in zip(output_summaries[len(X_train):], generated_summaries):\n",
    "    scores = scorer.score(ref, pred)\n",
    "    rouge1_scores.append(scores['rouge1'].fmeasure)\n",
    "    rouge2_scores.append(scores['rouge2'].fmeasure)\n",
    "    rougeL_scores.append(scores['rougeL'].fmeasure)\n",
    "\n",
    "print(\"Average ROUGE-1:\", sum(rouge1_scores) / len(rouge1_scores))\n",
    "print(\"Average ROUGE-2:\", sum(rouge2_scores) / len(rouge2_scores))\n",
    "print(\"Average ROUGE-L:\", sum(rougeL_scores) / len(rougeL_scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e0b0f5c",
   "metadata": {},
   "source": [
    "#### 7.2.3 Perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "3fb173da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Perplexity: 126.2369872642106\n"
     ]
    }
   ],
   "source": [
    "# Compute Perplexity \n",
    "\n",
    "import math\n",
    "\n",
    "final_loss = history.history[\"loss\"][-1]\n",
    "perplexity = math.exp(final_loss)\n",
    "\n",
    "print(\"Model Perplexity:\", perplexity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceac47f0",
   "metadata": {},
   "source": [
    "### 7.3 Show Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "1283e3d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 1\n",
      "------------------------------------------------------------\n",
      "Input paragraph:\n",
      "\"No, I don't think I have anything rarer than a Crown Derby tea-set.\"\n",
      "\n",
      "Reference summary:\n",
      "\"No, I don't think I have anything rarer than a Crown Derby tea-set.\"\n",
      "\n",
      "Model-generated summary:\n",
      "am  am  am  am  am  am  am  am  am  am  am  am  am  am \n",
      "\n",
      "Example 2\n",
      "------------------------------------------------------------\n",
      "Input paragraph:\n",
      "\"That would hardly justify all this mystery. Besides, why should they\n",
      "     not openly state what they want? If they covet your tea-set, they can\n",
      "     surely offer a price for it without buying you out, lock, stock, and\n",
      "     barrel. No, as I read it, there is something which you do not know\n",
      "     that you have, and which you would not give up if you did know.\"\n",
      "\n",
      "Reference summary:\n",
      "\"That would hardly justify all this mystery.\n",
      "\n",
      "Model-generated summary:\n",
      "am was  am  am  am  am  am  am  am  am  am  am  am  am  am\n",
      "\n",
      "Example 3\n",
      "------------------------------------------------------------\n",
      "Input paragraph:\n",
      "\"That is how I read it,\" said I.\n",
      "\n",
      "Reference summary:\n",
      "\"That is how I read it,\" said I.\n",
      "\n",
      "Model-generated summary:\n",
      "am  am  am  am  am  am  am  am  am  am  am  am  am  am \n"
     ]
    }
   ],
   "source": [
    "# Show Example Summaries from Test Set\n",
    "\n",
    "num_examples = 3  # number of examples to display\n",
    "\n",
    "for i in range(num_examples):\n",
    "    print(f\"\\nExample {i+1}\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "    # Input paragraph (test sample)\n",
    "    input_text = input_texts[len(X_train) + i]\n",
    "    print(\"Input paragraph:\")\n",
    "    print(input_text)\n",
    "\n",
    "    # Reference (ground truth) summary\n",
    "    reference_summary = output_summaries[len(X_train) + i]\n",
    "    print(\"\\nReference summary:\")\n",
    "    print(reference_summary)\n",
    "\n",
    "    # Model-generated summary\n",
    "    generated_summary = generated_summaries[i]\n",
    "    print(\"\\nModel-generated summary:\")\n",
    "    print(generated_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d11dfbd",
   "metadata": {},
   "source": [
    "___________________________________________________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7083b2af",
   "metadata": {},
   "source": [
    "# 8.0 Semantic Search with Embeddings and ChromaDB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "565c0518",
   "metadata": {},
   "source": [
    "### 8.1 Split stories into paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "fece3579",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All stories loaded successfully.\n",
      "Total characters: 13820505\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "folder = r\"C:\\Users\\Dirhem\\Deep Learning 202526\\Assessment\\Sherlock Holmes Stories\\sherlock\"\n",
    "\n",
    "# Read all .txt files and combine them\n",
    "full_text = \"\"\n",
    "\n",
    "for filename in os.listdir(folder):\n",
    "    if filename.endswith(\".txt\"):\n",
    "        file_path = os.path.join(folder, filename)\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            full_text += f.read() + \"\\n\\n\"   # add spacing between stories\n",
    "\n",
    "print(\"All stories loaded successfully.\")\n",
    "print(\"Total characters:\", len(full_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "83e78596",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total paragraphs: 55726\n",
      "Sample paragraph:\n",
      "THE ADVENTURE OF THE THREE GABLES\n"
     ]
    }
   ],
   "source": [
    "# Split stories into paragraphs\n",
    "# Assumes `full_text` contains the entire Sherlock Holmes story as a single string.\n",
    "\n",
    "import re\n",
    "\n",
    "# Simple paragraph split on double newlines\n",
    "paragraphs = [p.strip() for p in re.split(r\"\\n\\s*\\n\", full_text) if p.strip()]\n",
    "\n",
    "print(f\"Total paragraphs: {len(paragraphs)}\")\n",
    "print(\"Sample paragraph:\")\n",
    "print(paragraphs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61ceaed5",
   "metadata": {},
   "source": [
    "### 8.2 Generate Embeddings Using all-MiniLM-L6-v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "4953241c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentence-transformers in c:\\users\\dirhem\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (2.2.2)\n",
      "Requirement already satisfied: chromadb in c:\\users\\dirhem\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (0.4.14)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in c:\\users\\dirhem\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sentence-transformers) (4.35.2)\n",
      "Requirement already satisfied: tqdm in c:\\users\\dirhem\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sentence-transformers) (4.67.1)\n",
      "Requirement already satisfied: torch>=1.6.0 in c:\\users\\dirhem\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sentence-transformers) (2.9.1)\n",
      "Requirement already satisfied: torchvision in c:\\users\\dirhem\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sentence-transformers) (0.24.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\dirhem\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sentence-transformers) (1.26.4)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\dirhem\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sentence-transformers) (1.7.2)\n",
      "Requirement already satisfied: scipy in c:\\users\\dirhem\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sentence-transformers) (1.10.1)\n",
      "Requirement already satisfied: nltk in c:\\users\\dirhem\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sentence-transformers) (3.9.2)\n",
      "Requirement already satisfied: sentencepiece in c:\\users\\dirhem\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sentence-transformers) (0.2.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.4.0 in c:\\users\\dirhem\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sentence-transformers) (0.16.4)\n",
      "Requirement already satisfied: filelock in c:\\users\\dirhem\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (3.20.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\dirhem\\appdata\\roaming\\python\\python310\\site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\dirhem\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\dirhem\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2025.11.3)\n",
      "Requirement already satisfied: requests in c:\\users\\dirhem\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2.32.5)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in c:\\users\\dirhem\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.15.2)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in c:\\users\\dirhem\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.7.0)\n",
      "Requirement already satisfied: fsspec in c:\\users\\dirhem\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2026.1.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\dirhem\\appdata\\roaming\\python\\python310\\site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (4.15.0)\n",
      "Requirement already satisfied: pydantic>=1.9 in c:\\users\\dirhem\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from chromadb) (2.12.5)\n",
      "Requirement already satisfied: chroma-hnswlib==0.7.3 in c:\\users\\dirhem\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from chromadb) (0.7.3)\n",
      "Requirement already satisfied: fastapi>=0.95.2 in c:\\users\\dirhem\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from chromadb) (0.128.0)\n",
      "Requirement already satisfied: uvicorn>=0.18.3 in c:\\users\\dirhem\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.40.0)\n",
      "Requirement already satisfied: posthog>=2.4.0 in c:\\users\\dirhem\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from chromadb) (5.4.0)\n",
      "Requirement already satisfied: pulsar-client>=3.1.0 in c:\\users\\dirhem\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from chromadb) (3.9.0)\n",
      "Requirement already satisfied: onnxruntime>=1.14.1 in c:\\users\\dirhem\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from chromadb) (1.23.2)\n",
      "Requirement already satisfied: pypika>=0.48.9 in c:\\users\\dirhem\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from chromadb) (0.48.9)\n",
      "Requirement already satisfied: overrides>=7.3.1 in c:\\users\\dirhem\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from chromadb) (7.7.0)\n",
      "Requirement already satisfied: importlib-resources in c:\\users\\dirhem\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from chromadb) (6.5.2)\n",
      "Requirement already satisfied: grpcio>=1.58.0 in c:\\users\\dirhem\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from chromadb) (1.76.0)\n",
      "Requirement already satisfied: bcrypt>=4.0.1 in c:\\users\\dirhem\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from chromadb) (5.0.0)\n",
      "Requirement already satisfied: typer>=0.9.0 in c:\\users\\dirhem\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from chromadb) (0.21.1)\n",
      "Requirement already satisfied: starlette<0.51.0,>=0.40.0 in c:\\users\\dirhem\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from fastapi>=0.95.2->chromadb) (0.50.0)\n",
      "Requirement already satisfied: annotated-doc>=0.0.2 in c:\\users\\dirhem\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from fastapi>=0.95.2->chromadb) (0.0.4)\n",
      "Requirement already satisfied: anyio<5,>=3.6.2 in c:\\users\\dirhem\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from starlette<0.51.0,>=0.40.0->fastapi>=0.95.2->chromadb) (4.12.1)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in c:\\users\\dirhem\\appdata\\roaming\\python\\python310\\site-packages (from anyio<5,>=3.6.2->starlette<0.51.0,>=0.40.0->fastapi>=0.95.2->chromadb) (1.3.1)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\dirhem\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from anyio<5,>=3.6.2->starlette<0.51.0,>=0.40.0->fastapi>=0.95.2->chromadb) (3.11)\n",
      "Requirement already satisfied: coloredlogs in c:\\users\\dirhem\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in c:\\users\\dirhem\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb) (25.12.19)\n",
      "Requirement already satisfied: protobuf in c:\\users\\dirhem\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb) (3.19.6)\n",
      "Requirement already satisfied: sympy in c:\\users\\dirhem\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb) (1.14.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\dirhem\\appdata\\roaming\\python\\python310\\site-packages (from posthog>=2.4.0->chromadb) (1.17.0)\n",
      "Requirement already satisfied: python-dateutil>=2.2 in c:\\users\\dirhem\\appdata\\roaming\\python\\python310\\site-packages (from posthog>=2.4.0->chromadb) (2.9.0.post0)\n",
      "Requirement already satisfied: backoff>=1.10.0 in c:\\users\\dirhem\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from posthog>=2.4.0->chromadb) (2.2.1)\n",
      "Requirement already satisfied: distro>=1.5.0 in c:\\users\\dirhem\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from posthog>=2.4.0->chromadb) (1.9.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\dirhem\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\dirhem\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\dirhem\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers) (2026.1.4)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\dirhem\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pydantic>=1.9->chromadb) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in c:\\users\\dirhem\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pydantic>=1.9->chromadb) (2.41.5)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in c:\\users\\dirhem\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pydantic>=1.9->chromadb) (0.4.2)\n",
      "Requirement already satisfied: networkx>=2.5.1 in c:\\users\\dirhem\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch>=1.6.0->sentence-transformers) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\dirhem\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch>=1.6.0->sentence-transformers) (3.1.6)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\dirhem\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\dirhem\\appdata\\roaming\\python\\python310\\site-packages (from tqdm->sentence-transformers) (0.4.6)\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\users\\dirhem\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from typer>=0.9.0->chromadb) (8.3.1)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in c:\\users\\dirhem\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from typer>=0.9.0->chromadb) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in c:\\users\\dirhem\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from typer>=0.9.0->chromadb) (14.2.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\dirhem\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from rich>=10.11.0->typer>=0.9.0->chromadb) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\dirhem\\appdata\\roaming\\python\\python310\\site-packages (from rich>=10.11.0->typer>=0.9.0->chromadb) (2.19.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\dirhem\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer>=0.9.0->chromadb) (0.1.2)\n",
      "Requirement already satisfied: h11>=0.8 in c:\\users\\dirhem\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from uvicorn>=0.18.3->uvicorn[standard]>=0.18.3->chromadb) (0.16.0)\n",
      "Requirement already satisfied: httptools>=0.6.3 in c:\\users\\dirhem\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.7.1)\n",
      "Requirement already satisfied: python-dotenv>=0.13 in c:\\users\\dirhem\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.2.1)\n",
      "Requirement already satisfied: watchfiles>=0.13 in c:\\users\\dirhem\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.1.1)\n",
      "Requirement already satisfied: websockets>=10.4 in c:\\users\\dirhem\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (16.0)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in c:\\users\\dirhem\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb) (10.0)\n",
      "Requirement already satisfied: pyreadline3 in c:\\users\\dirhem\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from humanfriendly>=9.1->coloredlogs->onnxruntime>=1.14.1->chromadb) (3.5.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\dirhem\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jinja2->torch>=1.6.0->sentence-transformers) (3.0.3)\n",
      "Requirement already satisfied: joblib in c:\\users\\dirhem\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk->sentence-transformers) (1.5.3)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\dirhem\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\dirhem\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torchvision->sentence-transformers) (12.1.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1742/1742 [07:25<00:00,  3.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings shape: (55726, 384)\n"
     ]
    }
   ],
   "source": [
    "# Generate embeddings using all-MiniLM-L6-v2\n",
    "\n",
    "!pip install sentence-transformers chromadb\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "embedding_model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "paragraph_embeddings = embedding_model.encode(paragraphs, show_progress_bar=True)\n",
    "\n",
    "print(\"Embeddings shape:\", paragraph_embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08506255",
   "metadata": {},
   "source": [
    "### 8.3 Create ChromaDB Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "56f582e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChromaDB collection ready.\n"
     ]
    }
   ],
   "source": [
    "# Create ChromaDB collection\n",
    "\n",
    "import chromadb\n",
    "\n",
    "client = chromadb.Client()\n",
    "\n",
    "collection = client.get_or_create_collection(\n",
    "    name=\"sherlock_paragraphs\",\n",
    "    metadata={\"hnsw:space\": \"cosine\"}\n",
    ")\n",
    "\n",
    "print(\"ChromaDB collection ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3e571e4",
   "metadata": {},
   "source": [
    "### 8.4 Insert embeddings and metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "55f0890a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IDs\n",
    "ids = [f\"para_{i}\" for i in range(len(paragraphs))]\n",
    "\n",
    "# Metadata\n",
    "metadatas = [{\"paragraph_index\": i} for i in range(len(paragraphs))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a871438",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert embeddings and metadata in batches (ChromaDB limit ~166)\n",
    "\n",
    "batch_size = 100\n",
    "\n",
    "for i in range(0, len(paragraphs), batch_size):\n",
    "    batch_ids = ids[i:i+batch_size]\n",
    "    batch_embeddings = paragraph_embeddings[i:i+batch_size].tolist()\n",
    "    batch_metadatas = metadatas[i:i+batch_size]\n",
    "    batch_documents = paragraphs[i:i+batch_size]\n",
    "\n",
    "    collection.add(\n",
    "        ids=batch_ids,\n",
    "        embeddings=batch_embeddings,\n",
    "        metadatas=batch_metadatas,\n",
    "        documents=batch_documents\n",
    "    )\n",
    "\n",
    "print(\"Inserted all paragraph embeddings into ChromaDB in batches.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ca2ace3",
   "metadata": {},
   "source": [
    "### 8.5 Implement Semantic Search Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed7f975",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement semantic search function\n",
    "\n",
    "def semantic_search(query, top_k=5):\n",
    "    query_embedding = embedding_model.encode([query])\n",
    "    results = collection.query(\n",
    "        query_embeddings=query_embedding.tolist(),\n",
    "        n_results=top_k\n",
    "    )\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0eb730f",
   "metadata": {},
   "source": [
    "### 8.6 Run Required Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae5bc587",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Query: Detective investigations\n",
      "\n",
      "Retrieved paragraph (index: 20323):\n",
      "\"Hum! The investigation really was a very simple one,\" remarked\n",
      "     Holmes, \"but I don't think it struck him in that light when he first\n",
      "     called us in. However, let us see what Jim Browner has to say for\n",
      "     himself. This is his statement as made before Inspector Montgomery at\n",
      "     the Shadwell Police Station, and it has the advantage of being\n",
      "     verbatim.\"\n",
      "------------------------------------------------------------\n",
      "\n",
      "Retrieved paragraph (index: 34873):\n",
      "\"Hum! The investigation really was a very simple one,\" remarked\n",
      "     Holmes, \"but I don't think it struck him in that light when he first\n",
      "     called us in. However, let us see what Jim Browner has to say for\n",
      "     himself. This is his statement as made before Inspector Montgomery at\n",
      "     the Shadwell Police Station, and it has the advantage of being\n",
      "     verbatim.\"\n",
      "------------------------------------------------------------\n",
      "\n",
      "Retrieved paragraph (index: 41089):\n",
      "\"Hum! The investigation really was a very simple one,\" remarked\n",
      "     Holmes, \"but I don't think it struck him in that light when he first\n",
      "     called us in. However, let us see what Jim Browner has to say for\n",
      "     himself. This is his statement as made before Inspector Montgomery at\n",
      "     the Shadwell Police Station, and it has the advantage of being\n",
      "     verbatim.\"\n",
      "------------------------------------------------------------\n",
      "\n",
      "Query: Dr Watson\n",
      "\n",
      "Retrieved paragraph (index: 14134):\n",
      "CHAPTER IX\n",
      "          Second Report of Dr. Watson\n",
      "------------------------------------------------------------\n",
      "\n",
      "Retrieved paragraph (index: 32229):\n",
      "CHAPTER IX\n",
      "          Second Report of Dr. Watson\n",
      "------------------------------------------------------------\n",
      "\n",
      "Retrieved paragraph (index: 39085):\n",
      "CHAPTER IX\n",
      "          Second Report of Dr. Watson\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Run required queries\n",
    "\n",
    "queries = [\"Detective investigations\", \"Dr Watson\"]\n",
    "\n",
    "for q in queries:\n",
    "    print(f\"\\nQuery: {q}\")\n",
    "    results = semantic_search(q, top_k=3)\n",
    "    for doc, meta in zip(results[\"documents\"][0], results[\"metadatas\"][0]):\n",
    "        print(\"\\nRetrieved paragraph (index: {}):\".format(meta[\"paragraph_index\"]))\n",
    "        print(doc)\n",
    "        print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac5944c4",
   "metadata": {},
   "source": [
    "### 8.7 Extended Semantic Search Features (Optional Enhancements)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92aca15a",
   "metadata": {},
   "source": [
    "#### 8.7.1 Add cosine similarity scores "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd227ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add cosine similarity scores to semantic search results\n",
    "\n",
    "def semantic_search_with_scores(query, top_k=5):\n",
    "    query_embedding = embedding_model.encode([query])\n",
    "\n",
    "    results = collection.query(\n",
    "        query_embeddings=query_embedding.tolist(),\n",
    "        n_results=top_k,\n",
    "        include=[\"documents\", \"metadatas\", \"distances\"]\n",
    "    )\n",
    "\n",
    "    # Chroma returns distances; convert to similarity\n",
    "    similarities = [1 - d for d in results[\"distances\"][0]]\n",
    "\n",
    "    output = []\n",
    "    for doc, meta, sim in zip(results[\"documents\"][0], results[\"metadatas\"][0], similarities):\n",
    "        output.append({\n",
    "            \"paragraph\": doc,\n",
    "            \"index\": meta[\"index\"],\n",
    "            \"similarity\": sim\n",
    "        })\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2247941c",
   "metadata": {},
   "source": [
    "#### 8.7.2 Display Results in a Clean, Ranked Format "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57d42846",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dirhem\\.cache\\chroma\\onnx_models\\all-MiniLM-L6-v2\\onnx.tar.gz: 100%|██████████| 79.3M/79.3M [00:18<00:00, 4.59MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Results for query: Dr Watson ===\n",
      "\n",
      "Paragraph Index: 14134\n",
      "Similarity: 0.7602\n",
      "CHAPTER IX\n",
      "          Second Report of Dr. Watson\n",
      "------------------------------------------------------------\n",
      "\n",
      "Paragraph Index: 32229\n",
      "Similarity: 0.7602\n",
      "CHAPTER IX\n",
      "          Second Report of Dr. Watson\n",
      "------------------------------------------------------------\n",
      "\n",
      "Paragraph Index: 39085\n",
      "Similarity: 0.7602\n",
      "CHAPTER IX\n",
      "          Second Report of Dr. Watson\n",
      "------------------------------------------------------------\n",
      "\n",
      "Paragraph Index: 22604\n",
      "Similarity: 0.7552\n",
      "\"Dr. Watson has already heard of it.\"\n",
      "------------------------------------------------------------\n",
      "\n",
      "Paragraph Index: 30155\n",
      "Similarity: 0.7552\n",
      "\"Dr. Watson has already heard of it.\"\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Semantic Search with Similarity Scores\n",
    "\n",
    "\n",
    "def semantic_search_with_scores(query, top_k=5):\n",
    "    # Query ChromaDB\n",
    "    results = collection.query(\n",
    "        query_texts=[query],\n",
    "        n_results=top_k,\n",
    "        include=[\"documents\", \"metadatas\", \"distances\"]\n",
    "    )\n",
    "\n",
    "    # Convert distances to similarity scores\n",
    "    similarities = [1 - d for d in results[\"distances\"][0]]\n",
    "\n",
    "    # Build structured output\n",
    "    output = []\n",
    "    for doc, meta, sim in zip(results[\"documents\"][0], results[\"metadatas\"][0], similarities):\n",
    "        output.append({\n",
    "            \"paragraph\": doc,\n",
    "            \"index\": meta[\"paragraph_index\"],   # FIXED KEY\n",
    "            \"similarity\": sim\n",
    "        })\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "\n",
    "# Pretty-print function for semantic search results\n",
    "\n",
    "def show_results(results):\n",
    "    for r in results:\n",
    "        print(f\"\\nParagraph Index: {r['index']}\")\n",
    "        print(f\"Similarity: {r['similarity']:.4f}\")\n",
    "        print(r[\"paragraph\"])\n",
    "        print(\"-\" * 60)\n",
    "\n",
    "\n",
    "\n",
    "# Run the search and display results\n",
    "\n",
    "query = \"Dr Watson\"\n",
    "results = semantic_search_with_scores(query, top_k=5)\n",
    "\n",
    "print(f\"=== Results for query: {query} ===\")\n",
    "show_results(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0aa16a8",
   "metadata": {},
   "source": [
    "#### 8.7.3 Run the improved search for your required queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b3773e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Query: Detective investigations ===\n",
      "\n",
      "Paragraph Index: 16778\n",
      "Similarity: 0.5336\n",
      "\"Hum! The investigation really was a very simple one,\" remarked\n",
      "     Holmes, \"but I don't think it struck him in that light when he first\n",
      "     called us in. However, let us see what Jim Browner has to say for\n",
      "     himself. This is his statement as made before Inspector Montgomery at\n",
      "     the Shadwell Police Station, and it has the advantage of being\n",
      "     verbatim.\"\n",
      "------------------------------------------------------------\n",
      "\n",
      "Paragraph Index: 20323\n",
      "Similarity: 0.5336\n",
      "\"Hum! The investigation really was a very simple one,\" remarked\n",
      "     Holmes, \"but I don't think it struck him in that light when he first\n",
      "     called us in. However, let us see what Jim Browner has to say for\n",
      "     himself. This is his statement as made before Inspector Montgomery at\n",
      "     the Shadwell Police Station, and it has the advantage of being\n",
      "     verbatim.\"\n",
      "------------------------------------------------------------\n",
      "\n",
      "Paragraph Index: 34873\n",
      "Similarity: 0.5336\n",
      "\"Hum! The investigation really was a very simple one,\" remarked\n",
      "     Holmes, \"but I don't think it struck him in that light when he first\n",
      "     called us in. However, let us see what Jim Browner has to say for\n",
      "     himself. This is his statement as made before Inspector Montgomery at\n",
      "     the Shadwell Police Station, and it has the advantage of being\n",
      "     verbatim.\"\n",
      "------------------------------------------------------------\n",
      "\n",
      "Paragraph Index: 41089\n",
      "Similarity: 0.5336\n",
      "\"Hum! The investigation really was a very simple one,\" remarked\n",
      "     Holmes, \"but I don't think it struck him in that light when he first\n",
      "     called us in. However, let us see what Jim Browner has to say for\n",
      "     himself. This is his statement as made before Inspector Montgomery at\n",
      "     the Shadwell Police Station, and it has the advantage of being\n",
      "     verbatim.\"\n",
      "------------------------------------------------------------\n",
      "\n",
      "Paragraph Index: 40812\n",
      "Similarity: 0.5173\n",
      "The detective looked at my friend in a singular way.\n",
      "------------------------------------------------------------\n",
      "\n",
      "=== Query: Dr Watson ===\n",
      "\n",
      "Paragraph Index: 14134\n",
      "Similarity: 0.7602\n",
      "CHAPTER IX\n",
      "          Second Report of Dr. Watson\n",
      "------------------------------------------------------------\n",
      "\n",
      "Paragraph Index: 32229\n",
      "Similarity: 0.7602\n",
      "CHAPTER IX\n",
      "          Second Report of Dr. Watson\n",
      "------------------------------------------------------------\n",
      "\n",
      "Paragraph Index: 39085\n",
      "Similarity: 0.7602\n",
      "CHAPTER IX\n",
      "          Second Report of Dr. Watson\n",
      "------------------------------------------------------------\n",
      "\n",
      "Paragraph Index: 22604\n",
      "Similarity: 0.7552\n",
      "\"Dr. Watson has already heard of it.\"\n",
      "------------------------------------------------------------\n",
      "\n",
      "Paragraph Index: 30155\n",
      "Similarity: 0.7552\n",
      "\"Dr. Watson has already heard of it.\"\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Run improved semantic search\n",
    "\n",
    "queries = [\"Detective investigations\", \"Dr Watson\"]\n",
    "\n",
    "for q in queries:\n",
    "    print(f\"\\n=== Query: {q} ===\")\n",
    "    results = semantic_search_with_scores(q, top_k=5)\n",
    "    show_results(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f1a3ac",
   "metadata": {},
   "source": [
    "#### 8.7.4 Add a Function to Search by Character Name "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5525bc4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Paragraph Index: 777\n",
      "Similarity: 0.8522\n",
      "THE ADVENTURES OF SHERLOCK HOLMES\n",
      "------------------------------------------------------------\n",
      "\n",
      "Paragraph Index: 6225\n",
      "Similarity: 0.8522\n",
      "THE ADVENTURES OF SHERLOCK HOLMES\n",
      "------------------------------------------------------------\n",
      "\n",
      "Paragraph Index: 24320\n",
      "Similarity: 0.8522\n",
      "THE ADVENTURES OF SHERLOCK HOLMES\n",
      "------------------------------------------------------------\n",
      "\n",
      "Paragraph Index: 8732\n",
      "Similarity: 0.8400\n",
      "THE MEMOIRS OF SHERLOCK HOLMES\n",
      "------------------------------------------------------------\n",
      "\n",
      "Paragraph Index: 26827\n",
      "Similarity: 0.8400\n",
      "THE MEMOIRS OF SHERLOCK HOLMES\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Search for paragraphs mentioning a specific character\n",
    "\n",
    "def search_character(name, top_k=5):\n",
    "    query = f\"{name} in the story\"\n",
    "    return semantic_search_with_scores(query, top_k)\n",
    "\n",
    "results = search_character(\"Sherlock Holmes\", top_k=5)\n",
    "show_results(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db864ce4",
   "metadata": {},
   "source": [
    "#### 8.7.5 Theme Based Semantic Search and Result Exporting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6d94b8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Theme Search Results for: detective investigations ===\n",
      "\n",
      "Paragraph Index: 1375\n",
      "Similarity: 0.6182\n",
      "\"And now I will show you what is really a very interesting study, Mr.\n",
      "     Windibank,\" Holmes continued. \"I think of writing another little\n",
      "     monograph some of these days on the typewriter and its relation to\n",
      "     crime. It is a subject to which I have devoted some little attention.\n",
      "     I have here four letters which purport to come from the missing man.\n",
      "     They are all typewritten. In each case, not only are the 'e's'\n",
      "     slurred and the 'r's' tailless, but you will observe, if you care to\n",
      "     use my magnifying lens, that the fourteen other characteristics to\n",
      "     which I have alluded are there as well.\"\n",
      "------------------------------------------------------------\n",
      "\n",
      "Paragraph Index: 6820\n",
      "Similarity: 0.6182\n",
      "\"And now I will show you what is really a very interesting study, Mr.\n",
      "     Windibank,\" Holmes continued. \"I think of writing another little\n",
      "     monograph some of these days on the typewriter and its relation to\n",
      "     crime. It is a subject to which I have devoted some little attention.\n",
      "     I have here four letters which purport to come from the missing man.\n",
      "     They are all typewritten. In each case, not only are the 'e's'\n",
      "     slurred and the 'r's' tailless, but you will observe, if you care to\n",
      "     use my magnifying lens, that the fourteen other characteristics to\n",
      "     which I have alluded are there as well.\"\n",
      "------------------------------------------------------------\n",
      "\n",
      "Paragraph Index: 24915\n",
      "Similarity: 0.6182\n",
      "\"And now I will show you what is really a very interesting study, Mr.\n",
      "     Windibank,\" Holmes continued. \"I think of writing another little\n",
      "     monograph some of these days on the typewriter and its relation to\n",
      "     crime. It is a subject to which I have devoted some little attention.\n",
      "     I have here four letters which purport to come from the missing man.\n",
      "     They are all typewritten. In each case, not only are the 'e's'\n",
      "     slurred and the 'r's' tailless, but you will observe, if you care to\n",
      "     use my magnifying lens, that the fourteen other characteristics to\n",
      "     which I have alluded are there as well.\"\n",
      "------------------------------------------------------------\n",
      "\n",
      "Paragraph Index: 39941\n",
      "Similarity: 0.6182\n",
      "\"And now I will show you what is really a very interesting study, Mr.\n",
      "     Windibank,\" Holmes continued. \"I think of writing another little\n",
      "     monograph some of these days on the typewriter and its relation to\n",
      "     crime. It is a subject to which I have devoted some little attention.\n",
      "     I have here four letters which purport to come from the missing man.\n",
      "     They are all typewritten. In each case, not only are the 'e's'\n",
      "     slurred and the 'r's' tailless, but you will observe, if you care to\n",
      "     use my magnifying lens, that the fourteen other characteristics to\n",
      "     which I have alluded are there as well.\"\n",
      "------------------------------------------------------------\n",
      "\n",
      "Paragraph Index: 46637\n",
      "Similarity: 0.5891\n",
      "\"The conviction that the crime is a very peculiar one. Perhaps our\n",
      "     visit now may do something to make it less obscure. I think that we\n",
      "     are both agreed, Inspector, that the fragment of paper in the dead\n",
      "     man's hand, bearing, as it does, the very hour of his death written\n",
      "     upon it, is of extreme importance.\"\n",
      "------------------------------------------------------------\n",
      "Results exported to theme_detective_investigations.txt\n"
     ]
    }
   ],
   "source": [
    "# Theme Search\n",
    "def search_theme(theme, top_k=5):\n",
    "    query = f\"Paragraphs related to the theme: {theme}\"\n",
    "    return semantic_search_with_scores(query, top_k)\n",
    "\n",
    "# Print theme search results\n",
    "theme = \"detective investigations\"\n",
    "results = search_theme(theme, top_k=5)\n",
    "\n",
    "print(f\"=== Theme Search Results for: {theme} ===\")\n",
    "show_results(results)\n",
    "\n",
    "\n",
    "# B.6 Export Results\n",
    "def export_results(results, filename=\"semantic_results.txt\"):\n",
    "    with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "        for r in results:\n",
    "            f.write(f\"Paragraph Index: {r['index']}\\n\")\n",
    "            f.write(f\"Similarity: {r['similarity']:.4f}\\n\")\n",
    "            f.write(r[\"paragraph\"] + \"\\n\")\n",
    "            f.write(\"-\" * 60 + \"\\n\\n\")\n",
    "    print(f\"Results exported to {filename}\")\n",
    "\n",
    "# Export the theme search results\n",
    "export_results(results, filename=\"theme_detective_investigations.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "351e6419",
   "metadata": {},
   "source": [
    "___________________________________________________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0251bf1b",
   "metadata": {},
   "source": [
    "# 9.0 Topic Modelling with LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1985a87c",
   "metadata": {},
   "source": [
    "### 9.1 Prepare Text for LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c25ac8fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample processed paragraph:\n",
      "['adventure', 'three', 'gables']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "def preprocess(text):\n",
    "    # Lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove non-alphabetic characters\n",
    "    text = re.sub(r\"[^a-z\\s]\", \" \", text)\n",
    "    \n",
    "    # Tokenise\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Remove stopwords + short tokens\n",
    "    tokens = [t for t in tokens if t not in stop_words and len(t) > 2]\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "# Apply to all paragraphs\n",
    "processed_paragraphs = [preprocess(p) for p in paragraphs]\n",
    "\n",
    "print(\"Sample processed paragraph:\")\n",
    "print(processed_paragraphs[0][:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04a0aede",
   "metadata": {},
   "source": [
    "### 9.1.2 Remove rare and frequent words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e6aeb3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering complete.\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Flatten list\n",
    "all_tokens = [token for para in processed_paragraphs for token in para]\n",
    "\n",
    "freq = Counter(all_tokens)\n",
    "\n",
    "# Remove words that appear < 5 times or > 20% of paragraphs\n",
    "min_count = 5\n",
    "max_fraction = 0.2 * len(processed_paragraphs)\n",
    "\n",
    "filtered_paragraphs = [\n",
    "    [t for t in para if freq[t] >= min_count and freq[t] <= max_fraction]\n",
    "    for para in processed_paragraphs\n",
    "]\n",
    "\n",
    "print(\"Filtering complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "081cf069",
   "metadata": {},
   "source": [
    "### 9.2 Create Gensim Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eecada30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary size: 12160\n"
     ]
    }
   ],
   "source": [
    "from gensim.corpora import Dictionary\n",
    "\n",
    "dictionary = Dictionary(filtered_paragraphs)\n",
    "\n",
    "print(\"Dictionary size:\", len(dictionary))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93f82f48",
   "metadata": {},
   "source": [
    "### 9.3 Create Bag‑of‑Words Corpus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4539a0f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus created. Example:\n",
      "[(0, 1), (1, 1), (2, 1)]\n"
     ]
    }
   ],
   "source": [
    "corpus = [dictionary.doc2bow(text) for text in filtered_paragraphs]\n",
    "\n",
    "print(\"Corpus created. Example:\")\n",
    "print(corpus[0][:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c748262",
   "metadata": {},
   "source": [
    "### 9.4 Train LDA Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "352b57dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LDA model trained.\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import LdaModel\n",
    "\n",
    "num_topics = 10\n",
    "\n",
    "lda_model = LdaModel(\n",
    "    corpus=corpus,\n",
    "    id2word=dictionary,\n",
    "    num_topics=num_topics,\n",
    "    passes=10,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(\"LDA model trained.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3058620d",
   "metadata": {},
   "source": [
    "### 9.5 Display Topics and Interpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b938abb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic 0:\n",
      "['one', 'could', 'man', 'upon', 'road', 'place', 'side', 'horse', 'dog', 'moor']\n",
      "\n",
      "Topic 1:\n",
      "['holmes', 'said', 'yes', 'asked', 'sir', 'well', 'sherlock', 'certainly', 'nothing', 'know']\n",
      "\n",
      "Topic 2:\n",
      "['sir', 'could', 'man', 'would', 'woman', 'husband', 'one', 'wife', 'say', 'never']\n",
      "\n",
      "Topic 3:\n",
      "['room', 'door', 'house', 'one', 'window', 'two', 'came', 'could', 'night', 'upon']\n",
      "\n",
      "Topic 4:\n",
      "['street', 'found', 'london', 'holmes', 'day', 'one', 'station', 'morning', 'train', 'made']\n",
      "\n",
      "Topic 5:\n",
      "['upon', 'face', 'holmes', 'eyes', 'man', 'hand', 'head', 'like', 'looked', 'turned']\n",
      "\n",
      "Topic 6:\n",
      "['paper', 'upon', 'letter', 'read', 'table', 'note', 'inspector', 'took', 'pocket', 'papers']\n",
      "\n",
      "Topic 7:\n",
      "['said', 'cried', 'man', 'mcmurdo', 'know', 'holmes', 'shall', 'come', 'tell', 'good']\n",
      "\n",
      "Topic 8:\n",
      "['would', 'may', 'watson', 'said', 'one', 'case', 'must', 'upon', 'holmes', 'think']\n",
      "\n",
      "Topic 9:\n",
      "['well', 'would', 'said', 'good', 'come', 'man', 'came', 'see', 'never', 'time']\n"
     ]
    }
   ],
   "source": [
    "for i, topic in lda_model.show_topics(num_topics=num_topics, num_words=10, formatted=False):\n",
    "    print(f\"\\nTopic {i}:\")\n",
    "    print([word for word, prob in topic])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0a59563",
   "metadata": {},
   "source": [
    "### 9.6 Topic Distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e909948f",
   "metadata": {},
   "source": [
    "#### 9.6.1 Per paragraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7066f00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0.025022691),\n",
       " (1, 0.025022311),\n",
       " (2, 0.02502428),\n",
       " (3, 0.46271756),\n",
       " (4, 0.33708948),\n",
       " (5, 0.025031198),\n",
       " (6, 0.025022311),\n",
       " (7, 0.025022311),\n",
       " (8, 0.025023058),\n",
       " (9, 0.025024815)]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_topic_distribution(text_bow):\n",
    "    return lda_model.get_document_topics(text_bow, minimum_probability=0.0)\n",
    "\n",
    "topic_dist_paragraph_0 = get_topic_distribution(corpus[0])\n",
    "topic_dist_paragraph_0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd43a5a2",
   "metadata": {},
   "source": [
    "#### 9.6.2 Whole corpus (average distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "713965e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average topic distribution across the corpus:\n",
      "[0.06406432 0.10037359 0.07527379 0.10680516 0.06192338 0.11996917\n",
      " 0.06512542 0.09222951 0.18666439 0.12756489]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "topic_matrix = []\n",
    "\n",
    "for bow in corpus:\n",
    "    dist = lda_model.get_document_topics(bow, minimum_probability=0.0)\n",
    "    topic_matrix.append([p for _, p in dist])\n",
    "\n",
    "topic_matrix = np.array(topic_matrix)\n",
    "avg_topic_distribution = topic_matrix.mean(axis=0)\n",
    "\n",
    "print(\"Average topic distribution across the corpus:\")\n",
    "print(avg_topic_distribution)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47818875",
   "metadata": {},
   "source": [
    "### 9.7 Visualisation (pyLDAvis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c8fc73e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<link rel=\"stylesheet\" type=\"text/css\" href=\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.4.0/pyLDAvis/js/ldavis.v1.0.0.css\">\n",
       "\n",
       "\n",
       "<div id=\"ldavis_el2273217219085326566407052993\" style=\"background-color:white;\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "\n",
       "var ldavis_el2273217219085326566407052993_data = {\"mdsDat\": {\"x\": [0.10309546331761626, -0.16974346862622872, -0.19234787805621534, 0.1294546109472753, -0.14708987470681456, 0.20563832743591337, 0.12571990735740468, -0.13817077745550713, -0.12602095370443173, 0.20946464349098742], \"y\": [-0.06166569775019531, 0.011524637716472485, -0.0657454413372441, -0.13662558147136597, -0.14770130561830744, -0.009871992621040488, -0.1497394356400071, 0.08430771482134149, 0.22495968780537498, 0.2505574140949709], \"topics\": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10], \"cluster\": [1, 1, 1, 1, 1, 1, 1, 1, 1, 1], \"Freq\": [21.353868304598723, 15.993045340716552, 14.272749294616794, 12.771359777022425, 6.625697456342676, 6.43061808940784, 6.366086826389608, 5.832411975947966, 5.265254233756888, 5.0889087012005145]}, \"tinfo\": {\"Term\": [\"holmes\", \"said\", \"sir\", \"yes\", \"face\", \"room\", \"would\", \"asked\", \"well\", \"door\", \"man\", \"upon\", \"could\", \"cried\", \"know\", \"eyes\", \"watson\", \"house\", \"good\", \"may\", \"paper\", \"window\", \"mcmurdo\", \"case\", \"come\", \"street\", \"came\", \"shall\", \"never\", \"sherlock\", \"present\", \"events\", \"details\", \"absolutely\", \"serious\", \"report\", \"warranties\", \"consider\", \"results\", \"carruthers\", \"concerned\", \"share\", \"connected\", \"expressed\", \"provided\", \"probable\", \"meanwhile\", \"french\", \"mac\", \"difficulties\", \"due\", \"relations\", \"including\", \"chief\", \"advantage\", \"scowrers\", \"medium\", \"especially\", \"implied\", \"hypothesis\", \"cases\", \"explanation\", \"case\", \"facts\", \"fancy\", \"watson\", \"criminal\", \"points\", \"try\", \"may\", \"admit\", \"possible\", \"dear\", \"matter\", \"theory\", \"particular\", \"must\", \"entirely\", \"excellent\", \"make\", \"fact\", \"purpose\", \"would\", \"really\", \"think\", \"might\", \"point\", \"course\", \"use\", \"much\", \"take\", \"yet\", \"however\", \"said\", \"one\", \"first\", \"without\", \"friend\", \"quite\", \"shall\", \"upon\", \"well\", \"find\", \"could\", \"holmes\", \"come\", \"time\", \"little\", \"see\", \"say\", \"man\", \"face\", \"eyes\", \"figure\", \"expression\", \"thin\", \"tall\", \"shook\", \"yellow\", \"slowly\", \"mouth\", \"knees\", \"sank\", \"leaned\", \"ferguson\", \"sunk\", \"nose\", \"grey\", \"forehead\", \"leaning\", \"chin\", \"mountains\", \"breast\", \"stable\", \"flushed\", \"cap\", \"shining\", \"eyebrows\", \"distant\", \"skin\", \"gleam\", \"hair\", \"bright\", \"silence\", \"gray\", \"raised\", \"white\", \"silent\", \"looked\", \"sat\", \"head\", \"sprang\", \"turned\", \"black\", \"forward\", \"red\", \"dark\", \"stood\", \"pale\", \"hand\", \"upon\", \"chair\", \"like\", \"suddenly\", \"instant\", \"light\", \"hands\", \"holmes\", \"man\", \"seemed\", \"back\", \"long\", \"round\", \"little\", \"held\", \"lay\", \"great\", \"one\", \"saw\", \"could\", \"window\", \"garden\", \"bedroom\", \"passage\", \"gate\", \"amazement\", \"stairs\", \"stared\", \"eleven\", \"clothes\", \"morris\", \"dressing\", \"carpet\", \"lawn\", \"giving\", \"served\", \"concealed\", \"lock\", \"stables\", \"kitchen\", \"slept\", \"fastened\", \"downstairs\", \"trail\", \"dining\", \"lion\", \"brixton\", \"band\", \"peter\", \"porter\", \"door\", \"rushed\", \"bed\", \"bell\", \"room\", \"empty\", \"spot\", \"key\", \"walked\", \"open\", \"floor\", \"house\", \"followed\", \"entered\", \"stair\", \"sitting\", \"sound\", \"hour\", \"corner\", \"came\", \"opened\", \"behind\", \"two\", \"saw\", \"left\", \"found\", \"night\", \"front\", \"one\", \"half\", \"heard\", \"could\", \"back\", \"three\", \"upon\", \"round\", \"morning\", \"way\", \"went\", \"side\", \"last\", \"see\", \"guess\", \"maybe\", \"charles\", \"lives\", \"anyhow\", \"luck\", \"america\", \"scott\", \"stangerson\", \"eccles\", \"drink\", \"joseph\", \"woodley\", \"food\", \"circle\", \"till\", \"attack\", \"nurse\", \"gets\", \"bye\", \"simon\", \"hidden\", \"warrant\", \"frances\", \"san\", \"agree\", \"conscience\", \"shafter\", \"gossip\", \"secrets\", \"ago\", \"wanted\", \"honest\", \"week\", \"father\", \"months\", \"harm\", \"good\", \"lord\", \"lived\", \"marriage\", \"says\", \"well\", \"friends\", \"years\", \"get\", \"better\", \"day\", \"never\", \"came\", \"come\", \"would\", \"time\", \"got\", \"word\", \"night\", \"take\", \"see\", \"last\", \"said\", \"thought\", \"went\", \"man\", \"house\", \"done\", \"way\", \"find\", \"know\", \"one\", \"nothing\", \"could\", \"think\", \"two\", \"back\", \"little\", \"dog\", \"picture\", \"bear\", \"german\", \"straker\", \"scent\", \"soft\", \"italian\", \"coal\", \"wet\", \"size\", \"dancing\", \"struggle\", \"musgrave\", \"fog\", \"blaze\", \"gloomy\", \"advise\", \"mud\", \"toby\", \"consulting\", \"simpson\", \"pictures\", \"opportunity\", \"sky\", \"science\", \"coincidence\", \"charlington\", \"bartholomew\", \"inquest\", \"bicycle\", \"track\", \"moor\", \"horse\", \"east\", \"lonely\", \"grass\", \"height\", \"building\", \"road\", \"miles\", \"traces\", \"stranger\", \"ill\", \"country\", \"main\", \"side\", \"place\", \"could\", \"path\", \"line\", \"one\", \"clear\", \"man\", \"walk\", \"mind\", \"upon\", \"carriage\", \"end\", \"great\", \"see\", \"way\", \"far\", \"high\", \"two\", \"long\", \"whole\", \"mcmurdo\", \"mcginty\", \"baldwin\", \"chapter\", \"justice\", \"bodymaster\", \"swear\", \"arthur\", \"mason\", \"boss\", \"vermissa\", \"claim\", \"scanlan\", \"played\", \"vain\", \"tut\", \"pycroft\", \"cunningham\", \"officer\", \"heavens\", \"cart\", \"conan\", \"doyle\", \"william\", \"cries\", \"edwards\", \"express\", \"mawson\", \"gasped\", \"quarrel\", \"cried\", \"staunton\", \"answer\", \"gregson\", \"lodge\", \"valley\", \"want\", \"know\", \"brother\", \"hear\", \"shall\", \"said\", \"man\", \"devil\", \"tell\", \"ask\", \"let\", \"good\", \"come\", \"holmes\", \"heard\", \"sure\", \"enough\", \"see\", \"never\", \"right\", \"help\", \"murder\", \"say\", \"well\", \"head\", \"husband\", \"thousand\", \"pounds\", \"loved\", \"village\", \"heaven\", \"birlstone\", \"manor\", \"original\", \"barrymore\", \"gang\", \"stayed\", \"villain\", \"nerve\", \"mercy\", \"convict\", \"gennaro\", \"joke\", \"boys\", \"henderson\", \"suspect\", \"york\", \"interrupted\", \"bless\", \"guessed\", \"leonardo\", \"officers\", \"actual\", \"thanks\", \"genius\", \"henry\", \"sake\", \"god\", \"children\", \"douglas\", \"wife\", \"hundred\", \"baskerville\", \"love\", \"woman\", \"sir\", \"master\", \"family\", \"money\", \"knew\", \"name\", \"suppose\", \"could\", \"exactly\", \"death\", \"man\", \"would\", \"got\", \"say\", \"told\", \"never\", \"heart\", \"one\", \"life\", \"tell\", \"old\", \"know\", \"said\", \"seen\", \"secretary\", \"scotland\", \"constable\", \"faces\", \"test\", \"von\", \"pleasure\", \"bork\", \"intended\", \"suggestion\", \"pearl\", \"tuesday\", \"moran\", \"rubbed\", \"mister\", \"melancholy\", \"afford\", \"chuckled\", \"hide\", \"cage\", \"pile\", \"midnight\", \"helpless\", \"awaiting\", \"intently\", \"works\", \"masterful\", \"bureau\", \"charing\", \"direct\", \"adventure\", \"yard\", \"acquaintance\", \"baker\", \"inn\", \"street\", \"famous\", \"train\", \"station\", \"ring\", \"contents\", \"london\", \"shortly\", \"found\", \"engaged\", \"practice\", \"evening\", \"police\", \"clock\", \"day\", \"morning\", \"journey\", \"many\", \"made\", \"lestrade\", \"year\", \"holmes\", \"friend\", \"one\", \"already\", \"small\", \"last\", \"old\", \"hands\", \"upon\", \"great\", \"paper\", \"message\", \"handed\", \"hudson\", \"sergeant\", \"housekeeper\", \"sheet\", \"envelope\", \"cigarette\", \"tracks\", \"cecil\", \"version\", \"copy\", \"clerk\", \"barclay\", \"hullo\", \"prime\", \"plenty\", \"robbery\", \"tossed\", \"earlier\", \"pen\", \"indications\", \"cipher\", \"killing\", \"solemn\", \"documents\", \"initials\", \"destroyed\", \"pound\", \"telegram\", \"barker\", \"document\", \"letter\", \"read\", \"collection\", \"writing\", \"written\", \"note\", \"office\", \"pocket\", \"table\", \"letters\", \"oldacre\", \"papers\", \"inspector\", \"pipe\", \"box\", \"took\", \"post\", \"surely\", \"comes\", \"received\", \"upon\", \"mrs\", \"none\", \"one\", \"morning\", \"put\", \"holmes\", \"said\", \"hand\", \"yes\", \"asked\", \"sholto\", \"pray\", \"smiled\", \"statement\", \"bust\", \"shrugged\", \"corridor\", \"presume\", \"brothers\", \"joy\", \"thaddeus\", \"fool\", \"halloa\", \"european\", \"impatience\", \"telegraph\", \"proceed\", \"patch\", \"swung\", \"bitterly\", \"friendly\", \"africa\", \"unlikely\", \"murmured\", \"chill\", \"planned\", \"drama\", \"chairs\", \"loud\", \"sir\", \"holmes\", \"madam\", \"said\", \"thank\", \"baynes\", \"sherlock\", \"mean\", \"book\", \"certainly\", \"inquiries\", \"lestrade\", \"voice\", \"anything\", \"nothing\", \"well\", \"know\", \"answered\", \"interesting\", \"professor\", \"shoulders\", \"say\", \"indeed\", \"come\", \"think\", \"see\", \"little\", \"else\", \"note\", \"one\"], \"Freq\": [10800.0, 11138.0, 3118.0, 2313.0, 3354.0, 3784.0, 7829.0, 1862.0, 5782.0, 2787.0, 8256.0, 8913.0, 6690.0, 1854.0, 3685.0, 2288.0, 3593.0, 3351.0, 2805.0, 4152.0, 1188.0, 1697.0, 1197.0, 2519.0, 3968.0, 1527.0, 3229.0, 2091.0, 2803.0, 1520.0, 533.52495078546, 340.62138611203534, 336.1672438061494, 334.5282661735714, 276.9544699863385, 263.4472996654233, 238.20932282325842, 236.30510149918356, 193.85884303436848, 193.50477604369652, 191.76105997953448, 182.26811777887275, 178.93020614812465, 172.0919361338375, 166.63678820053562, 159.5108834617556, 152.93894284221957, 152.74407826755538, 150.9914594367087, 149.45083374598406, 148.23507601488242, 139.44606548532175, 136.5076415602106, 136.51126908637764, 135.1029459169149, 134.96350330013402, 129.5814040363723, 128.26205744183815, 125.36177363339894, 124.13592524852275, 416.84621027102986, 343.62957605096506, 2356.184925428281, 689.9058156301824, 313.02378013607637, 3134.697001766516, 426.8060431899467, 381.66347262571264, 297.16656041596315, 3379.591599856072, 251.24588548889523, 752.4082958562537, 1016.6712268981687, 1469.6290769728716, 317.8374818138027, 346.911516485713, 2268.173987746969, 372.59003338950396, 419.45332865486944, 1326.9504002994822, 530.6534828186858, 311.23367261051027, 4025.238735100754, 632.3392234033657, 1727.021064719037, 1435.1918981387205, 750.2251892986171, 850.0171057645039, 486.8963717906785, 1098.785952892395, 1043.8737901653433, 1092.017908719187, 1002.3710078284497, 3094.1670095454083, 2736.0159353285335, 1146.2379803270987, 870.2063325608924, 977.970018784436, 780.4532626648712, 992.7384441892946, 2153.7757094396306, 1660.3824893756703, 902.4681197170017, 1637.2966062970095, 1940.7246600666977, 1154.735996838568, 1096.7313425081409, 1074.3842485615316, 1140.1031693836726, 940.6183525886844, 924.132319089683, 3353.9506867966556, 2287.2389996519128, 468.1186101829678, 431.2736511766434, 386.39478462097645, 377.78896167081626, 350.1053251378469, 321.64723418248025, 320.1506342064746, 313.427165223732, 194.75131392910222, 188.17457381056647, 186.23991097436004, 180.19678254218869, 176.12924184393378, 173.74796381967417, 171.34026350287706, 166.7618025295173, 158.6821008040789, 152.01532208403611, 148.95082403416765, 147.750130775312, 147.53407448387168, 146.6151775578456, 144.79104447108801, 143.78503625040375, 137.21020940431816, 124.81523646377133, 124.57617307111721, 121.75721484114683, 373.78853857011484, 202.54382389340137, 427.9238555053614, 255.10704312175693, 371.51756070260996, 801.2152110965638, 344.8369381189402, 1114.5892638770779, 850.092434418686, 1190.5956191106488, 443.7853815684645, 1073.2913545701276, 779.7590085308949, 404.9682978513585, 489.14558920715604, 805.8705090915242, 698.0298258060429, 285.73432585027706, 1477.7387831296403, 3588.948911210357, 664.0605918232866, 1173.9270364625968, 542.3175351982713, 687.8712674661878, 809.2654197271822, 804.7894336262558, 2452.9417102166144, 2041.3210405867633, 702.1720606607541, 1003.7509418475008, 871.8318778378846, 738.2579862206301, 945.4735048538901, 511.7772681007869, 539.7304081910934, 598.0907281472473, 726.4944218877403, 559.3906219077473, 577.4742225945279, 1696.4044825538706, 434.8703097573654, 432.3321202552132, 374.3588420796677, 293.6758265441712, 270.90528614882777, 263.3551405158719, 261.769412378579, 237.49371939326554, 234.96186460780268, 231.5908391454416, 227.56648412867932, 201.1947170569446, 183.76167917622766, 174.98662813664836, 173.69741937948064, 170.08473557398284, 163.80557998469743, 154.28776206991213, 151.41630841589517, 142.5597427200497, 136.22823607586025, 134.91694118382026, 133.90894433100428, 129.88986539903172, 127.88921331858936, 125.49862613023959, 121.80146520662768, 120.21498578218021, 116.39786649711088, 2736.991557443734, 457.4999667365271, 581.8180134022602, 447.3166108510795, 3302.480754335351, 315.4555387056504, 228.64437629892836, 389.97509687365863, 550.2537422367227, 1076.6985798570058, 455.36849678030273, 2191.1307619466093, 597.5316623310107, 567.6263356509163, 243.8222891876691, 410.8939022548825, 380.5646788725545, 648.3423053729637, 426.86131549478245, 1439.626143938141, 543.3931586886658, 665.6430825650677, 1529.4615103371618, 968.4491653005368, 979.5966958005711, 1018.6511291496588, 1168.701751190997, 526.1651539023024, 2006.3605651389698, 696.9010676174713, 769.6049013306531, 1326.3272601172664, 923.5114705431832, 710.7438066349537, 1161.7367728778772, 671.1752879756731, 669.2736435899017, 693.8873133465038, 551.6638740763531, 552.8977607794147, 575.9565393214756, 573.779514044154, 402.0179367006883, 276.1479506221487, 272.94369776952544, 236.32461025697899, 220.8695851817926, 213.0605996545435, 218.11423207770747, 192.56233158572257, 190.49562959514148, 169.91620174740788, 155.82275724777972, 151.19683319262833, 150.34680979281478, 135.62973978567896, 134.90506179742417, 130.49829359105368, 126.28775573492143, 121.41000260297618, 115.61040150030834, 108.39790673520756, 99.55883982725513, 98.82647671686306, 98.57308922563978, 96.07333900583939, 94.86261905847846, 91.74077118845415, 91.13646543629947, 89.62671326136326, 88.45513701761197, 88.44246344144328, 494.0868916336917, 317.32685180517876, 109.39129662324166, 428.0169035124709, 584.024180084409, 314.6392591716279, 216.50083006464175, 1737.133518418481, 404.6476846574765, 288.32118408756156, 227.22090318890847, 243.66336714800877, 2796.595839145457, 301.68174600532114, 678.3736453562811, 1108.343975520316, 572.4689954474181, 1068.411041952183, 1291.1060193451615, 1322.3656126849103, 1506.4323244014943, 2395.9823699742815, 1208.339301255679, 690.5342281633846, 549.0341704006595, 1028.4440066094767, 774.7444658145529, 1300.2736519619841, 917.7648026888992, 1922.922553093889, 745.120535368494, 554.8805468540693, 1474.3927904755376, 916.0768870761051, 662.974921822001, 851.3710921663977, 681.0344393730751, 817.0943473769935, 1101.2908973842016, 708.6794279509314, 919.3882427243192, 714.8470211196765, 758.6588176454992, 722.9096462346007, 681.2124041894862, 491.6826832383779, 238.35710195596266, 224.3473869151324, 219.64267433553857, 207.2112528100155, 183.5883242720344, 131.83694562904213, 127.98280958614562, 125.39000525153686, 120.85250056728853, 118.75457453265612, 117.86815471068964, 116.95718675637497, 115.60947288911709, 112.10335437895534, 103.9093352326, 102.92607829959128, 100.43079274841156, 95.89721164359892, 95.24481153730822, 94.68227328972964, 94.05821807245319, 92.29196528611834, 92.06644269600835, 91.21728498904635, 91.21195050629842, 90.39116146761997, 87.35924914598345, 86.75672209385318, 82.46259030454148, 319.7409377819575, 349.52555058283406, 465.57596246271555, 509.37043298003823, 145.43350379208198, 210.16885491506685, 150.63212764803365, 115.69027484622313, 189.42331308034062, 535.7944326196441, 233.1549587681535, 158.92460882216724, 208.97263084614238, 213.46578719313268, 408.1165604566716, 159.06628570869987, 519.4250055683779, 532.0811253360083, 1027.9069816357123, 233.40438745034038, 252.94348013034005, 1121.5267258349859, 399.6164862614857, 945.9817552589761, 227.71665573043092, 393.52049619326306, 840.0350914963617, 203.01473492121008, 300.32915930982887, 321.1656330703861, 425.0092899611594, 309.86740469727704, 257.2843223309943, 225.0502545573162, 247.10192824370503, 234.66091574622806, 219.74834940834, 1195.41809164825, 547.2725086565961, 284.94497139631545, 272.1271247481378, 218.52559124800766, 217.47821371538006, 216.15044001753859, 213.51661778566609, 212.0244436803778, 199.12200429096083, 181.767952670813, 164.02914101425694, 161.23325706112686, 149.29886450702392, 137.51068599802784, 131.11791807195672, 126.069856239459, 124.23734174926447, 117.16320837222682, 114.23024202039322, 108.79228888935496, 106.81594519852285, 106.81594519852285, 105.62036112169955, 104.90634638064388, 103.22897756274601, 101.63107791700189, 98.68494106926053, 97.64571816610855, 93.78545504719894, 1597.3851963268644, 139.4435178168656, 426.5666984487676, 243.2316260119576, 444.9375708201496, 227.85070238913167, 429.21892244828695, 1177.3465711090894, 412.1842707399958, 438.8151057820547, 709.5023075979215, 1882.4880290409528, 1297.724411351065, 188.5817610414121, 598.5777446546768, 365.0426039190369, 472.0038323040189, 566.2691026723436, 635.9863842276114, 986.2679702249017, 448.505298146192, 351.46228402879717, 373.73297716509205, 531.0582126735503, 423.1641869931196, 348.27194680044823, 304.0394517379569, 277.09510965779447, 341.28582957042033, 353.47567743241655, 279.55543830105023, 811.9188011653841, 269.90208177087305, 250.94486788685322, 242.14140577599986, 232.62605998835542, 186.5762474346594, 172.2189746385048, 166.31727206502268, 153.07540097988368, 139.71814981361638, 139.634451655755, 123.04842410039586, 120.82239581439553, 117.70884718949318, 112.46419419484022, 109.41582960389015, 106.78895644594098, 105.47871852268085, 103.17168047278868, 101.03152847687433, 97.35021084681063, 96.49201151862437, 90.10728146887293, 86.11787236673241, 86.11976871262027, 82.76105074305246, 81.59798144604467, 81.51004126143818, 77.56236700052426, 74.50708722055343, 310.5537972457859, 317.5899259159376, 509.57485645662564, 199.65928241603527, 443.31823708482614, 696.2934288388841, 408.2588075539512, 180.58042822470313, 309.6040706690848, 866.5792578171279, 1349.3379860796626, 327.34613047887996, 282.5178845425314, 321.2916540157983, 514.160266508531, 541.9921049772037, 326.37422655589205, 1191.7299224037743, 327.67260660546793, 392.8654749401932, 1114.2054747051602, 1013.2037470373588, 455.7759044106398, 579.6059200348609, 398.7499047728173, 548.6122865509262, 340.10541580687214, 781.9293627954595, 416.15656268341183, 417.41512708518974, 364.77454535865024, 349.3145588524807, 349.0184851868739, 313.0762571421992, 344.6652876805852, 246.82020861149553, 220.08274234479282, 209.82209362788913, 193.91332045683487, 186.29466559847398, 169.24458457206632, 143.89989178507165, 140.96036748386152, 131.2630603757021, 124.4819590860505, 121.0044211510241, 120.79772254791973, 114.69413412148842, 113.49542316703497, 110.3257492093504, 109.2408955262127, 108.81881842997349, 105.54859310956489, 102.42330939333827, 101.13087795424742, 98.43539532000253, 95.9076983091998, 95.12574766947174, 93.28318982398197, 90.1607947528298, 87.68214888906407, 87.56647051541879, 87.20834830022383, 85.9999527096271, 340.9116120222109, 347.86261308858377, 182.2255136154723, 355.2992135332193, 167.05312166148627, 908.2307594407545, 186.7678870628186, 363.67126051853126, 379.29045748697826, 341.6181439068432, 139.7227714593601, 480.1287031358585, 130.80306806851, 529.9820283055006, 177.92962392633774, 154.55752685500758, 316.8645209303461, 326.1410906037501, 240.94743967228052, 389.7394542356197, 372.9387316576911, 166.03143441279036, 276.01824228933623, 359.12269113682237, 256.76898134400955, 211.08711084838703, 447.10604756023065, 278.9543057983799, 380.436230880611, 236.91991487892645, 253.74519791739135, 276.0773827619231, 259.01115602367423, 238.1628416397488, 276.2783319844928, 211.8426938800564, 1187.6119052165711, 399.5254271952567, 399.0461057897195, 250.18750686858422, 250.08077823770452, 206.51880474294413, 201.58914382720127, 177.70757964759366, 144.84064138319752, 142.5106515651539, 131.33133139283046, 123.96119788132734, 121.98322861718738, 114.35846820680078, 106.76710458589271, 105.25105285529278, 99.96098754430412, 98.15572793137349, 95.73616139828378, 95.55241606711097, 92.0979080977198, 100.86302210433304, 88.59738663396033, 85.36939985340864, 84.75031836395029, 82.53920776277356, 82.22222801231558, 79.22211038607838, 78.63829646061293, 76.63021213602467, 328.3994116147177, 417.74840868016355, 189.48872231217229, 726.569516215007, 715.0786522464541, 192.86257911361113, 294.982793186113, 265.8628336057567, 652.3902958445708, 345.27336940874585, 530.7256531983841, 709.4371017884093, 270.4554149680944, 117.85371748783797, 494.1569875769216, 571.7895695162048, 268.34646629233305, 357.08227764254764, 560.8140062933442, 172.97260525942576, 308.2206204102551, 287.11001972613604, 199.26635995543515, 781.1781423641736, 327.4402900869324, 266.11698178792943, 408.206576810367, 290.48016017416256, 274.49255896788645, 354.4723393311838, 336.12771341051507, 273.77072975493246, 2312.747527722585, 1861.2727726411467, 274.6349749133265, 259.8511693886794, 250.3106380567172, 238.63903577186647, 203.11979444828594, 175.3064717927047, 166.81206758690968, 144.65194442437735, 136.7563742733689, 134.84181479138869, 128.2172358900844, 127.34153453482628, 118.45434866554947, 111.72186712926656, 106.38591014968732, 100.33560655630073, 91.64892087774759, 90.75089504931937, 89.87919347013782, 87.64030511341991, 86.89162252233433, 84.04423246332671, 81.48388265125018, 80.75885779202687, 80.15448371253761, 74.17453843287524, 72.1587486973166, 70.96918972280278, 139.67443313100165, 1768.120678687199, 4387.891261564121, 157.54361843518134, 3553.858782039506, 319.47766383167294, 201.3580665928293, 721.2023639372402, 350.8654469530676, 319.2819244094301, 524.2522210430789, 164.8815942317725, 403.43149039737875, 313.2068672551191, 353.33766166885744, 522.2940962679421, 732.9725421341635, 503.02927524618684, 305.64075202368315, 194.33176893324813, 197.11892492147723, 190.36417370347735, 338.34620871282624, 241.19721218963656, 359.75751534960983, 305.432398391348, 289.73842159950135, 272.5635657649285, 208.4473622476011, 208.10154294736833, 213.52380255352492], \"Total\": [10800.0, 11138.0, 3118.0, 2313.0, 3354.0, 3784.0, 7829.0, 1862.0, 5782.0, 2787.0, 8256.0, 8913.0, 6690.0, 1854.0, 3685.0, 2288.0, 3593.0, 3351.0, 2805.0, 4152.0, 1188.0, 1697.0, 1197.0, 2519.0, 3968.0, 1527.0, 3229.0, 2091.0, 2803.0, 1520.0, 534.437790408712, 341.53415026167465, 337.0799633421417, 335.4409844967158, 277.86716069868334, 264.3600965817174, 239.12188995436833, 237.21780035411197, 194.7715284906617, 194.41805318336364, 192.67380587869096, 183.18097683307786, 179.84303983827098, 173.00460480477395, 167.54944007556335, 160.42361690499212, 153.8516740799224, 153.65693255839557, 151.90444275562746, 150.3636088661415, 149.14777189146184, 140.3587893445036, 137.42026299967586, 137.42443400600283, 136.01578755578436, 135.8767426620726, 130.49402777110555, 129.17492269450045, 126.27434891197831, 125.04859886649922, 422.21522506362015, 350.58297651337995, 2519.298344723684, 727.3756300180231, 323.1872315616553, 3593.1608358407534, 451.3011412273393, 404.2540538555492, 311.5313202272107, 4152.67706022304, 262.46415204410187, 856.2279030273047, 1211.139466346745, 1869.8406234406639, 347.08407622417496, 383.45345141484665, 3248.139222978017, 416.2498515951344, 485.2956411675648, 1941.0474906724685, 649.697428362461, 345.05747101562315, 7829.93072422531, 826.7359694898987, 2969.504564485953, 2409.7638368883286, 1075.8240910490601, 1304.042957592363, 612.8472352501285, 2109.7733135208387, 1967.0252500056188, 2120.9685885936133, 1905.4470076615446, 11138.995177256493, 9487.561525857987, 2427.5021754262007, 1563.7507748664013, 1944.9381773027849, 1380.1685869348375, 2091.286706777151, 8913.692517001498, 5782.135012938475, 1901.9117608678987, 6690.610435030523, 10800.477672131423, 3968.737438323327, 3571.4990140220825, 3436.94731969534, 4948.606153331117, 2717.332859820628, 8256.821769170983, 3354.863051002517, 2288.1513311388508, 469.0309015758887, 432.2324205532034, 387.30719050403104, 378.70130145588377, 351.01767824831836, 322.5595682838888, 321.0629681150237, 314.3395049489443, 195.66364143594805, 189.08688282319514, 187.1522171718707, 181.11013203639592, 177.04155309778747, 174.6602559722588, 172.25258867286126, 167.6741106378377, 159.59440614243405, 152.92759766334188, 149.86372989825554, 148.6625217566814, 148.44677977806307, 147.5275101330403, 145.70336180938372, 144.69732466930068, 138.12250204371537, 125.72775707944717, 125.48850947862766, 122.66950096281361, 386.9465092715647, 207.40125825801348, 454.23168996388165, 268.85545320684054, 404.7178258778347, 934.9000344072184, 375.8547112567619, 1403.281925263908, 1074.775910548537, 1607.2818182046785, 520.4285925829705, 1436.6083170776, 1011.6061556404314, 479.9146775337777, 610.8037818081864, 1126.9525079238042, 1007.8701961466898, 325.62191653554504, 2754.1490720843485, 8913.692517001498, 1026.3651329900372, 2298.138171999138, 788.7849100297251, 1115.3814202242727, 1497.7219585279804, 1562.0057566511368, 10800.477672131423, 8256.821769170983, 1296.1602413441146, 3174.0605496600415, 2465.0663148634985, 1728.2009174247512, 3436.94731969534, 794.2397070554684, 1073.3238016813357, 1772.719637246523, 9487.561525857987, 2040.0298678511679, 6690.610435030523, 1697.3713316132546, 435.7816552841595, 433.2434165800799, 375.2701442749645, 294.5871486071859, 271.81672010434863, 264.2664667305937, 262.68859838109415, 238.4050854255883, 235.87455034726938, 232.50439597837845, 228.4777673586272, 202.1060701944322, 184.6729448169562, 175.89837649478886, 174.60898991122625, 170.99620627652988, 164.71684378068085, 155.19919062087195, 152.32759647623206, 143.47102428799585, 137.13952339791734, 135.82827168963362, 134.82045723472092, 130.8011747268025, 128.8008642879517, 126.41004606598402, 122.71295086826808, 121.12643480256885, 117.30918579632336, 2787.581712410836, 465.57590958567977, 606.4529618709187, 470.17434305797053, 3784.988434084089, 330.96169129725456, 235.9610765443837, 416.90295689118017, 609.220781118595, 1264.8610190976176, 527.6364102072774, 3351.256458993092, 740.1101896736159, 718.4107918651479, 262.8018354916579, 519.4850292487573, 474.3670500155169, 1016.2654647858135, 572.5838336550728, 3229.0364745079414, 814.3407410468368, 1098.764042687454, 3741.779707385314, 2040.0298678511679, 2195.538780080164, 2357.7976863106155, 3014.727029546151, 839.1218788013483, 9487.561525857987, 1432.8079014432744, 2009.83147527543, 6690.610435030523, 3174.0605496600415, 1720.336697906679, 8913.692517001498, 1728.2009174247512, 1928.801368126858, 3073.3460436370688, 1182.4950095286638, 1540.5875206023388, 2732.2110832047993, 4948.606153331117, 402.93286352220684, 277.0628178930993, 273.8586226893846, 237.23957308891693, 221.78446107624188, 213.9755197435466, 219.13178524159224, 193.47733068753314, 191.41135076727804, 170.83119062843497, 156.73765611414512, 152.11183171708566, 151.26205502838934, 136.5447881841129, 135.82028581281517, 131.41333027571275, 127.20277056181513, 122.32495902570425, 116.52530782226066, 109.31275624031419, 100.47378416187352, 99.74151184324896, 99.48801282173767, 96.98825663696813, 95.7777059573891, 92.65568543598266, 92.0515123143746, 90.54201365501694, 89.37010415620864, 89.35743230656904, 505.1536095152438, 340.7505473789995, 111.82674461243855, 489.17818053715604, 707.3627484116914, 360.07753130818156, 244.01343205309936, 2805.762437447019, 517.8268993206577, 348.3681734710827, 266.8553506460748, 292.92785778947143, 5782.135012938475, 381.8357366988796, 1076.8762196656867, 2070.2076936520943, 925.1337559388949, 2135.0711056939517, 2803.381100347566, 3229.0364745079414, 3968.737438323327, 7829.93072422531, 3571.4990140220825, 1510.5729192717392, 1082.7884199516207, 3014.727029546151, 1967.0252500056188, 4948.606153331117, 2732.2110832047993, 11138.995177256493, 2009.6562901719, 1182.4950095286638, 8256.821769170983, 3351.256458993092, 1729.5077690294472, 3073.3460436370688, 1901.9117608678987, 3685.614358567647, 9487.561525857987, 2723.921959986824, 6690.610435030523, 2969.504564485953, 3741.779707385314, 3174.0605496600415, 3436.94731969534, 492.5914019171186, 239.2657977130068, 225.25614709770687, 220.55141224529362, 208.12004100022997, 184.49720691854935, 132.74571355116387, 128.89684116198848, 126.29969735349182, 121.76122427418782, 119.66328308186701, 118.77693912148429, 117.86602655586151, 116.51821848152215, 113.01206827256172, 104.81802652669114, 103.83485842644049, 101.33962005449975, 96.80589335789877, 96.15392577872456, 95.5912004969547, 94.9669054329155, 93.20070689003943, 92.97519896224216, 92.12596199460332, 92.12076456494792, 91.29992888994776, 88.26809315730601, 87.66628292373962, 83.37138742830236, 333.03297453520895, 371.8032278940311, 511.1625801037774, 566.7173360933012, 151.26737662635745, 233.36152845892997, 164.1749758284999, 123.60215400682094, 233.75918656457205, 939.0977068290847, 317.4643615300989, 194.7843295771337, 285.6449589762953, 310.55913494189684, 844.579651345341, 202.4169454810142, 1540.5875206023388, 1629.026375207253, 6690.610435030523, 430.12309108123065, 503.15491601300477, 9487.561525857987, 1313.9929833290134, 8256.821769170983, 427.539134895194, 1393.966384405277, 8913.692517001498, 405.12897683703534, 1419.8898595900182, 1772.719637246523, 4948.606153331117, 3073.3460436370688, 1431.5199989360513, 734.2635115835465, 3741.779707385314, 2465.0663148634985, 1178.3061992652129, 1197.271212480775, 548.1921039433864, 285.8645023813908, 273.04633277528, 219.44493461254646, 218.39752168486714, 217.06973953175455, 214.43581733628372, 212.94384177497255, 200.0414595807233, 182.6875839056107, 164.9484631875107, 162.1527866711588, 150.21825667765245, 138.430203072338, 132.03709663464187, 126.98928345404734, 125.15658618239138, 118.09017394163608, 115.14945293025036, 109.71162600826955, 107.73506551858655, 107.73506551858655, 106.53959936257695, 105.82568576633454, 104.14855776219946, 102.55035419869196, 99.60434040403773, 98.56490387366551, 94.70468590168916, 1854.4412759865907, 153.84487000087623, 630.3089724712778, 325.5154231950774, 766.437954132244, 312.1043276920313, 780.8832489362305, 3685.614358567647, 797.5958756856808, 882.867515148956, 2091.286706777151, 11138.995177256493, 8256.821769170983, 264.4089257151417, 2211.1449717531164, 969.0660364047695, 1698.366566456584, 2805.762437447019, 3968.737438323327, 10800.477672131423, 2009.83147527543, 1120.3457774102797, 1462.2489332439386, 4948.606153331117, 2803.381100347566, 1555.9528633777693, 1025.9379636308608, 712.9724083697602, 2717.332859820628, 5782.135012938475, 1607.2818182046785, 812.8882884700728, 270.81179369639887, 251.85450611821182, 243.05108252738276, 233.53592569613122, 187.48589452495756, 173.12886282549997, 167.22713758855417, 153.98525393200433, 140.62776583203095, 140.54413705117875, 123.9581584599685, 121.74026026532663, 118.61856765157508, 113.3738733353323, 110.32551704530675, 107.69857831870544, 106.38835956154702, 104.08146081217093, 101.94160379968056, 98.25989987658373, 97.40172404428334, 91.01697082979545, 87.02749493641056, 87.02942503841868, 83.67112026913478, 82.50776267366388, 82.41998865713842, 78.47201932168701, 75.4169034995687, 333.7319379538081, 348.4885149426554, 597.1005701836262, 231.27196583717134, 627.2148758897966, 1129.5347365564414, 592.5882303449346, 216.96210972153372, 440.6258217859071, 1698.732259932933, 3118.267198653342, 497.192694698633, 487.2648697466412, 617.2280981335132, 1415.0475479766792, 1556.2426466886966, 681.4766179357407, 6690.610435030523, 699.7092736773297, 1029.9182520757086, 8256.821769170983, 7829.93072422531, 1510.5729192717392, 2717.332859820628, 1204.0603930788443, 2803.381100347566, 863.5363269603405, 9487.561525857987, 1570.2645421379127, 2211.1449717531164, 2216.30925646877, 3685.614358567647, 11138.995177256493, 1512.055664962651, 345.5724723515969, 247.72727552540056, 220.9899257588682, 210.7293734966761, 194.82049532946172, 187.20171977590599, 170.15173911015688, 144.8069530127334, 141.86766645215988, 132.1702497139633, 125.38910939256822, 121.91151017111753, 121.70479186669326, 115.60119006260126, 114.40264650290224, 111.23300456151563, 110.14814061961522, 109.72586870226412, 106.45600813643269, 103.33068434012407, 102.03817617878427, 99.3425630924873, 96.8149841328207, 96.03289081525328, 94.19045650788631, 91.08438591518495, 88.5893575831175, 88.47359514031814, 88.11546853717407, 86.90710767824649, 355.14651853996503, 386.21148686684273, 194.9844469747051, 415.29640148176856, 186.64578928443703, 1527.539247927401, 224.33606792351216, 557.3933100134378, 590.760353742145, 534.1243596604731, 161.83595939938212, 1335.4176152852403, 154.0143190207773, 2357.7976863106155, 286.9158048591971, 219.25923022299298, 1139.0258460015384, 1268.721600531067, 642.6581889394296, 2135.0711056939517, 1928.801368126858, 265.1927458317091, 1063.695720015579, 2468.208742490892, 949.3865557424562, 563.3913181942306, 10800.477672131423, 1944.9381773027849, 9487.561525857987, 1019.2913906677386, 1486.5245530765835, 2732.2110832047993, 2216.30925646877, 1562.0057566511368, 8913.692517001498, 1772.719637246523, 1188.5210943616657, 400.4346352477866, 399.9553355819791, 251.09675153946282, 250.9901002141875, 207.42809096612896, 202.4982990226861, 178.61672291923014, 145.74983500374591, 143.41995224482886, 132.24059340300582, 124.87027993252423, 122.8924159913242, 115.26766208142692, 107.67634131476585, 106.16026995624141, 100.87041939384474, 99.06505037124133, 96.64534586669359, 96.46163468461945, 93.00722353889014, 101.86899754112035, 89.50659744365261, 86.27853653895822, 85.66003721768186, 83.44857599677259, 83.13145735580643, 80.13125248367125, 79.54749016375818, 77.5394541711843, 366.36777702092314, 494.68137463905236, 210.37224230310645, 974.7697889154673, 968.2500648033446, 218.89576206854315, 357.01541860537094, 316.95838249359747, 913.4934846899419, 436.99259302865545, 730.8129038018354, 1155.671327438267, 383.3915246428768, 126.07787796460477, 903.4754716333317, 1139.6788108055503, 424.4353966790946, 652.988912998224, 1680.6342394557075, 223.22498723690754, 656.755680502143, 593.2956326009113, 314.44802319687216, 8913.692517001498, 1131.1687478922966, 675.285187505921, 9487.561525857987, 1928.801368126858, 1517.6970557114635, 10800.477672131423, 11138.995177256493, 2754.1490720843485, 2313.659389897694, 1862.1846414123868, 275.54753155210255, 260.7630181306618, 251.22249547112085, 239.55099587689344, 204.03190897251457, 176.2183182307872, 167.72409242141754, 145.56381620615542, 137.66845629882917, 135.75383792393774, 129.12948143714885, 128.2534770306668, 119.36620089516138, 112.63397276829137, 107.29801367148652, 101.24753026895912, 92.56078309163239, 91.66324862408555, 90.79126457332636, 88.55230668432907, 87.80364711491377, 84.9562556203574, 82.39582074126183, 81.67071897398358, 81.06650993599162, 75.0869273540983, 73.07080356299537, 71.88126479216152, 155.32738492541787, 3118.267198653342, 10800.477672131423, 188.63371091414862, 11138.995177256493, 493.1835566957014, 270.3909850975578, 1520.3565361256085, 598.1229554529255, 539.1236294997308, 1165.128857590148, 230.7757751964162, 949.3865557424562, 829.3280746366117, 1154.7652452158527, 2723.921959986824, 5782.135012938475, 3685.614358567647, 1150.285761852247, 377.5318703159203, 395.69426821616094, 366.2415020413957, 2717.332859820628, 862.9645431953663, 3968.737438323327, 2969.504564485953, 4948.606153331117, 3436.94731969534, 726.3566060469294, 913.4934846899419, 9487.561525857987], \"Category\": [\"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\"], \"logprob\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, -6.0193, -6.4681, -6.4812, -6.4861, -6.675, -6.725, -6.8257, -6.8337, -7.0317, -7.0335, -7.0426, -7.0933, -7.1118, -7.1508, -7.183, -7.2267, -7.2688, -7.2701, -7.2816, -7.2919, -7.3, -7.3611, -7.3824, -7.3824, -7.3928, -7.3938, -7.4345, -7.4448, -7.4676, -7.4774, -6.2661, -6.4593, -4.534, -5.7623, -6.5525, -4.2485, -6.2425, -6.3543, -6.6045, -4.1733, -6.7724, -5.6755, -5.3745, -5.0061, -6.5373, -6.4498, -4.5721, -6.3783, -6.2599, -5.1082, -6.0247, -6.5583, -3.9985, -5.8494, -4.8447, -5.0298, -5.6785, -5.5536, -6.1108, -5.2969, -5.3481, -5.303, -5.3887, -4.2616, -4.3846, -5.2546, -5.5301, -5.4133, -5.639, -5.3984, -4.6238, -4.884, -5.4937, -4.898, -4.728, -5.2472, -5.2987, -5.3193, -5.26, -5.4523, -5.47, -3.8919, -4.2746, -5.861, -5.943, -6.0529, -6.0754, -6.1515, -6.2363, -6.241, -6.2622, -6.738, -6.7724, -6.7827, -6.8157, -6.8385, -6.8521, -6.8661, -6.8932, -6.9428, -6.9858, -7.0061, -7.0142, -7.0157, -7.0219, -7.0345, -7.0414, -7.0882, -7.1829, -7.1848, -7.2077, -6.0861, -6.6988, -5.9508, -6.4681, -6.0922, -5.3236, -6.1667, -4.9935, -5.2644, -4.9275, -5.9144, -5.0313, -5.3508, -6.0059, -5.8171, -5.3178, -5.4615, -6.3547, -4.7115, -3.8241, -5.5114, -4.9416, -5.7139, -5.4761, -5.3136, -5.3192, -4.2047, -4.3884, -5.4556, -5.0982, -5.2391, -5.4055, -5.1581, -5.7719, -5.7187, -5.616, -5.4215, -5.6829, -5.6511, -4.4597, -5.8209, -5.8268, -5.9707, -6.2135, -6.2942, -6.3224, -6.3285, -6.4258, -6.4365, -6.451, -6.4685, -6.5917, -6.6823, -6.7312, -6.7386, -6.7596, -6.7973, -6.8571, -6.8759, -6.9362, -6.9816, -6.9913, -6.9988, -7.0293, -7.0448, -7.0636, -7.0936, -7.1067, -7.1389, -3.9813, -5.7702, -5.5298, -5.7927, -3.7935, -6.1419, -6.4638, -5.9299, -5.5856, -4.9143, -5.7748, -4.2038, -5.5031, -5.5545, -6.3995, -5.8776, -5.9543, -5.4215, -5.8395, -4.6238, -5.5981, -5.3952, -4.5633, -5.0202, -5.0088, -4.9697, -4.8323, -5.6303, -4.2919, -5.3493, -5.2501, -4.7058, -5.0678, -5.3296, -4.8383, -5.3869, -5.3898, -5.3536, -5.583, -5.5808, -5.5399, -5.5437, -5.7883, -6.1639, -6.1755, -6.3196, -6.3872, -6.4232, -6.3998, -6.5244, -6.5352, -6.6495, -6.7361, -6.7662, -6.7719, -6.8749, -6.8802, -6.9134, -6.9462, -6.9856, -7.0346, -7.099, -7.184, -7.1914, -7.194, -7.2197, -7.2324, -7.2658, -7.2724, -7.2891, -7.3023, -7.3024, -5.5821, -6.0249, -7.0899, -5.7256, -5.4149, -6.0334, -6.4072, -4.3248, -5.7818, -6.1207, -6.3589, -6.289, -3.8486, -6.0754, -5.2651, -4.7742, -5.4348, -4.8109, -4.6215, -4.5976, -4.4673, -4.0032, -4.6878, -5.2473, -5.4766, -4.849, -5.1323, -4.6145, -4.9629, -4.2232, -5.1713, -5.466, -4.4888, -4.9647, -5.2881, -5.0379, -5.2612, -5.079, -4.7806, -5.2214, -4.9611, -5.2127, -5.1532, -5.2015, -5.2609, -4.9307, -5.6548, -5.7154, -5.7365, -5.7948, -5.9159, -6.247, -6.2767, -6.2971, -6.334, -6.3515, -6.359, -6.3667, -6.3783, -6.4091, -6.485, -6.4945, -6.5191, -6.5653, -6.5721, -6.578, -6.5846, -6.6036, -6.606, -6.6153, -6.6154, -6.6244, -6.6585, -6.6654, -6.7162, -5.361, -5.272, -4.9853, -4.8954, -6.1488, -5.7806, -6.1137, -6.3776, -5.8846, -4.8448, -5.6768, -6.0601, -5.7863, -5.7651, -5.117, -6.0592, -4.8758, -4.8518, -4.1933, -5.6758, -5.5954, -4.1061, -5.138, -4.2763, -5.7004, -5.1534, -4.3951, -5.8153, -5.4237, -5.3566, -5.0764, -5.3924, -5.5784, -5.7122, -5.6187, -5.6704, -5.7361, -4.0124, -4.7937, -5.4464, -5.4924, -5.7118, -5.7166, -5.7227, -5.7349, -5.742, -5.8047, -5.8959, -5.9986, -6.0158, -6.0927, -6.175, -6.2226, -6.2618, -6.2765, -6.3351, -6.3604, -6.4092, -6.4276, -6.4276, -6.4388, -6.4456, -6.4617, -6.4773, -6.5067, -6.5173, -6.5577, -3.7225, -6.161, -5.0429, -5.6046, -5.0007, -5.67, -5.0367, -4.0276, -5.0772, -5.0146, -4.5341, -3.5583, -3.9303, -5.8591, -4.7041, -5.1986, -4.9417, -4.7596, -4.6435, -4.2047, -4.9927, -5.2366, -5.1751, -4.8238, -5.0509, -5.2457, -5.3815, -5.4743, -5.2659, -5.2308, -5.4655, -4.3892, -5.4905, -5.5633, -5.5991, -5.6391, -5.8597, -5.9398, -5.9747, -6.0576, -6.149, -6.1495, -6.276, -6.2943, -6.3204, -6.3659, -6.3934, -6.4177, -6.4301, -6.4522, -6.4731, -6.5103, -6.5191, -6.5876, -6.6329, -6.6328, -6.6726, -6.6868, -6.6879, -6.7375, -6.7777, -5.3502, -5.3278, -4.855, -5.792, -4.9943, -4.5428, -5.0767, -5.8924, -5.3533, -4.324, -3.8812, -5.2976, -5.4448, -5.3162, -4.846, -4.7933, -5.3005, -4.0054, -5.2966, -5.1151, -4.0727, -4.1677, -4.9666, -4.7262, -5.1002, -4.7812, -5.2593, -4.4268, -5.0575, -5.0545, -5.1893, -5.2326, -5.2335, -5.3421, -5.1584, -5.4924, -5.607, -5.6548, -5.7336, -5.7737, -5.8697, -6.0319, -6.0525, -6.1238, -6.1769, -6.2052, -6.2069, -6.2588, -6.2693, -6.2976, -6.3075, -6.3113, -6.3419, -6.3719, -6.3846, -6.4116, -6.4376, -6.4458, -6.4654, -6.4994, -6.5273, -6.5286, -6.5327, -6.5467, -5.1694, -5.1492, -5.7958, -5.1281, -5.8827, -4.1895, -5.7712, -5.1048, -5.0627, -5.1673, -6.0614, -4.827, -6.1273, -4.7282, -5.8196, -5.9605, -5.2425, -5.2137, -5.5164, -5.0355, -5.0796, -5.8888, -5.3806, -5.1174, -5.4528, -5.6488, -4.8982, -5.37, -5.0597, -5.5333, -5.4647, -5.3803, -5.4442, -5.5281, -5.3796, -5.6452, -3.819, -4.9084, -4.9096, -5.3765, -5.3769, -5.5683, -5.5925, -5.7186, -5.9231, -5.9393, -6.021, -6.0788, -6.0948, -6.1594, -6.2281, -6.2424, -6.2939, -6.3122, -6.3371, -6.339, -6.3759, -6.285, -6.4146, -6.4517, -6.459, -6.4854, -6.4893, -6.5265, -6.5339, -6.5597, -5.1045, -4.8638, -5.6544, -4.3104, -4.3263, -5.6367, -5.2118, -5.3157, -4.4181, -5.0544, -4.6245, -4.3343, -5.2986, -6.1293, -4.6959, -4.55, -5.3064, -5.0208, -4.5693, -5.7456, -5.1679, -5.2389, -5.6041, -4.2379, -5.1074, -5.3148, -4.8869, -5.2272, -5.2838, -5.0281, -5.0812, -5.2864, -3.1185, -3.3356, -5.2492, -5.3045, -5.342, -5.3897, -5.5509, -5.6981, -5.7478, -5.8903, -5.9465, -5.9606, -6.0109, -6.0178, -6.0901, -6.1486, -6.1976, -6.2561, -6.3467, -6.3565, -6.3662, -6.3914, -6.4, -6.4333, -6.4643, -6.4732, -6.4807, -6.5582, -6.5858, -6.6024, -5.9253, -3.387, -2.4781, -5.805, -2.6889, -5.098, -5.5596, -4.2837, -5.0043, -5.0986, -4.6027, -5.7594, -4.8646, -5.1178, -4.9972, -4.6064, -4.2675, -4.644, -5.1422, -5.5951, -5.5808, -5.6157, -5.0406, -5.379, -4.9792, -5.1429, -5.1957, -5.2568, -5.525, -5.5266, -5.5009], \"loglift\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, 1.5422, 1.5413, 1.5412, 1.5412, 1.5406, 1.5405, 1.5401, 1.5401, 1.5392, 1.5392, 1.5392, 1.5389, 1.5388, 1.5386, 1.5385, 1.5382, 1.538, 1.538, 1.5379, 1.5378, 1.5378, 1.5374, 1.5373, 1.5373, 1.5372, 1.5372, 1.5369, 1.5368, 1.5367, 1.5366, 1.5311, 1.5239, 1.477, 1.491, 1.512, 1.4074, 1.4881, 1.4864, 1.4967, 1.3379, 1.5003, 1.4147, 1.3689, 1.3031, 1.4559, 1.4438, 1.1848, 1.4331, 1.3981, 1.1636, 1.3415, 1.4408, 0.8786, 1.2759, 1.0019, 1.0257, 1.1835, 1.116, 1.3139, 0.8916, 0.9104, 0.8801, 0.9016, 0.263, 0.3005, 0.7936, 0.9578, 0.8564, 0.9739, 0.7989, 0.1236, 0.2962, 0.7985, 0.1363, -0.1726, 0.3094, 0.3633, 0.3811, 0.076, 0.4831, -0.646, 1.8327, 1.8326, 1.8311, 1.8308, 1.8307, 1.8306, 1.8304, 1.8302, 1.8302, 1.8301, 1.8283, 1.8282, 1.8281, 1.828, 1.8278, 1.8278, 1.8277, 1.8276, 1.8273, 1.827, 1.8269, 1.8269, 1.8268, 1.8268, 1.8267, 1.8267, 1.8264, 1.8257, 1.8257, 1.8256, 1.7984, 1.8093, 1.7734, 1.7805, 1.7474, 1.6787, 1.7469, 1.6027, 1.5985, 1.5329, 1.6737, 1.5415, 1.5727, 1.6632, 1.6109, 1.4977, 1.4657, 1.7023, 1.2104, 0.9233, 1.3976, 1.1613, 1.4584, 1.3497, 1.2174, 1.1699, 0.3507, 0.4356, 1.22, 0.6817, 0.7936, 0.9825, 0.5424, 1.3935, 1.1456, 0.7465, -0.7365, 0.5391, -0.6168, 1.9462, 1.9447, 1.9447, 1.9444, 1.9437, 1.9435, 1.9434, 1.9433, 1.943, 1.9429, 1.9429, 1.9428, 1.9423, 1.9419, 1.9416, 1.9416, 1.9415, 1.9413, 1.9409, 1.9408, 1.9404, 1.9402, 1.9401, 1.94, 1.9398, 1.9397, 1.9396, 1.9394, 1.9393, 1.939, 1.9285, 1.9293, 1.9053, 1.897, 1.8104, 1.8988, 1.9153, 1.88, 1.845, 1.7858, 1.7995, 1.5219, 1.7328, 1.7112, 1.8719, 1.7123, 1.7265, 1.4973, 1.6531, 1.139, 1.5423, 1.4456, 1.0522, 1.2018, 1.1398, 1.1076, 0.9992, 1.4801, 0.3932, 1.2261, 0.9869, 0.3285, 0.7122, 1.0629, -0.0909, 1.001, 0.8884, 0.4586, 1.1844, 0.9221, 0.39, -0.2078, 2.0557, 2.0547, 2.0546, 2.0541, 2.0538, 2.0537, 2.0533, 2.0532, 2.0532, 2.0526, 2.0521, 2.0519, 2.0519, 2.0512, 2.0512, 2.051, 2.0507, 2.0505, 2.0501, 2.0496, 2.0488, 2.0487, 2.0487, 2.0485, 2.0484, 2.048, 2.048, 2.0478, 2.0477, 2.0477, 2.0358, 1.9867, 2.0359, 1.9244, 1.8664, 1.9231, 1.9383, 1.5785, 1.8113, 1.8688, 1.8972, 1.8738, 1.3316, 1.8223, 1.5958, 1.4332, 1.578, 1.3656, 1.2826, 1.1652, 1.0893, 0.8738, 0.9742, 1.2752, 1.3788, 0.9825, 1.1262, 0.7214, 0.967, 0.3014, 1.0658, 1.3013, 0.3352, 0.761, 1.0991, 0.7743, 1.031, 0.5515, -0.0955, 0.7115, 0.0732, 0.6339, 0.4622, 0.5785, 0.4395, 2.7124, 2.7104, 2.7102, 2.7101, 2.7098, 2.7093, 2.7073, 2.7071, 2.707, 2.7067, 2.7066, 2.7065, 2.7065, 2.7064, 2.7061, 2.7055, 2.7054, 2.7052, 2.7048, 2.7047, 2.7047, 2.7046, 2.7044, 2.7044, 2.7043, 2.7043, 2.7042, 2.7039, 2.7038, 2.7033, 2.6735, 2.6524, 2.6208, 2.6075, 2.6749, 2.6095, 2.6281, 2.6481, 2.5039, 2.153, 2.4056, 2.5108, 2.4017, 2.3393, 1.9869, 2.4732, 1.627, 1.5953, 0.841, 2.1029, 2.0265, 0.5789, 1.5239, 0.5476, 2.0843, 1.4494, 0.3523, 2.0233, 1.1608, 1.0059, 0.2595, 0.4198, 0.9979, 1.5317, -0.0033, 0.3624, 1.0349, 2.7426, 2.7424, 2.7409, 2.7407, 2.7399, 2.7399, 2.7399, 2.7398, 2.7398, 2.7395, 2.7391, 2.7385, 2.7384, 2.738, 2.7374, 2.7371, 2.7368, 2.7367, 2.7362, 2.7361, 2.7357, 2.7355, 2.7355, 2.7354, 2.7354, 2.7352, 2.7351, 2.7348, 2.7347, 2.7343, 2.5949, 2.6458, 2.3537, 2.4527, 2.2003, 2.4295, 2.1456, 1.6029, 2.084, 2.045, 1.6631, 0.9662, 0.8937, 2.4061, 1.4374, 1.7678, 1.4637, 1.1437, 0.9131, 0.3507, 1.2442, 1.5848, 1.3799, 0.5121, 0.8533, 1.2472, 1.5279, 1.799, 0.6694, -0.0506, 0.995, 2.753, 2.7508, 2.7506, 2.7504, 2.7503, 2.7493, 2.7489, 2.7487, 2.7483, 2.7477, 2.7477, 2.7468, 2.7466, 2.7465, 2.7461, 2.7459, 2.7457, 2.7456, 2.7454, 2.7452, 2.7449, 2.7448, 2.7441, 2.7437, 2.7437, 2.7432, 2.7431, 2.7431, 2.7425, 2.742, 2.6822, 2.6613, 2.5957, 2.6072, 2.4072, 2.2704, 2.3816, 2.5706, 2.4013, 2.0811, 1.9165, 2.3362, 2.2091, 2.1013, 1.7418, 1.6994, 2.018, 1.0289, 1.9955, 1.7904, 0.7513, 0.7093, 1.5559, 1.2091, 1.6491, 1.123, 1.8224, 0.2582, 1.4262, 1.087, 0.9499, 0.398, -0.7089, 1.1794, 2.8391, 2.8381, 2.8376, 2.8374, 2.8371, 2.8369, 2.8364, 2.8355, 2.8353, 2.8349, 2.8345, 2.8343, 2.8343, 2.8339, 2.8338, 2.8335, 2.8335, 2.8334, 2.8332, 2.8329, 2.8328, 2.8326, 2.8323, 2.8322, 2.8321, 2.8315, 2.8314, 2.8314, 2.8314, 2.8312, 2.8008, 2.7372, 2.7741, 2.6857, 2.7308, 2.3218, 2.6585, 2.4147, 2.3986, 2.3948, 2.6948, 1.8188, 2.6784, 1.3491, 2.3639, 2.4921, 1.5623, 1.4833, 1.8607, 1.141, 1.1985, 2.3735, 1.4927, 0.9142, 1.5341, 1.86, -0.3428, 0.8998, -0.3747, 1.3826, 1.0739, 0.5496, 0.695, 0.961, -0.6322, 0.7173, 2.9433, 2.9418, 2.9418, 2.9404, 2.9404, 2.9396, 2.9395, 2.9389, 2.9378, 2.9377, 2.9371, 2.9367, 2.9366, 2.9361, 2.9356, 2.9354, 2.935, 2.9348, 2.9346, 2.9346, 2.9342, 2.9341, 2.9338, 2.9334, 2.9334, 2.9331, 2.933, 2.9326, 2.9325, 2.9322, 2.8346, 2.775, 2.8395, 2.6502, 2.6409, 2.8174, 2.7532, 2.7683, 2.6074, 2.7085, 2.6241, 2.4561, 2.5951, 2.8766, 2.3406, 2.2543, 2.4856, 2.3404, 1.8465, 2.689, 2.1875, 2.2182, 2.4879, 0.5095, 1.7043, 2.0128, -0.2019, 1.0509, 1.234, -0.4727, -0.5567, 0.6355, 2.9777, 2.9776, 2.9748, 2.9746, 2.9745, 2.9743, 2.9736, 2.9729, 2.9727, 2.9718, 2.9715, 2.9714, 2.971, 2.971, 2.9704, 2.97, 2.9696, 2.9691, 2.9682, 2.9681, 2.968, 2.9678, 2.9677, 2.9673, 2.967, 2.9669, 2.9668, 2.9659, 2.9655, 2.9653, 2.8719, 2.4107, 2.0774, 2.798, 1.8357, 2.5439, 2.6833, 2.2323, 2.4447, 2.4542, 2.1795, 2.6419, 2.1223, 2.0044, 1.7939, 1.3265, 0.9127, 0.9866, 1.6528, 2.314, 2.2813, 2.3238, 0.8948, 1.7033, 0.5773, 0.7037, 0.1402, 0.4436, 1.7298, 1.4989, -0.8159]}, \"token.table\": {\"Topic\": [1, 7, 8, 7, 1, 5, 1, 3, 8, 5, 8, 10, 4, 9, 4, 1, 2, 3, 4, 5, 6, 8, 3, 4, 2, 6, 10, 1, 3, 4, 5, 6, 7, 10, 4, 1, 4, 6, 10, 6, 1, 6, 7, 10, 4, 8, 1, 2, 3, 4, 6, 8, 9, 3, 8, 9, 6, 3, 9, 4, 9, 7, 5, 2, 5, 7, 8, 10, 5, 2, 3, 3, 2, 3, 5, 1, 3, 1, 4, 5, 8, 7, 10, 2, 3, 4, 5, 6, 8, 5, 7, 6, 2, 7, 8, 10, 8, 6, 1, 3, 8, 9, 7, 2, 2, 9, 3, 1, 2, 4, 6, 8, 9, 10, 2, 5, 8, 10, 4, 8, 2, 3, 4, 6, 7, 2, 3, 2, 5, 8, 1, 6, 1, 8, 1, 8, 9, 1, 9, 10, 1, 2, 3, 8, 9, 10, 10, 6, 8, 4, 5, 1, 3, 7, 10, 2, 8, 9, 9, 4, 6, 1, 2, 5, 7, 9, 9, 3, 4, 8, 3, 5, 5, 9, 10, 1, 2, 3, 4, 6, 8, 10, 1, 4, 6, 9, 6, 3, 1, 1, 4, 1, 8, 5, 6, 8, 7, 9, 2, 3, 9, 10, 1, 2, 3, 4, 5, 6, 7, 9, 1, 2, 4, 5, 7, 8, 1, 4, 5, 6, 8, 9, 6, 7, 6, 1, 5, 6, 5, 2, 3, 5, 1, 3, 4, 6, 7, 8, 9, 1, 4, 5, 7, 9, 1, 2, 3, 4, 5, 6, 7, 9, 9, 1, 4, 6, 1, 3, 8, 2, 3, 6, 9, 9, 5, 1, 3, 4, 5, 6, 9, 10, 2, 3, 3, 4, 7, 3, 6, 10, 3, 4, 1, 9, 5, 8, 4, 6, 3, 1, 3, 4, 10, 3, 9, 1, 2, 3, 4, 5, 6, 8, 4, 8, 9, 1, 3, 4, 5, 6, 7, 2, 3, 8, 9, 1, 2, 3, 6, 9, 1, 10, 1, 3, 4, 8, 9, 1, 1, 3, 5, 7, 9, 10, 1, 3, 1, 2, 6, 1, 2, 2, 2, 2, 8, 1, 3, 5, 8, 1, 4, 1, 2, 7, 3, 8, 1, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 3, 4, 6, 2, 2, 1, 4, 5, 7, 8, 9, 10, 1, 2, 3, 4, 5, 7, 8, 2, 3, 2, 5, 3, 6, 7, 8, 4, 10, 2, 2, 3, 6, 8, 1, 3, 4, 5, 6, 7, 8, 9, 10, 4, 1, 1, 2, 3, 4, 8, 10, 10, 1, 4, 8, 1, 2, 3, 7, 3, 6, 3, 7, 7, 5, 1, 3, 4, 6, 7, 4, 3, 2, 5, 4, 7, 1, 4, 6, 4, 3, 4, 5, 7, 10, 2, 5, 2, 8, 1, 2, 4, 5, 6, 8, 1, 6, 2, 4, 7, 2, 9, 1, 2, 3, 4, 5, 6, 7, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 9, 2, 6, 7, 8, 4, 5, 6, 1, 2, 3, 5, 6, 1, 3, 4, 6, 1, 2, 3, 4, 6, 7, 2, 6, 7, 7, 6, 5, 8, 2, 3, 5, 8, 9, 1, 4, 6, 7, 8, 7, 7, 8, 4, 8, 1, 2, 3, 5, 8, 1, 2, 3, 6, 7, 8, 9, 10, 3, 4, 3, 5, 1, 3, 4, 3, 4, 5, 7, 9, 1, 2, 3, 4, 5, 7, 8, 9, 9, 3, 7, 7, 1, 3, 5, 8, 10, 1, 1, 1, 2, 3, 5, 7, 8, 10, 9, 9, 7, 8, 5, 9, 10, 1, 3, 8, 9, 1, 2, 3, 6, 8, 8, 1, 10, 7, 5, 7, 4, 4, 5, 8, 10, 6, 3, 8, 9, 3, 2, 1, 2, 3, 4, 5, 6, 7, 10, 1, 4, 6, 7, 10, 1, 2, 3, 4, 6, 7, 8, 9, 10, 3, 1, 2, 3, 8, 9, 2, 2, 2, 3, 4, 5, 7, 8, 9, 7, 1, 2, 6, 8, 10, 1, 3, 4, 5, 6, 7, 7, 9, 10, 1, 4, 7, 9, 1, 2, 4, 5, 6, 7, 8, 1, 2, 3, 5, 1, 2, 4, 6, 7, 9, 1, 2, 5, 6, 3, 1, 2, 3, 4, 8, 9, 10, 4, 8, 4, 3, 3, 4, 6, 1, 3, 4, 5, 7, 8, 9, 2, 3, 5, 1, 2, 3, 4, 5, 7, 8, 2, 3, 5, 4, 6, 6, 10, 1, 2, 4, 7, 7, 4, 1, 1, 10, 1, 2, 3, 4, 5, 7, 8, 9, 10, 3, 5, 1, 2, 3, 4, 7, 1, 2, 3, 4, 5, 6, 7, 9, 7, 1, 2, 4, 5, 8, 4, 8, 6, 2, 7, 10, 8, 1, 4, 6, 7, 8, 10, 6, 1, 4, 5, 6, 7, 4, 6, 2, 6, 1, 6, 9, 10, 1, 1, 8, 7, 9, 8, 1, 2, 3, 4, 5, 6, 8, 3, 5, 7, 1, 2, 4, 5, 6, 8, 4, 7, 4, 8, 2, 3, 5, 8, 1, 3, 4, 6, 8, 9, 10, 3, 2, 2, 1, 3, 7, 9, 1, 2, 3, 4, 6, 7, 8, 10, 5, 1, 2, 6, 7, 10, 10, 5, 1, 3, 4, 5, 7, 9, 10, 1, 4, 6, 7, 8, 9, 10, 7, 1, 2, 3, 4, 6, 7, 1, 3, 4, 6, 7, 8, 9, 1, 2, 4, 5, 6, 9, 2, 4, 9, 10, 1, 3, 4, 5, 6, 7, 9, 10, 4, 1, 4, 9, 6, 7, 1, 2, 3, 4, 5, 6, 7, 8, 6, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 2, 3, 2, 3, 5, 7, 2, 3, 9, 1, 3, 8, 9, 1, 2, 8, 3, 10, 2, 3, 5, 7, 8, 9, 3, 5, 5, 8, 2, 3, 9, 1, 3, 4, 5, 7, 10, 6, 8, 9, 2, 3, 5, 9, 1, 3, 5, 8, 1, 5, 9, 1, 3, 6, 7, 8, 3, 1, 3, 9, 6, 9, 9, 7, 1, 8, 10, 1, 10, 9, 1, 10, 2, 3, 10, 1, 1, 3, 5, 1, 2, 3, 4, 5, 6, 7, 9, 6, 6, 1, 3, 4, 5, 9, 10, 2, 3, 1, 7, 8, 9, 1, 4, 5, 10, 3, 8, 9, 2, 6, 9, 1, 1, 1, 1, 2, 3, 4, 5, 6, 7, 9, 10, 3, 8, 3, 5, 9, 2, 3, 1, 2, 3, 4, 5, 6, 10, 8, 2, 3, 1, 4, 6, 7, 9, 10, 1, 7, 4, 2, 2, 3, 9, 10, 2, 3, 4, 5, 6, 10, 1, 4, 5, 6, 7, 10, 4, 6, 6, 5, 5, 8, 4, 1, 8, 4, 1, 2, 3, 4, 5, 6, 7, 9, 10, 1, 2, 3, 4, 5, 7, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 9, 1, 3, 4, 1, 4, 6, 1, 9, 1, 2, 8, 10, 2, 10, 2, 6, 8, 2, 10, 10, 2, 3, 5, 9, 2, 10, 2, 4, 4, 5, 7, 10, 2, 3, 8, 5, 2, 5, 3, 2, 1, 2, 3, 5, 8, 9, 10, 10, 5, 9, 2, 3, 6, 2, 3, 2, 3, 2, 3, 3, 9, 3, 4, 3, 10, 2, 3, 7, 8, 6, 8, 7, 2, 3, 7, 5, 3, 5, 3, 8, 9, 5, 2, 3, 5, 9, 8, 2, 1, 5, 6, 7, 1, 2, 4, 6, 8, 1, 5, 9, 7, 6, 10, 2, 3, 9, 1, 4, 6, 9, 2, 7, 9, 10, 1, 4, 6, 7, 10, 8, 10, 4, 10, 7, 1, 5, 2, 1, 4, 6, 7, 10, 1, 2, 3, 4, 5, 6, 7, 10, 7, 1, 2, 3, 4, 5, 7, 8, 4, 1, 2, 3, 4, 6, 7, 9, 10, 5, 1, 2, 3, 4, 6, 7, 10, 2, 3, 4, 6, 8, 9, 9, 5, 8, 5, 8, 9, 3, 1, 6, 8, 1, 5, 8, 2, 3, 4, 8, 9, 6, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 5, 7, 8, 9, 1, 3, 4, 5, 6, 10, 6, 2, 4, 6, 6, 9, 7, 7, 2, 6, 10, 8, 1, 2, 4, 5, 6, 7, 8, 2, 3, 1, 4, 6, 7, 1, 4, 7, 4, 1, 1, 4, 6, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 3, 4, 9, 1, 2, 3, 4, 5, 6, 7, 10, 3, 4, 5, 8, 5, 2, 3, 1, 2, 3, 5, 7, 3, 4, 7, 9, 6, 3, 1, 2, 3, 4, 5, 6, 8, 1, 2, 3, 4, 7, 4, 1, 2, 3, 4, 6, 7, 8, 1, 4, 5, 6, 7, 8, 1, 9, 5, 9, 10, 6, 8, 4, 7, 8, 1, 2, 4, 7, 8, 9, 2, 10, 1, 2, 3, 4, 5, 7, 8, 9, 7], \"Freq\": [0.9986853589242308, 0.061543370182529145, 0.933407781101692, 0.9949042863996798, 0.9563210748789208, 0.03810044123023589, 0.9925318407955566, 0.03660461055184776, 0.9601670921676989, 0.9867808853656713, 0.9895763958142498, 0.9887441411656536, 0.9779203606484234, 0.01979595871757942, 0.9929234192926489, 0.419899553668963, 0.04414831755865265, 0.1226342154407018, 0.04611046500570388, 0.10301274097018952, 0.030413285429294046, 0.2325144724755706, 0.9969953279399623, 0.9948351388624683, 0.16817149148996172, 0.6774455364737136, 0.15389277994836117, 0.2260307904544825, 0.06520118955417764, 0.15909090251219343, 0.012170888716779826, 0.22516144126042678, 0.045206158090896495, 0.26602085338104475, 0.9964629574478069, 0.4451121144574466, 0.15587583774774394, 0.09265952577227, 0.30568983736085337, 0.997967609414801, 0.4148324106904191, 0.3766513181641865, 0.20741620534520955, 0.9993638432054255, 0.9905444625419488, 0.9892444056772138, 0.0475731315258535, 0.3163140665692511, 0.29110975847608367, 0.2277839343920005, 0.0519838854421578, 0.05986023172127261, 0.005355915469798076, 0.13243553231802924, 0.8548111631436431, 0.012039593847093567, 0.9969758316468499, 0.9941900927064052, 0.9937187565392036, 0.15363424599410866, 0.8449883529675977, 0.9955359752157282, 0.9923997812896982, 0.08296379502901506, 0.08296379502901506, 0.8342470500139848, 0.251487674322668, 0.7433679785125923, 0.9944234725050056, 0.0395743800573742, 0.9596787163913244, 0.9971299815934997, 0.26848348557026214, 0.6061355979315072, 0.12468555092585057, 0.046791153802468316, 0.9507111704410608, 0.3804855219478659, 0.6182889731652821, 0.9608658135027074, 0.03603246800635153, 0.9934796381892846, 0.9937629328358669, 0.7710510613749624, 0.02669022904759485, 0.023724648042306534, 0.1275199832273976, 0.01383937802467881, 0.036575499065222575, 0.9921957457720039, 0.9881934446445766, 0.9936010185734448, 0.03709724246098916, 0.3060522503031606, 0.06492017430673103, 0.5917010172527771, 0.9944273876637509, 0.9947937813345985, 0.009188517416705847, 0.3231295291541556, 0.11945072641717601, 0.5467167862939979, 0.9896094770026088, 0.9955434513766304, 0.9787790185316129, 0.019286286079440648, 0.9888454588075382, 0.1253767766966342, 0.027582890873259523, 0.2093792170833791, 0.5165523199901328, 0.036359265242023917, 0.08400244038674491, 0.9951444483594833, 0.18822789660865688, 0.8085243740690033, 0.9946470453747581, 0.9949424137738495, 0.9879908229792669, 0.9871220794808251, 0.08237751480974972, 0.4459534636317279, 0.4094100548063502, 0.002477519242398488, 0.05946046181756372, 0.9951726452934978, 0.9945272787038603, 0.22708817502581985, 0.5010749948939286, 0.2715184701395672, 0.9978497203499442, 0.9935136682030772, 0.9351810217056311, 0.06430361864020044, 0.9876479464641894, 0.011842301516357187, 0.9906186642763687, 0.504665210349496, 0.04463025669757447, 0.4497356636447889, 0.04968991868559028, 0.6469432550437636, 0.1948624262180011, 0.044818358030140254, 0.03799817311251021, 0.024357803277250137, 0.9877399932414987, 0.9961679295794053, 0.987340831800679, 0.996864722823212, 0.985633617857292, 0.9969115098848849, 0.13404132181686806, 0.8647827213991488, 0.9868440131833267, 0.9939343998237393, 0.9933847076277543, 0.9948553286271189, 0.9851812908488439, 0.993960505914811, 0.9942499422596469, 0.5281611156261615, 0.08828053229486273, 0.30441562860297494, 0.04490130521893881, 0.03424675821783468, 0.9890024482275747, 0.5368328077626289, 0.08713807894118034, 0.37500494687186536, 0.9962923073049559, 0.9897094183064098, 0.9857619944971185, 0.8816982027252114, 0.11420961175197039, 0.29102454318266846, 0.005039386029137116, 0.06248838676130024, 0.3794657679940248, 0.1602524757265603, 0.010834679962644799, 0.09070894852446809, 0.3320435701445907, 0.08090401709106779, 0.10281552171989865, 0.4837386021903428, 0.9931771005563668, 0.994174103050457, 0.9965028672391762, 0.9953123577146543, 0.9885769142957318, 0.9948663196762887, 0.995520493726269, 0.9938153251148516, 0.12976102516360882, 0.8650735010907256, 0.9879853992004188, 0.9927382338110499, 0.18861866796096047, 0.7457423261049085, 0.06461935846810683, 0.9956828359541919, 0.24467124724958403, 0.08624026246976785, 0.19818819416795871, 0.13735667454023684, 0.15364816259778397, 0.0005978527727540233, 0.17816012628069894, 0.0008967791591310348, 0.20128356127122526, 0.0023680418973085326, 0.10182580158426689, 0.4830805470509406, 0.0876175502004157, 0.12432219960869795, 0.6518190179634447, 0.1272964199787433, 0.08281935757653179, 0.05751344276148041, 0.009968996745323271, 0.0697829772172629, 0.8611758272854296, 0.1380469704352348, 0.9921976809282609, 0.9461531580415441, 0.05317956860186665, 0.9907588867859828, 0.9934588386665728, 0.7152031645813556, 0.2191751633394477, 0.06566381411789121, 0.11475027662854763, 0.06650832359695413, 0.5002175324052607, 0.020139844469500196, 0.03653274113072128, 0.18266370565360643, 0.07868590397386123, 0.8397051109792144, 0.025595731013132398, 0.014036368620104862, 0.08339254297827006, 0.03715509340615993, 0.12913646275512677, 0.027186623737921425, 0.042721837302447954, 0.16991639836200892, 0.07379226443150101, 0.03398327967240178, 0.38158368317868285, 0.14175882377630458, 0.9931174426417337, 0.9967961212187343, 0.28365154389984737, 0.7148018906276153, 0.9909312573938324, 0.9938748659675584, 0.9895623303722769, 0.9942116435037706, 0.07130218243520858, 0.02852087297408343, 0.8984074986836281, 0.9863895402318794, 0.9987994067399127, 0.3041327766311087, 0.002890995975580881, 0.38334606636202484, 0.002890995975580881, 0.13703320924253376, 0.09077727363323967, 0.07921328973091614, 0.017936693937038915, 0.9818546261135102, 0.046236148271928706, 0.24552989082334556, 0.7062970236022212, 0.9939020670783015, 0.9931771005563668, 0.9853456714476636, 0.9979089109450318, 0.995293689261195, 0.9923044650489509, 0.9891704805222039, 0.9585675592045312, 0.03305405376567349, 0.9951344328551638, 0.9889719283024356, 0.9941063110164784, 0.29599785864150174, 0.1528174991125893, 0.2629562372117527, 0.28636071905782495, 0.9517717859287874, 0.04532246599660893, 0.2584707521652196, 0.020424119380902907, 0.2852333913539889, 0.13170035600789118, 0.21128399359554734, 0.0838093174595671, 0.008451359743821893, 0.3624756748797356, 0.6203910589287782, 0.013941372110759062, 0.2208926213975331, 0.005471024678576671, 0.3754490685673241, 0.1018978346384905, 0.2557704037234594, 0.040348807004502954, 0.15868358506145439, 0.7906340027623342, 0.01531157399715788, 0.033407070539253556, 0.8960964155797433, 0.057657678214246215, 0.019219226071415407, 0.024024032589269256, 0.9965472274423653, 0.9909044056695188, 0.9943713894421928, 0.10710907081544419, 0.21421814163088837, 0.31605955322590085, 0.2783079954794738, 0.08428254752690689, 0.9984360267889305, 0.21580391411195374, 0.07002908471182605, 0.06717075472358824, 0.4687661180709988, 0.06717075472358824, 0.11290403453539301, 0.8633912288845925, 0.1339389734546504, 0.9812227719130888, 0.017114350672902713, 0.9946333271787082, 0.9941931903725477, 0.9971487086701502, 0.9918731413990741, 0.9994968291112644, 0.9997427462792382, 0.9965388142878544, 0.8173035274871973, 0.05848876467893314, 0.06464547675039978, 0.0600279426967998, 0.9486157791441308, 0.05086780264975774, 0.38377484528534295, 0.03488862229866754, 0.5807929476778184, 0.16493112472941812, 0.8335708195784105, 0.9684788550821449, 0.027847634810668705, 0.4331060693953299, 0.03353079246931586, 0.182323684051905, 0.027243768881319137, 0.1795294513461287, 0.06147311952707908, 0.0006985581764440805, 0.04261204876308891, 0.006985581764440805, 0.031435117939983624, 0.9916907732382082, 0.8256018588924997, 0.17388532302016688, 0.9938704034726625, 0.9978020604347709, 0.47425964682419897, 0.3580607754847888, 0.05205288806607062, 0.0383824326143753, 0.017350962688690207, 0.022083043421969352, 0.03733085911809105, 0.47209020515040107, 0.049845475412913204, 0.20144163204061616, 0.12605549980455735, 0.0004119460777926711, 0.12440771549338668, 0.02554065682314561, 0.1345623589018587, 0.8623362436668409, 0.9964243270115208, 0.9910446000322654, 0.8079877947143443, 0.06890865807764475, 0.09458051108696339, 0.028374153326089016, 0.9960101869038142, 0.9902265649268356, 0.9959796379102691, 0.8439000075622713, 0.004167407444751957, 0.06876222283840729, 0.08126444517266317, 0.06955643435914148, 0.43218296714612914, 0.039443587776830234, 0.06998055895889235, 0.03901946317707937, 0.04029183697633196, 0.22478603786795726, 0.08143192315216564, 0.003392996798006902, 0.9898105536563336, 0.995724679990303, 0.502843746610125, 0.1259680142325978, 0.019537896085055982, 0.13625111743525883, 0.14344928967712156, 0.07146756725849425, 0.9908472239898874, 0.09690030671272307, 0.7909160169524964, 0.10999494275498294, 0.025026161908682027, 0.34798282273024533, 0.6268457697127022, 0.9961283546749402, 0.9982063143900589, 0.9942687117679375, 0.9980068763693123, 0.9944720151554474, 0.9935135790127314, 0.9974998471345976, 0.2714703465373393, 0.04347389891167355, 0.5352119999348255, 0.12945561009253903, 0.019804776170873506, 0.9954918992957141, 0.9948926390755205, 0.9945422378214731, 0.991959748016299, 0.14570409801023118, 0.8541274710944586, 0.17891749967854478, 0.6190830616367177, 0.2017276988407497, 0.9846693234930793, 0.14497810546317874, 0.4574423327628151, 0.03310002407835131, 0.30187221959456395, 0.06289004574886749, 0.07918380943500203, 0.9197504018988697, 0.9484650467692727, 0.04835312003137468, 0.20138548279101273, 0.33733478629979163, 0.15061603334790027, 0.18107770301376774, 0.009589784894810131, 0.11959025868822044, 0.24883613564281928, 0.7465084069284578, 0.9927281866559338, 0.9976848164876593, 0.9881715289057207, 0.9665418631222781, 0.0310120383889501, 0.06351165421989595, 0.21356666144272704, 0.4864573955084338, 0.07467853847833919, 0.009073093459985136, 0.02023997771842838, 0.03210479224302432, 0.10119988859214189, 0.9885545415292114, 0.13180114456378372, 0.5366448806205849, 0.044659891959629196, 0.08205801286891218, 0.01488663065320973, 0.02941017275390215, 0.013071187890623179, 0.009803390917967385, 0.09948626338974308, 0.038850475119352225, 0.997611394330847, 0.5153630174359154, 0.1549290064838404, 0.1773360941984454, 0.1523681964593141, 0.8892953071238267, 0.036883215502831516, 0.07376643100566303, 0.04106311615826121, 0.7410025961286228, 0.03919661087834025, 0.004977347413122572, 0.17420715945929, 0.3001582858729258, 0.07135838494337482, 0.13139004211796, 0.4972433490498658, 0.08309154377090949, 0.023385045252890698, 0.383116698823954, 0.1427980422889283, 0.22340181528825367, 0.14429070475187877, 0.4493152029466615, 0.15517586905889855, 0.3937298170151157, 0.9974083675671245, 0.9900177299935013, 0.9384949714840616, 0.05663331724472785, 0.6446416559783541, 0.03021757762398535, 0.04532636643597802, 0.06798954965396703, 0.2102639776335647, 0.45909208616581504, 0.08772460245206658, 0.29631421272698044, 0.1559548488036739, 0.9915820454847917, 0.9907633020809555, 0.9318856382365346, 0.0659211705504944, 0.992565664691204, 0.9957164640642144, 0.16479097502618076, 0.3922297587400005, 0.1184860729527085, 0.3064294990156254, 0.017704815498680578, 0.17971427365739398, 0.22711958438000382, 0.021110177431162197, 0.091292258540026, 0.0002777654925152921, 0.04138705838477852, 0.03277632811680446, 0.4062783270523672, 0.017884809281817713, 0.9747221058590655, 0.10057924183673787, 0.8981549841210452, 0.2912624803819192, 0.6376286732685258, 0.07084763036316953, 0.653784640718992, 0.27333031989894874, 0.0020887688201884727, 0.07042134879492565, 0.997936195796167, 0.5258608588804063, 0.0944660225533664, 0.19837864736206945, 0.06927508320580203, 0.02256688316552642, 0.00472330112766832, 0.08344498658880699, 0.9956321556024175, 0.9890705820857496, 0.31050228569827826, 0.6885050682874866, 0.998907244103929, 0.9916144692863077, 0.00643999733053798, 0.6858597157022949, 0.30589987320055406, 0.9879027241318685, 0.9899080935838631, 0.9969417683353085, 0.35343282919904523, 0.16918423955101838, 0.01622314625831683, 0.03939906948448373, 0.07995693513027581, 0.061416196549342286, 0.27926987487531113, 0.9943401105826692, 0.9858825059061473, 0.10179709959084657, 0.894742927982704, 0.9835508623449295, 0.2816586790562297, 0.7149797237581215, 0.30798150906386107, 0.009651842164394507, 0.1798752403364431, 0.5018957925485144, 0.07351745197935254, 0.6168293531926164, 0.2483455390034226, 0.06096569188531674, 0.9938839731853031, 0.9873611770021877, 0.48207850597540713, 0.5138639019737856, 0.9888265801363877, 0.9930421788935744, 0.9869500801848172, 0.99269069536186, 0.16968789948936586, 0.19985463717636423, 0.6259598070052163, 0.9944470231157655, 0.9979724543957598, 0.9354694984852258, 0.062364633232348386, 0.9922946891091752, 0.9912845964425152, 0.996608253679234, 0.08480280409770208, 0.02261408109272055, 0.09328308450747227, 0.32578410574200545, 0.06854893331230917, 0.03250774157078579, 0.3632386775518239, 0.009186970443917725, 0.22737050555817642, 0.2216726766599405, 0.3193497434868421, 0.09469248978496847, 0.13647656837203193, 0.14603556894000494, 0.02854830671007615, 0.21081826493594696, 0.3359916097416655, 0.03989442860767052, 0.06258667240285926, 0.10101708528180792, 0.024156259523910588, 0.05124055050526489, 0.9963560183782025, 0.1062121233326064, 0.5031100578912935, 0.28695907005651555, 0.015838649970651833, 0.08757841748478072, 0.993843422272617, 0.9962755201964688, 0.06103285499475776, 0.44635968578255675, 0.13254149853339184, 0.09154928249213663, 0.09291568969351181, 0.08335083928388559, 0.09246022062638676, 0.9919790691581987, 0.22540871124159106, 0.06003876888210603, 0.01790629949115443, 0.270701115836864, 0.4244846291138374, 0.45690960675175135, 0.011776020792570912, 0.18959393476039169, 0.034739261338084186, 0.2779140907046735, 0.028851250941798735, 0.15593425414744932, 0.7458171234552345, 0.09848479209312588, 0.15128138279537112, 0.005216599406736935, 0.1356315845751603, 0.7042409199094862, 0.26173933688932705, 0.10635150671658787, 0.3458016056713007, 0.01592088423901016, 0.0038210122173624383, 0.2649235137371291, 0.0006368353695604065, 0.13153309189218224, 0.5401536616283017, 0.30446238529357916, 0.02336882343262121, 0.12662424024176955, 0.5108483094289947, 0.19668095047862488, 0.08398102531498805, 0.07092697992923862, 0.010878371154791198, 0.2563822709359473, 0.17688389235115745, 0.5028272445487959, 0.0635987028678319, 0.9937821512892859, 0.3124866051468028, 0.2749532978247008, 0.09717926081846569, 0.19814094795621295, 0.013965881794270517, 0.023567425527831497, 0.07943095270491357, 0.8267115710669427, 0.16936105101718618, 0.9947750155137384, 0.9956480238193774, 0.14091160206474493, 0.2766042559048697, 0.5806079899889953, 0.2890481566079625, 0.04867391238216986, 0.10408728955571707, 0.05466454775228307, 0.10184080129192462, 0.3594381222067928, 0.04043678874826419, 0.01714078591452135, 0.08141873309397642, 0.8998912605123709, 0.12778560889038107, 0.35374301889654697, 0.1914755790357456, 0.14928604467193726, 0.09533212091822081, 0.013387063788516113, 0.06896366194084058, 0.7945659242994294, 0.11758150449274068, 0.0876516669854976, 0.7821146420383405, 0.21628849360072624, 0.09657022171075894, 0.9013220693004168, 0.2337584292779964, 0.05446798352108654, 0.009077997253514422, 0.7035447871473678, 0.9956754665872992, 0.9954409750016461, 0.9940459756198016, 0.15903838107523455, 0.8376021403295687, 0.3334401932185997, 0.017826693197592208, 0.11546835366622225, 0.23944490181311354, 0.0737376854991314, 0.03160186521391346, 0.14544961040762733, 0.017016388961338016, 0.025929735560134122, 0.21243280743031073, 0.7855073577074281, 0.6836514852814167, 0.018546686865207992, 0.014940386641417547, 0.21225652745738033, 0.07058044723704152, 0.1119074658302541, 0.24718954302981452, 0.03633359280203055, 0.17851905263397677, 0.11457192930240301, 0.15720334485678553, 0.1349187412715401, 0.019135692209069424, 0.9926618513821995, 0.25947269957611346, 0.04888616078970254, 0.25571222566921326, 0.17580215514758413, 0.25947269957611346, 0.8506481112348608, 0.14614659179805978, 0.9955676493524994, 0.2514115781121184, 0.6576926883413017, 0.09050816812036262, 0.9933473094376539, 0.7861632598906084, 0.12140072108514838, 0.026205441996353617, 0.029414271628560182, 0.0016044148161032826, 0.035297125954272215, 0.9939325896684195, 0.8139327838361841, 0.15845200348053526, 0.012522042828248986, 0.0014448510955671907, 0.013726085407888312, 0.9961639822290793, 0.997825390160108, 0.0008352326436780984, 0.9981030091953277, 0.22403420363381507, 0.12037658702712452, 0.06854777872377923, 0.586835861757232, 0.994464317109218, 0.9962141733262145, 0.9889151195153257, 0.9878819229253223, 0.9989145912727613, 0.9864855198950587, 0.595494038890127, 0.04274277770430877, 0.07843092219528502, 0.20209449264076088, 0.0713762889819525, 0.0016599136972547093, 0.00788459006195987, 0.14174819429529426, 0.7339406504623014, 0.11969847518269293, 0.4684474513197077, 0.05380330604743963, 0.17360533417973856, 0.2826467011025495, 0.021521322418975853, 0.9877393876297551, 0.4779432447940636, 0.5200670561996421, 0.874811596423658, 0.12497308520337971, 0.007825298947328873, 0.08020931421012095, 0.9116473273638137, 0.9942090047903354, 0.0865822695688882, 0.34684753497955817, 0.15968466483363813, 0.03732888268838294, 0.19338435059398384, 0.1503524441615424, 0.02540437849626061, 0.9978305959495692, 0.9942365647856093, 0.995738668134755, 0.2687485846532114, 0.1927210245210529, 0.249299673921729, 0.2890815367815794, 0.520909044093445, 0.0018959382860543948, 0.011849614287839967, 0.20381336575084744, 0.007583753144217579, 0.10569855944753251, 0.05498221029557745, 0.09290097601666535, 0.9916751622246869, 0.32539828649256886, 0.018233524674152565, 0.3885143334415585, 0.2454512936905153, 0.021038682316329882, 0.9917875221081714, 0.9955524682038943, 0.6982459323035457, 0.0932841787865848, 0.10559892186072141, 0.009236057305602455, 0.0003078685768534152, 0.05664781814102839, 0.03663636064555641, 0.06875534495065394, 0.2853025528793491, 0.07325335817172476, 0.3482747379743405, 0.03341381249938322, 0.06939791826794978, 0.1214463569689121, 0.9947852375574789, 0.08775118016223361, 0.09702569513872986, 0.007490954404093113, 0.46051533979448617, 0.15088922442530414, 0.19583495084986283, 0.08989205236296217, 0.3877631336247335, 0.34099272999677166, 0.08392136253811598, 0.024214464289654018, 0.06269224316088506, 0.010614559688615459, 0.23693693118153444, 0.020731981478384263, 0.15697071690776657, 0.1673367076469587, 0.023693693118153444, 0.393907648089301, 0.996219769812065, 0.05692432499138168, 0.7137434595073242, 0.22769729996552673, 0.2995680537058951, 0.08039877911959686, 0.2602864584282839, 0.0025698239901241005, 0.05139647980248201, 0.08553842709984506, 0.028635181604239977, 0.19163544612068292, 0.9891685308030569, 0.03432552459537086, 0.17391599128321233, 0.7894870656935297, 0.9907682925238566, 0.9938458799849877, 0.05640007126043485, 0.1660418097907202, 0.19852825083673067, 0.20664986109823327, 0.06271687924160355, 0.027974435345175686, 0.16468820808046974, 0.116860947651621, 0.05552123903897865, 0.9359294580856401, 0.288377576529347, 0.07652124289484866, 0.2114347289904496, 0.11604667827441924, 0.1182601026556752, 0.0012648139321462589, 0.0824237079115312, 0.040052441184631526, 0.043003673692972796, 0.022555848456608282, 0.14784232985012877, 0.8514769478534154, 0.33155654186343964, 0.6667970453031397, 0.989511192520941, 0.9936016345276841, 0.8783192576313582, 0.11977080785882158, 0.9995615606957775, 0.33426474705952425, 0.024350412037448785, 0.09297430050662264, 0.5467774339318046, 0.9049338289162802, 0.049549690920487964, 0.04433393398148923, 0.9966153868237548, 0.9927642906612926, 0.06974747606455373, 0.3487373803227687, 0.5417053974347007, 0.03952356976991378, 0.9889216105027177, 0.9914694601684917, 0.9907003388285563, 0.9947096587765332, 0.9871169765755526, 0.9898256102013696, 0.32513782092575677, 0.0424092809903161, 0.6314270725224842, 0.18906998971137878, 0.2314265783155513, 0.2062581705942314, 0.3265754367741997, 0.04603977022192665, 0.9855244129384532, 0.9918900890970486, 0.9932311058577471, 0.9892489796628569, 0.10399378500931325, 0.11357215994438158, 0.05610191033397162, 0.7265881557887545, 0.6971399936477145, 0.1719611984331029, 0.10410623905139203, 0.025097039771317723, 0.9449503260553543, 0.04205276320141629, 0.01236845976512244, 0.46267045485336655, 0.066996573530726, 0.16000358149102795, 0.05202086885915195, 0.256951564364902, 0.9888398697217419, 0.8782708404400353, 0.05489192752750221, 0.06540314769234305, 0.2195094760964038, 0.7750028441770992, 0.9930428428088577, 0.9966071438173485, 0.29189193054682416, 0.7069257692930898, 0.9970739020581535, 0.9991808393482482, 0.9961266733667046, 0.9913709152883937, 0.99735938564929, 0.993941461244151, 0.06570729497096645, 0.43467902826947036, 0.4978591195877073, 0.9967207286678156, 0.9012991345604537, 0.03767488343821832, 0.0579613591357205, 0.2332481299003723, 0.07840826965577487, 0.10674066961542461, 0.1963501206505959, 0.007247823245491795, 0.12255410215104307, 0.07511380454418769, 0.18053668811497742, 0.992209709141281, 0.9925591231840346, 0.5651483502694924, 0.04274840085371802, 0.21808929927066312, 0.115203317554935, 0.004347295002073019, 0.054341187525912736, 0.9191589206458362, 0.07906743403405042, 0.15182028418439228, 0.09295119439860751, 0.016524656781974668, 0.738445599944493, 0.7644520419136329, 0.05443092070587576, 0.007257456094116768, 0.17296937024311632, 0.10176562623821993, 0.26077441723543854, 0.6328549881689302, 0.8005844340917375, 0.1457096413786598, 0.05238998341704622, 0.9903191716681987, 0.9948551366136418, 0.9960388025054766, 0.12146897534524653, 0.06941084305442659, 0.17352710763606646, 0.20116290625958816, 0.0687681500631819, 0.22365716095315233, 0.006426929912446907, 0.06234122015073499, 0.07326700100189473, 0.35946684798657874, 0.6403003229760933, 0.42807047347328236, 0.5707606312977098, 0.993322535494014, 0.12734517116606112, 0.8723936829674976, 0.05092000537248394, 0.4270336814192403, 0.38826504096519004, 0.0862168272784103, 0.037611367604675636, 0.0011572728493746349, 0.008100909945622445, 0.9947994474600503, 0.015035142188154374, 0.9815799971409355, 0.27776293559380505, 0.17263675667320202, 0.16895599379041404, 0.03133137185592694, 0.030164300697969777, 0.319059299644597, 0.08608605079836439, 0.9125121384626625, 0.9918800941240429, 0.9942519396006363, 0.7908625339082848, 0.14142482959301092, 0.033495354377292064, 0.03256492786681173, 0.2740155959524326, 0.4745028566761265, 0.13382157011630427, 0.0813713576531374, 0.023038878371671432, 0.012744911439648026, 0.34629544797913187, 0.18878806037544596, 0.001472031659847532, 0.1254906990020021, 0.21344459067789212, 0.12438667525711646, 0.8329695981846967, 0.1638628717740387, 0.9928907378354428, 0.9973050707549798, 0.9878337466016389, 0.9970642089214516, 0.9975328857089515, 0.9935475148660793, 0.998343408698901, 0.9848089602450534, 0.23036789849049064, 0.1083133277113184, 0.11599225766100142, 0.2627002351207349, 0.08588276917408642, 0.10730294219162327, 0.02243055853723198, 0.008487238365439128, 0.05860236014231779, 0.09566718376688699, 0.5415997016480215, 0.20059248209185981, 0.14735832338286625, 0.013887171837128755, 0.0007715095465071531, 0.11772053379026674, 0.19311458352111172, 0.18121025987939937, 0.15078809946168997, 0.0026454052537138594, 0.033067565671423244, 0.20700296110310948, 0.05952161820856183, 0.05026269982056333, 0.005290810507427719, 0.9960552220452418, 0.9968792256828662, 0.9965122648522515, 0.9940136779254533, 0.474827290195086, 0.18553171056968115, 0.3395039033620454, 0.9935529504564548, 0.9975392434154211, 0.14404515966898615, 0.2723045484153437, 0.10852717509307176, 0.47423086813396814, 0.9951808046838849, 0.9980129324729624, 0.9971007777916004, 0.1428438611414572, 0.850570264069586, 0.47782678649078947, 0.5187833681899999, 0.9930863133695805, 0.26353582290557703, 0.35895396568173427, 0.33688446327092236, 0.03959528373704483, 0.9422504185783086, 0.055037991739387185, 0.9179078768133792, 0.07981807624464167, 0.9952844996749579, 0.9898185012083128, 0.43261206114170736, 0.5669815597468781, 0.17709846255443382, 0.7911681316290466, 0.030799732618162403, 0.994457087714924, 0.9961071377717586, 0.9877780164220237, 0.9967169378602168, 0.9966892223003343, 0.1271421986329364, 0.18364984246979704, 0.2704294383621187, 0.032290082192491784, 0.1708683516019357, 0.16548667123652042, 0.0497805433800915, 0.9951338136784754, 0.9943823907287488, 0.9946245218517576, 0.1770780664408548, 0.8031755156424486, 0.018972649975805875, 0.029665909744581612, 0.9704990473584556, 0.8531429793208648, 0.1460334829468147, 0.9969903033347639, 0.9922732160130823, 0.9284562246055745, 0.06849267230696861, 0.9952076146994283, 0.9926266088107074, 0.9973786514323885, 0.9976998806668431, 0.0033854675374389797, 0.20651351978377777, 0.14726783787859563, 0.6415460983446867, 0.9035075397652734, 0.0910007594008189, 0.9922703074015259, 0.6925494995968807, 0.2946808042697329, 0.01190629512200941, 0.9946182933904537, 0.2660645588578617, 0.7316775368591195, 0.35089114779031483, 0.5944200787194139, 0.05433575609439576, 0.9926524497247639, 0.6871328205043564, 0.25355454631157065, 0.041836500141409154, 0.01648104551025209, 0.9911458916322251, 0.9941169003572162, 0.39913328328698144, 0.019076223098274846, 0.1027181243753261, 0.47837297923366157, 0.27580770707617147, 0.02052937625486066, 0.36952877258749184, 0.3132961332806996, 0.01963679467856237, 0.44460978209848495, 0.08526762944354506, 0.4689719619394978, 0.9871778835703457, 0.9950719085301244, 0.9912847940047437, 0.12719879476965942, 0.2587240791573345, 0.6134962278346158, 0.5307506855832267, 0.3939959591254796, 0.04372084191585967, 0.03101129484729581, 0.998148140887851, 0.10099141442203566, 0.8952752413629107, 0.9876784128398478, 0.27406615474856455, 0.1799972435477371, 0.2709003740831521, 0.1885900767824281, 0.0863805867276829, 0.9957884547615271, 0.9912531094791192, 0.3507821735969647, 0.6468179964013395, 0.9939848709671657, 0.9162045215655756, 0.08067209623847836, 0.9966249258054571, 0.5815784965122486, 0.2407809061993386, 0.07004535453071668, 0.004714591170336699, 0.10271073621090666, 0.20600537615543585, 0.13783451496390273, 0.09752911528131745, 0.3707101575743954, 0.010947145592800939, 0.04080299720953077, 0.0711564463532061, 0.06518527602986013, 0.9970023694857656, 0.1174188751805361, 0.02673895177378545, 0.4132911893730751, 0.2348377503610722, 0.01860100992958988, 0.09242376808764971, 0.09707402057004717, 0.9892451528870967, 0.3071539417183267, 0.08987822724848028, 0.11199779096383836, 0.33823332871079187, 0.027159464308730803, 0.05571890100450958, 0.011759768051203028, 0.05795885682378635, 0.9879991818391269, 0.17357933306449544, 0.08471335872047145, 0.1112900987112076, 0.187698226184574, 0.10962905246178659, 0.33137872675949126, 0.0016610462494210088, 0.19516441608748047, 0.22134500848945957, 0.22908018351731702, 0.01844541737412163, 0.0017850403910440286, 0.3338025531252334, 0.9952143182506832, 0.8162874310535168, 0.17968591249605714, 0.9413581532964923, 0.05648148919778954, 0.9970718701390169, 0.9939144455407646, 0.1309667674307754, 0.2152878368725075, 0.6530397718466061, 0.953355186834465, 0.04493930173630475, 0.9925231820208107, 0.7468980843593714, 0.14060895903130757, 0.04872587689203728, 0.042461121291632486, 0.02088251866801598, 0.9921454147275625, 0.09193486172396312, 0.10797001202465435, 0.4086290801626151, 0.20284465130374418, 0.0660113687378456, 0.01576789779567972, 0.03714809819660137, 0.01175911022050691, 0.05772654108248847, 0.9830595686928714, 0.24165069592557473, 0.4026389729233462, 0.1303612389347808, 0.09423704019381744, 0.012452751739897305, 0.030963598920825734, 0.0876180099897279, 0.7946515411809519, 0.044056656287239634, 0.014685552095746544, 0.0032634560212770098, 0.08974504058511777, 0.052215296340432156, 0.9968922744979779, 0.25632454567864865, 0.012816227283932433, 0.7305249551841486, 0.9962362855159005, 0.9930305278966741, 0.9977051680826677, 0.993919347110699, 0.44373271718945156, 0.1784577232174968, 0.3774139686964629, 0.9935806157264766, 0.011694835845204412, 0.0678300479021856, 0.16606666900190267, 0.5332845145413212, 0.12396525995916678, 0.004677934338081766, 0.0935586867616353, 0.0952035810293696, 0.9027925787267806, 0.1920901750733412, 0.23435001358947627, 0.5493779007097558, 0.023050821008800945, 0.002934698147051693, 0.9302993126153867, 0.061628661088085554, 0.9950947575703206, 0.995308292542425, 0.8724908634006221, 0.0005566129910051816, 0.07208138233517102, 0.054826379614010386, 0.27071471555327753, 0.0078090783332676215, 0.22581251513698872, 0.2768969025671144, 0.10086726180470677, 0.0709324615271809, 0.006182187013836867, 0.015292778402649093, 0.025054126319233618, 0.020442448984579024, 0.8749368165399822, 0.10425648982135302, 0.28709118626346114, 0.006226073918966627, 0.0022483044707379486, 0.48373135420415714, 0.0319951020835785, 0.061050113705422765, 0.000691785990996292, 0.1267697828500705, 0.46680958105694187, 0.4693465896496426, 0.022833077334306937, 0.04143780701411259, 0.993748220923981, 0.856776094256838, 0.14226119917123525, 0.37681207166428954, 0.08995963873066372, 0.13748548560724078, 0.1867086841579813, 0.20877425592210636, 0.10800907316222569, 0.2664814018182781, 0.6161829091877793, 0.008853202718215221, 0.9949352225294129, 0.9991920850860895, 0.5563546403833618, 0.07801754727214957, 0.14260584460401113, 0.1777776896857179, 0.017266178494656056, 0.0006394880923946686, 0.026858499880576085, 0.001766022857609381, 0.29080509721967807, 0.10066330288373472, 0.09595390859677637, 0.5103806058491112, 0.9916564995222862, 0.10713080954927734, 0.08773643885501162, 0.006464790231421908, 0.5070242624358039, 0.16254329724717942, 0.12837226316680647, 0.9880947112472745, 0.5140530793646622, 0.30600526165409453, 0.027841881068694747, 0.018263252260657562, 0.1293753464338889, 0.00434231172631019, 0.1708609679612373, 0.8262948450584425, 0.06625475507158798, 0.8392268975734477, 0.09464965010226854, 0.0983916876949379, 0.9010607188904839, 0.43309151582609295, 0.19169624470991, 0.3745176632758427, 0.156935399736532, 0.03714447331042178, 0.6295988226116491, 0.12257676192439187, 0.001857223665521089, 0.05107365080182995, 0.998265224972659, 0.9997150013089338, 0.5148591100654116, 0.11692770997822537, 0.023102652374730012, 0.1263573640087274, 0.0820379900653678, 0.09759691921569617, 0.008486688627451841, 0.030646375599131647, 0.9856088374406387], \"Term\": [\"absolutely\", \"acquaintance\", \"acquaintance\", \"actual\", \"admit\", \"admit\", \"advantage\", \"adventure\", \"adventure\", \"advise\", \"afford\", \"africa\", \"ago\", \"ago\", \"agree\", \"already\", \"already\", \"already\", \"already\", \"already\", \"already\", \"already\", \"amazement\", \"america\", \"answer\", \"answer\", \"answer\", \"answered\", \"answered\", \"answered\", \"answered\", \"answered\", \"answered\", \"answered\", \"anyhow\", \"anything\", \"anything\", \"anything\", \"anything\", \"arthur\", \"ask\", \"ask\", \"ask\", \"asked\", \"attack\", \"awaiting\", \"back\", \"back\", \"back\", \"back\", \"back\", \"back\", \"back\", \"baker\", \"baker\", \"baker\", \"baldwin\", \"band\", \"barclay\", \"barker\", \"barker\", \"barrymore\", \"bartholomew\", \"baskerville\", \"baskerville\", \"baskerville\", \"baynes\", \"baynes\", \"bear\", \"bed\", \"bed\", \"bedroom\", \"behind\", \"behind\", \"behind\", \"bell\", \"bell\", \"better\", \"better\", \"bicycle\", \"bicycle\", \"birlstone\", \"bitterly\", \"black\", \"black\", \"black\", \"black\", \"black\", \"black\", \"blaze\", \"bless\", \"bodymaster\", \"book\", \"book\", \"book\", \"book\", \"bork\", \"boss\", \"box\", \"box\", \"box\", \"box\", \"boys\", \"breast\", \"bright\", \"bright\", \"brixton\", \"brother\", \"brother\", \"brother\", \"brother\", \"brother\", \"brother\", \"brothers\", \"building\", \"building\", \"bureau\", \"bust\", \"bye\", \"cage\", \"came\", \"came\", \"came\", \"came\", \"came\", \"cap\", \"carpet\", \"carriage\", \"carriage\", \"carriage\", \"carruthers\", \"cart\", \"case\", \"case\", \"cases\", \"cases\", \"cecil\", \"certainly\", \"certainly\", \"certainly\", \"chair\", \"chair\", \"chair\", \"chair\", \"chair\", \"chair\", \"chairs\", \"chapter\", \"charing\", \"charles\", \"charlington\", \"chief\", \"children\", \"children\", \"chill\", \"chin\", \"chuckled\", \"cigarette\", \"cipher\", \"circle\", \"claim\", \"clear\", \"clear\", \"clear\", \"clear\", \"clear\", \"clerk\", \"clock\", \"clock\", \"clock\", \"clothes\", \"coal\", \"coincidence\", \"collection\", \"collection\", \"come\", \"come\", \"come\", \"come\", \"come\", \"come\", \"come\", \"comes\", \"comes\", \"comes\", \"comes\", \"conan\", \"concealed\", \"concerned\", \"connected\", \"conscience\", \"consider\", \"constable\", \"consulting\", \"contents\", \"contents\", \"convict\", \"copy\", \"corner\", \"corner\", \"corner\", \"corridor\", \"could\", \"could\", \"could\", \"could\", \"could\", \"could\", \"could\", \"could\", \"country\", \"country\", \"country\", \"country\", \"country\", \"country\", \"course\", \"course\", \"course\", \"course\", \"course\", \"course\", \"cried\", \"cried\", \"cries\", \"criminal\", \"criminal\", \"cunningham\", \"dancing\", \"dark\", \"dark\", \"dark\", \"day\", \"day\", \"day\", \"day\", \"day\", \"day\", \"day\", \"dear\", \"dear\", \"dear\", \"dear\", \"dear\", \"death\", \"death\", \"death\", \"death\", \"death\", \"death\", \"death\", \"death\", \"destroyed\", \"details\", \"devil\", \"devil\", \"difficulties\", \"dining\", \"direct\", \"distant\", \"document\", \"document\", \"document\", \"documents\", \"dog\", \"done\", \"done\", \"done\", \"done\", \"done\", \"done\", \"done\", \"door\", \"door\", \"douglas\", \"douglas\", \"douglas\", \"downstairs\", \"doyle\", \"drama\", \"dressing\", \"drink\", \"due\", \"earlier\", \"east\", \"east\", \"eccles\", \"edwards\", \"eleven\", \"else\", \"else\", \"else\", \"else\", \"empty\", \"empty\", \"end\", \"end\", \"end\", \"end\", \"end\", \"end\", \"end\", \"engaged\", \"engaged\", \"engaged\", \"enough\", \"enough\", \"enough\", \"enough\", \"enough\", \"enough\", \"entered\", \"entered\", \"entered\", \"entered\", \"entirely\", \"entirely\", \"entirely\", \"entirely\", \"envelope\", \"especially\", \"european\", \"evening\", \"evening\", \"evening\", \"evening\", \"evening\", \"events\", \"exactly\", \"exactly\", \"exactly\", \"exactly\", \"exactly\", \"exactly\", \"excellent\", \"excellent\", \"explanation\", \"explanation\", \"express\", \"expressed\", \"expression\", \"eyebrows\", \"eyes\", \"face\", \"faces\", \"fact\", \"fact\", \"fact\", \"fact\", \"facts\", \"facts\", \"family\", \"family\", \"family\", \"famous\", \"famous\", \"fancy\", \"fancy\", \"far\", \"far\", \"far\", \"far\", \"far\", \"far\", \"far\", \"far\", \"far\", \"far\", \"fastened\", \"father\", \"father\", \"ferguson\", \"figure\", \"find\", \"find\", \"find\", \"find\", \"find\", \"find\", \"find\", \"first\", \"first\", \"first\", \"first\", \"first\", \"first\", \"first\", \"floor\", \"floor\", \"flushed\", \"fog\", \"followed\", \"followed\", \"followed\", \"followed\", \"food\", \"fool\", \"forehead\", \"forward\", \"forward\", \"forward\", \"forward\", \"found\", \"found\", \"found\", \"found\", \"found\", \"found\", \"found\", \"found\", \"found\", \"frances\", \"french\", \"friend\", \"friend\", \"friend\", \"friend\", \"friend\", \"friend\", \"friendly\", \"friends\", \"friends\", \"friends\", \"front\", \"front\", \"front\", \"gang\", \"garden\", \"gasped\", \"gate\", \"genius\", \"gennaro\", \"german\", \"get\", \"get\", \"get\", \"get\", \"get\", \"gets\", \"giving\", \"gleam\", \"gloomy\", \"god\", \"god\", \"good\", \"good\", \"good\", \"gossip\", \"got\", \"got\", \"got\", \"got\", \"got\", \"grass\", \"grass\", \"gray\", \"gray\", \"great\", \"great\", \"great\", \"great\", \"great\", \"great\", \"gregson\", \"gregson\", \"grey\", \"guess\", \"guessed\", \"hair\", \"hair\", \"half\", \"half\", \"half\", \"half\", \"half\", \"half\", \"half\", \"half\", \"halloa\", \"hand\", \"hand\", \"hand\", \"hand\", \"hand\", \"hand\", \"hand\", \"hand\", \"hand\", \"hand\", \"handed\", \"hands\", \"hands\", \"hands\", \"hands\", \"harm\", \"harm\", \"harm\", \"head\", \"head\", \"head\", \"head\", \"head\", \"hear\", \"hear\", \"hear\", \"hear\", \"heard\", \"heard\", \"heard\", \"heard\", \"heard\", \"heard\", \"heart\", \"heart\", \"heart\", \"heaven\", \"heavens\", \"height\", \"height\", \"held\", \"held\", \"held\", \"held\", \"held\", \"help\", \"help\", \"help\", \"help\", \"helpless\", \"henderson\", \"henry\", \"henry\", \"hidden\", \"hide\", \"high\", \"high\", \"high\", \"high\", \"high\", \"holmes\", \"holmes\", \"holmes\", \"holmes\", \"holmes\", \"holmes\", \"holmes\", \"holmes\", \"honest\", \"honest\", \"horse\", \"horse\", \"hour\", \"hour\", \"hour\", \"house\", \"house\", \"house\", \"house\", \"housekeeper\", \"however\", \"however\", \"however\", \"however\", \"however\", \"however\", \"however\", \"hudson\", \"hullo\", \"hundred\", \"hundred\", \"husband\", \"hypothesis\", \"ill\", \"ill\", \"ill\", \"impatience\", \"implied\", \"including\", \"indeed\", \"indeed\", \"indeed\", \"indeed\", \"indeed\", \"indeed\", \"indeed\", \"indications\", \"initials\", \"inn\", \"inn\", \"inquest\", \"inquiries\", \"inquiries\", \"inspector\", \"inspector\", \"inspector\", \"inspector\", \"instant\", \"instant\", \"instant\", \"instant\", \"intended\", \"intently\", \"interesting\", \"interesting\", \"interrupted\", \"italian\", \"joke\", \"joseph\", \"journey\", \"journey\", \"journey\", \"joy\", \"justice\", \"key\", \"key\", \"killing\", \"kitchen\", \"knees\", \"knew\", \"knew\", \"knew\", \"knew\", \"knew\", \"knew\", \"knew\", \"knew\", \"know\", \"know\", \"know\", \"know\", \"know\", \"last\", \"last\", \"last\", \"last\", \"last\", \"last\", \"last\", \"last\", \"last\", \"lawn\", \"lay\", \"lay\", \"lay\", \"lay\", \"lay\", \"leaned\", \"leaning\", \"left\", \"left\", \"left\", \"left\", \"left\", \"left\", \"left\", \"leonardo\", \"lestrade\", \"lestrade\", \"lestrade\", \"lestrade\", \"lestrade\", \"let\", \"let\", \"let\", \"let\", \"let\", \"let\", \"letter\", \"letter\", \"letter\", \"letters\", \"letters\", \"letters\", \"letters\", \"life\", \"life\", \"life\", \"life\", \"life\", \"life\", \"life\", \"light\", \"light\", \"light\", \"light\", \"like\", \"like\", \"like\", \"like\", \"like\", \"like\", \"line\", \"line\", \"line\", \"line\", \"lion\", \"little\", \"little\", \"little\", \"little\", \"little\", \"little\", \"little\", \"lived\", \"lived\", \"lives\", \"lock\", \"lodge\", \"lodge\", \"lodge\", \"london\", \"london\", \"london\", \"london\", \"london\", \"london\", \"london\", \"lonely\", \"lonely\", \"lonely\", \"long\", \"long\", \"long\", \"long\", \"long\", \"long\", \"long\", \"looked\", \"looked\", \"looked\", \"lord\", \"lord\", \"loud\", \"loud\", \"love\", \"love\", \"love\", \"love\", \"loved\", \"luck\", \"mac\", \"madam\", \"madam\", \"made\", \"made\", \"made\", \"made\", \"made\", \"made\", \"made\", \"made\", \"made\", \"main\", \"main\", \"make\", \"make\", \"make\", \"make\", \"make\", \"man\", \"man\", \"man\", \"man\", \"man\", \"man\", \"man\", \"man\", \"manor\", \"many\", \"many\", \"many\", \"many\", \"many\", \"marriage\", \"marriage\", \"mason\", \"master\", \"master\", \"master\", \"masterful\", \"matter\", \"matter\", \"matter\", \"matter\", \"matter\", \"matter\", \"mawson\", \"may\", \"may\", \"may\", \"may\", \"may\", \"maybe\", \"mcginty\", \"mcmurdo\", \"mcmurdo\", \"mean\", \"mean\", \"mean\", \"mean\", \"meanwhile\", \"medium\", \"melancholy\", \"mercy\", \"message\", \"midnight\", \"might\", \"might\", \"might\", \"might\", \"might\", \"might\", \"might\", \"miles\", \"miles\", \"miles\", \"mind\", \"mind\", \"mind\", \"mind\", \"mind\", \"mister\", \"money\", \"money\", \"months\", \"months\", \"moor\", \"moor\", \"moor\", \"moran\", \"morning\", \"morning\", \"morning\", \"morning\", \"morning\", \"morning\", \"morning\", \"morris\", \"mountains\", \"mouth\", \"mrs\", \"mrs\", \"mrs\", \"mrs\", \"much\", \"much\", \"much\", \"much\", \"much\", \"much\", \"much\", \"much\", \"mud\", \"murder\", \"murder\", \"murder\", \"murder\", \"murder\", \"murmured\", \"musgrave\", \"must\", \"must\", \"must\", \"must\", \"must\", \"must\", \"must\", \"name\", \"name\", \"name\", \"name\", \"name\", \"name\", \"name\", \"nerve\", \"never\", \"never\", \"never\", \"never\", \"never\", \"never\", \"night\", \"night\", \"night\", \"night\", \"night\", \"night\", \"night\", \"none\", \"none\", \"none\", \"none\", \"none\", \"none\", \"nose\", \"note\", \"note\", \"note\", \"nothing\", \"nothing\", \"nothing\", \"nothing\", \"nothing\", \"nothing\", \"nothing\", \"nothing\", \"nurse\", \"office\", \"office\", \"office\", \"officer\", \"officers\", \"old\", \"old\", \"old\", \"old\", \"old\", \"old\", \"old\", \"old\", \"oldacre\", \"oldacre\", \"one\", \"one\", \"one\", \"one\", \"one\", \"one\", \"one\", \"one\", \"one\", \"one\", \"open\", \"open\", \"opened\", \"opened\", \"opportunity\", \"original\", \"pale\", \"pale\", \"paper\", \"papers\", \"papers\", \"papers\", \"papers\", \"particular\", \"particular\", \"particular\", \"passage\", \"patch\", \"path\", \"path\", \"path\", \"path\", \"pearl\", \"pen\", \"peter\", \"picture\", \"pictures\", \"pile\", \"pipe\", \"pipe\", \"pipe\", \"place\", \"place\", \"place\", \"place\", \"place\", \"planned\", \"played\", \"pleasure\", \"plenty\", \"pocket\", \"pocket\", \"pocket\", \"pocket\", \"point\", \"point\", \"point\", \"point\", \"points\", \"points\", \"points\", \"police\", \"police\", \"police\", \"police\", \"police\", \"porter\", \"possible\", \"possible\", \"possible\", \"post\", \"post\", \"pound\", \"pounds\", \"practice\", \"practice\", \"pray\", \"present\", \"presume\", \"prime\", \"probable\", \"proceed\", \"professor\", \"professor\", \"professor\", \"provided\", \"purpose\", \"purpose\", \"purpose\", \"put\", \"put\", \"put\", \"put\", \"put\", \"put\", \"put\", \"put\", \"pycroft\", \"quarrel\", \"quite\", \"quite\", \"quite\", \"quite\", \"quite\", \"quite\", \"raised\", \"raised\", \"read\", \"read\", \"read\", \"read\", \"really\", \"really\", \"really\", \"really\", \"received\", \"received\", \"received\", \"red\", \"red\", \"red\", \"relations\", \"report\", \"results\", \"right\", \"right\", \"right\", \"right\", \"right\", \"right\", \"right\", \"right\", \"right\", \"ring\", \"ring\", \"road\", \"road\", \"robbery\", \"room\", \"room\", \"round\", \"round\", \"round\", \"round\", \"round\", \"round\", \"round\", \"rubbed\", \"rushed\", \"rushed\", \"said\", \"said\", \"said\", \"said\", \"said\", \"said\", \"sake\", \"sake\", \"san\", \"sank\", \"sat\", \"sat\", \"sat\", \"sat\", \"saw\", \"saw\", \"saw\", \"saw\", \"saw\", \"saw\", \"say\", \"say\", \"say\", \"say\", \"say\", \"say\", \"says\", \"says\", \"scanlan\", \"scent\", \"science\", \"scotland\", \"scott\", \"scowrers\", \"secretary\", \"secrets\", \"see\", \"see\", \"see\", \"see\", \"see\", \"see\", \"see\", \"see\", \"see\", \"seemed\", \"seemed\", \"seemed\", \"seemed\", \"seemed\", \"seemed\", \"seen\", \"seen\", \"seen\", \"seen\", \"seen\", \"seen\", \"seen\", \"seen\", \"seen\", \"seen\", \"sergeant\", \"serious\", \"served\", \"shafter\", \"shall\", \"shall\", \"shall\", \"share\", \"sheet\", \"sherlock\", \"sherlock\", \"sherlock\", \"sherlock\", \"shining\", \"sholto\", \"shook\", \"shortly\", \"shortly\", \"shoulders\", \"shoulders\", \"shrugged\", \"side\", \"side\", \"side\", \"side\", \"silence\", \"silence\", \"silent\", \"silent\", \"simon\", \"simpson\", \"sir\", \"sir\", \"sitting\", \"sitting\", \"sitting\", \"size\", \"skin\", \"sky\", \"slept\", \"slowly\", \"small\", \"small\", \"small\", \"small\", \"small\", \"small\", \"small\", \"smiled\", \"soft\", \"solemn\", \"sound\", \"sound\", \"sound\", \"spot\", \"spot\", \"sprang\", \"sprang\", \"stable\", \"stables\", \"stair\", \"stair\", \"stairs\", \"stangerson\", \"stared\", \"statement\", \"station\", \"station\", \"station\", \"station\", \"staunton\", \"staunton\", \"stayed\", \"stood\", \"stood\", \"stood\", \"straker\", \"stranger\", \"stranger\", \"street\", \"street\", \"street\", \"struggle\", \"suddenly\", \"suddenly\", \"suddenly\", \"suddenly\", \"suggestion\", \"sunk\", \"suppose\", \"suppose\", \"suppose\", \"suppose\", \"sure\", \"sure\", \"sure\", \"sure\", \"sure\", \"surely\", \"surely\", \"surely\", \"suspect\", \"swear\", \"swung\", \"table\", \"table\", \"table\", \"take\", \"take\", \"take\", \"take\", \"tall\", \"telegram\", \"telegram\", \"telegraph\", \"tell\", \"tell\", \"tell\", \"tell\", \"tell\", \"test\", \"thaddeus\", \"thank\", \"thank\", \"thanks\", \"theory\", \"theory\", \"thin\", \"think\", \"think\", \"think\", \"think\", \"think\", \"thought\", \"thought\", \"thought\", \"thought\", \"thought\", \"thought\", \"thought\", \"thought\", \"thousand\", \"three\", \"three\", \"three\", \"three\", \"three\", \"three\", \"three\", \"till\", \"time\", \"time\", \"time\", \"time\", \"time\", \"time\", \"time\", \"time\", \"toby\", \"told\", \"told\", \"told\", \"told\", \"told\", \"told\", \"told\", \"took\", \"took\", \"took\", \"took\", \"took\", \"took\", \"tossed\", \"traces\", \"traces\", \"track\", \"track\", \"tracks\", \"trail\", \"train\", \"train\", \"train\", \"try\", \"try\", \"tuesday\", \"turned\", \"turned\", \"turned\", \"turned\", \"turned\", \"tut\", \"two\", \"two\", \"two\", \"two\", \"two\", \"two\", \"two\", \"two\", \"two\", \"unlikely\", \"upon\", \"upon\", \"upon\", \"upon\", \"upon\", \"upon\", \"upon\", \"use\", \"use\", \"use\", \"use\", \"use\", \"use\", \"vain\", \"valley\", \"valley\", \"valley\", \"vermissa\", \"version\", \"village\", \"villain\", \"voice\", \"voice\", \"voice\", \"von\", \"walk\", \"walk\", \"walk\", \"walk\", \"walk\", \"walk\", \"walk\", \"walked\", \"walked\", \"want\", \"want\", \"want\", \"want\", \"wanted\", \"wanted\", \"wanted\", \"warrant\", \"warranties\", \"watson\", \"watson\", \"watson\", \"watson\", \"way\", \"way\", \"way\", \"way\", \"way\", \"way\", \"way\", \"way\", \"way\", \"week\", \"week\", \"week\", \"well\", \"well\", \"well\", \"well\", \"well\", \"well\", \"well\", \"well\", \"went\", \"went\", \"went\", \"went\", \"wet\", \"white\", \"white\", \"whole\", \"whole\", \"whole\", \"whole\", \"whole\", \"wife\", \"wife\", \"wife\", \"wife\", \"william\", \"window\", \"without\", \"without\", \"without\", \"without\", \"without\", \"without\", \"without\", \"woman\", \"woman\", \"woman\", \"woman\", \"woman\", \"woodley\", \"word\", \"word\", \"word\", \"word\", \"word\", \"word\", \"works\", \"would\", \"would\", \"would\", \"would\", \"would\", \"would\", \"writing\", \"writing\", \"written\", \"written\", \"written\", \"yard\", \"yard\", \"year\", \"year\", \"year\", \"years\", \"years\", \"years\", \"years\", \"years\", \"years\", \"yellow\", \"yes\", \"yet\", \"yet\", \"yet\", \"yet\", \"yet\", \"yet\", \"yet\", \"yet\", \"york\"]}, \"R\": 30, \"lambda.step\": 0.01, \"plot.opts\": {\"xlab\": \"PC1\", \"ylab\": \"PC2\"}, \"topic.order\": [9, 6, 4, 10, 1, 8, 3, 5, 7, 2]};\n",
       "\n",
       "function LDAvis_load_lib(url, callback){\n",
       "  var s = document.createElement('script');\n",
       "  s.src = url;\n",
       "  s.async = true;\n",
       "  s.onreadystatechange = s.onload = callback;\n",
       "  s.onerror = function(){console.warn(\"failed to load library \" + url);};\n",
       "  document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "}\n",
       "\n",
       "if(typeof(LDAvis) !== \"undefined\"){\n",
       "   // already loaded: just create the visualization\n",
       "   !function(LDAvis){\n",
       "       new LDAvis(\"#\" + \"ldavis_el2273217219085326566407052993\", ldavis_el2273217219085326566407052993_data);\n",
       "   }(LDAvis);\n",
       "}else if(typeof define === \"function\" && define.amd){\n",
       "   // require.js is available: use it to load d3/LDAvis\n",
       "   require.config({paths: {d3: \"https://d3js.org/d3.v5\"}});\n",
       "   require([\"d3\"], function(d3){\n",
       "      window.d3 = d3;\n",
       "      LDAvis_load_lib(\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.4.0/pyLDAvis/js/ldavis.v3.0.0.js\", function(){\n",
       "        new LDAvis(\"#\" + \"ldavis_el2273217219085326566407052993\", ldavis_el2273217219085326566407052993_data);\n",
       "      });\n",
       "    });\n",
       "}else{\n",
       "    // require.js not available: dynamically load d3 & LDAvis\n",
       "    LDAvis_load_lib(\"https://d3js.org/d3.v5.js\", function(){\n",
       "         LDAvis_load_lib(\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.4.0/pyLDAvis/js/ldavis.v3.0.0.js\", function(){\n",
       "                 new LDAvis(\"#\" + \"ldavis_el2273217219085326566407052993\", ldavis_el2273217219085326566407052993_data);\n",
       "            })\n",
       "         });\n",
       "}\n",
       "</script>"
      ],
      "text/plain": [
       "PreparedData(topic_coordinates=              x         y  topics  cluster       Freq\n",
       "topic                                                \n",
       "8      0.103095 -0.061666       1        1  21.353868\n",
       "5     -0.169743  0.011525       2        1  15.993045\n",
       "3     -0.192348 -0.065745       3        1  14.272749\n",
       "9      0.129455 -0.136626       4        1  12.771360\n",
       "0     -0.147090 -0.147701       5        1   6.625697\n",
       "7      0.205638 -0.009872       6        1   6.430618\n",
       "2      0.125720 -0.149739       7        1   6.366087\n",
       "4     -0.138171  0.084308       8        1   5.832412\n",
       "6     -0.126021  0.224960       9        1   5.265254\n",
       "1      0.209465  0.250557      10        1   5.088909, topic_info=        Term          Freq         Total Category  logprob  loglift\n",
       "22    holmes  10800.000000  10800.000000  Default  30.0000  30.0000\n",
       "38      said  11138.000000  11138.000000  Default  29.0000  29.0000\n",
       "914      sir   3118.000000   3118.000000  Default  28.0000  28.0000\n",
       "195      yes   2313.000000   2313.000000  Default  27.0000  27.0000\n",
       "58      face   3354.000000   3354.000000  Default  26.0000  26.0000\n",
       "...      ...           ...           ...      ...      ...      ...\n",
       "98       see    289.738422   4948.606153  Topic10  -5.1957   0.1402\n",
       "412   little    272.563566   3436.947320  Topic10  -5.2568   0.4436\n",
       "1180    else    208.447362    726.356606  Topic10  -5.5250   1.7298\n",
       "271     note    208.101543    913.493485  Topic10  -5.5266   1.4989\n",
       "33       one    213.523803   9487.561526  Topic10  -5.5009  -0.8159\n",
       "\n",
       "[708 rows x 6 columns], token_table=      Topic      Freq          Term\n",
       "term                               \n",
       "2112      1  0.998685    absolutely\n",
       "1653      7  0.061543  acquaintance\n",
       "1653      8  0.933408  acquaintance\n",
       "7315      7  0.994904        actual\n",
       "1146      1  0.956321         admit\n",
       "...     ...       ...           ...\n",
       "575       5  0.082038           yet\n",
       "575       7  0.097597           yet\n",
       "575       8  0.008487           yet\n",
       "575       9  0.030646           yet\n",
       "3190      7  0.985609          york\n",
       "\n",
       "[1472 rows x 3 columns], R=30, lambda_step=0.01, plot_opts={'xlab': 'PC1', 'ylab': 'PC2'}, topic_order=[9, 6, 4, 10, 1, 8, 3, 5, 7, 2])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyLDAvis\n",
    "import pyLDAvis.gensim\n",
    "\n",
    "pyLDAvis.enable_notebook()\n",
    "\n",
    "vis = pyLDAvis.gensim.prepare(lda_model, corpus, dictionary)\n",
    "vis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb464123",
   "metadata": {},
   "source": [
    "___________________________________________________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6cd731d",
   "metadata": {},
   "source": [
    "# 10 Demonstration Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6b941e5",
   "metadata": {},
   "source": [
    "### 10.1 Demo Summarisation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca196272",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dirhem\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\utils\\generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "Your max_length is set to 120, but your input_length is only 21. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=10)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Demo Summary Output ===\n",
      "Sherlock Holmes was known for his remarkable deductive abilities. He often solved cases that baffled Scotland Yard. He was also known for solving cases that had never been solved before. He died in 1903 at the age of 87.\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Load summariser model (fast and reliable)\n",
    "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
    "\n",
    "def demo_summary(text):\n",
    "    \"\"\"\n",
    "    Simple demonstration function:\n",
    "    Input: raw text\n",
    "    Output: short summary\n",
    "    \"\"\"\n",
    "    summary = summarizer(text, max_length=120, min_length=40, do_sample=False)\n",
    "    return summary[0]['summary_text']\n",
    "\n",
    "sample_text = \"Sherlock Holmes was known for his remarkable deductive abilities and often solved cases that baffled Scotland Yard.\"\n",
    "\n",
    "print(\"=== Demo Summary Output ===\")\n",
    "print(demo_summary(sample_text))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d639b53",
   "metadata": {},
   "source": [
    "### 10.2 Demo Semantic Search Function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16425f45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Demo Semantic Search Output ===\n",
      "\n",
      "Paragraph Index: 14134\n",
      "Similarity: 0.7602\n",
      "CHAPTER IX\n",
      "          Second Report of Dr. Watson\n",
      "------------------------------------------------------------\n",
      "\n",
      "Paragraph Index: 32229\n",
      "Similarity: 0.7602\n",
      "CHAPTER IX\n",
      "          Second Report of Dr. Watson\n",
      "------------------------------------------------------------\n",
      "\n",
      "Paragraph Index: 39085\n",
      "Similarity: 0.7602\n",
      "CHAPTER IX\n",
      "          Second Report of Dr. Watson\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def demo_semantic_search(query, top_k=5):\n",
    "    \"\"\"\n",
    "    Simple demonstration function:\n",
    "    Input: query string\n",
    "    Output: printed semantic search results\n",
    "    \"\"\"\n",
    "    results = semantic_search_with_scores(query, top_k)\n",
    "    show_results(results)\n",
    "\n",
    "print(\"=== Demo Semantic Search Output ===\")\n",
    "demo_semantic_search(\"Dr Watson\", top_k=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a62ee669",
   "metadata": {},
   "source": [
    "___________________________________________________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3700a989",
   "metadata": {},
   "source": [
    "# 11 Final Outputs and Saving"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2b193fa",
   "metadata": {},
   "source": [
    "### 11.1 Save Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30c47a21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summariser model saved.\n"
     ]
    }
   ],
   "source": [
    "summarizer.model.save_pretrained(\"saved_models/summarizer_model\")\n",
    "summarizer.tokenizer.save_pretrained(\"saved_models/summarizer_model\")\n",
    "\n",
    "print(\"Summariser model saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e397719a",
   "metadata": {},
   "source": [
    "#### 11.1.1 Save the Embedding Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e367a3d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding model saved.\n"
     ]
    }
   ],
   "source": [
    "embedding_model.save(\"saved_models/embedding_model\")\n",
    "\n",
    "print(\"Embedding model saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b51f4dc",
   "metadata": {},
   "source": [
    "### 11.2 Save Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36c172e7",
   "metadata": {},
   "source": [
    "##### 11.2.1 Save embeddings with NumP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca5fd58a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings saved.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.save(\"saved_models/paragraph_embeddings.npy\", paragraph_embeddings)\n",
    "\n",
    "print(\"Embeddings saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b912e286",
   "metadata": {},
   "source": [
    "##### 11.2.2 Save Metadata (Paragraphs and IDs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e3cef3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata saved.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "metadata = {\n",
    "    \"paragraphs\": paragraphs,\n",
    "    \"ids\": ids\n",
    "}\n",
    "\n",
    "with open(\"saved_models/paragraph_metadata.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(metadata, f, indent=2)\n",
    "\n",
    "print(\"Metadata saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e65605cc",
   "metadata": {},
   "source": [
    "### 11.3 Save LDA Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dee6f22",
   "metadata": {},
   "source": [
    "##### 11.3.1 Save Dictionary, Corpus, and LDA Mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e34443",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary saved.\n",
      "LDA model saved.\n"
     ]
    }
   ],
   "source": [
    "dictionary.save(\"saved_models/lda_dictionary.dict\")\n",
    "print(\"Dictionary saved.\")\n",
    "\n",
    "lda_model.save(\"saved_models/lda_model\")\n",
    "print(\"LDA model saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d829abd5",
   "metadata": {},
   "source": [
    "##### 11.3.2 Save Corpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faade797",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LDA model, dictionary, and corpus saved.\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"saved_models/lda_corpus.pkl\", \"wb\") as f:\n",
    "    pickle.dump(corpus, f)\n",
    "\n",
    "print(\"LDA model, dictionary, and corpus saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dc7a065",
   "metadata": {},
   "source": [
    "# 11.4 Export Results for Report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c051309",
   "metadata": {},
   "source": [
    "##### 11.4.1 Export any Text Results to a File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ca2812",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Running export_text() ===\n",
      "Results exported to test_export_output.txt\n",
      "=== Finished ===\n"
     ]
    }
   ],
   "source": [
    "# === Export Function ===\n",
    "def export_text(text, filename=\"exported_results.txt\"):\n",
    "    with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(text)\n",
    "    print(f\"Results exported to {filename}\")\n",
    "\n",
    "\n",
    "# === Test Data ===\n",
    "test_text = \"\"\"\n",
    "This is a test export to verify that the export_text function works correctly.\n",
    "It should create a file and print a confirmation message.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# === Run Export + Print Output ===\n",
    "print(\"=== Running export_text() ===\")\n",
    "export_text(test_text, \"test_export_output.txt\")\n",
    "print(\"=== Finished ===\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ba222fd",
   "metadata": {},
   "source": [
    "##### 11.4.2 Example: Export LDA topics\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69f2bcb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results exported to lda_topics.txt\n"
     ]
    }
   ],
   "source": [
    "topics_text = \"\"\n",
    "\n",
    "for i, topic in lda_model.show_topics(num_topics=10, num_words=10, formatted=False):\n",
    "    topics_text += f\"Topic {i}:\\n\"\n",
    "    topics_text += \", \".join([word for word, prob in topic]) + \"\\n\\n\"\n",
    "\n",
    "export_text(topics_text, \"lda_topics.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c5b6ec",
   "metadata": {},
   "source": [
    "##### 11.4.3 Example: Export Average Topic Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed87965",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results exported to lda_topic_distribution.txt\n"
     ]
    }
   ],
   "source": [
    "export_text(str(avg_topic_distribution), \"lda_topic_distribution.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e4a6cb",
   "metadata": {},
   "source": [
    "___________________________________________________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d843dc61",
   "metadata": {},
   "source": [
    "# Final Completion Cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6411349a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========================================\n",
      "        ALL ASSESSMENT TASKS COMPLETED      \n",
      "===========================================\n",
      "This notebook contains the full implementation\n",
      "of the required NLP pipeline, including:\n",
      " - Data loading and preprocessing\n",
      " - Summarisation model development and evaluation\n",
      " - Semantic search with embeddings and ChromaDB\n",
      " - Topic modelling using LDA\n",
      " - Demonstration functions for deployment\n",
      " - Saving of models, embeddings, and outputs\n",
      "\n",
      "All results have been generated, saved, and\n",
      "prepared for inclusion in the accompanying report.\n",
      "===========================================\n"
     ]
    }
   ],
   "source": [
    "# FINAL CELL - ASSESSMENT TASKS COMPLETED\n",
    "\n",
    "print(\"====================================================\")\n",
    "print(\"        ALL ASSESSMENT TASKS COMPLETED      \")\n",
    "print(\"====================================================\")\n",
    "print(\"This notebook contains the full implementation\")\n",
    "print(\"of the required NLP pipeline, including:\")\n",
    "print(\" - Data loading and preprocessing\")\n",
    "print(\" - Summarisation model development and evaluation\")\n",
    "print(\" - Semantic search with embeddings and ChromaDB\")\n",
    "print(\" - Topic modelling using LDA\")\n",
    "print(\" - Demonstration functions for deployment\")\n",
    "print(\" - Saving of models, embeddings, and outputs\")\n",
    "print(\"\")\n",
    "print(\"All results have been generated, saved, and\")\n",
    "print(\"prepared for inclusion in the accompanying report.\")\n",
    "print(\"======================================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db944bb7",
   "metadata": {},
   "source": [
    "___________________________________________________________________________________________"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
